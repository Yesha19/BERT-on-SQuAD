{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "import json\n",
    "import ast \n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "import torch\n",
    "import pickle\n",
    "from scipy import spatial\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import spacy\n",
    "from nltk import Tree\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "st = LancasterStemmer()\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "en_nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert JSON to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_json(\"data/train-v1.1.json\") #Read the train data JSON File\n",
    "valid = pd.read_json(\"data/dev-v1.1.json\") #Read the valid data JSON File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((442, 2), (48, 2))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': 'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".',\n",
       " 'qas': [{'answers': [{'answer_start': 269, 'text': 'in the late 1990s'}],\n",
       "   'question': 'When did Beyonce start becoming popular?',\n",
       "   'id': '56be85543aeaaa14008c9063'},\n",
       "  {'answers': [{'answer_start': 207, 'text': 'singing and dancing'}],\n",
       "   'question': 'What areas did Beyonce compete in when she was growing up?',\n",
       "   'id': '56be85543aeaaa14008c9065'},\n",
       "  {'answers': [{'answer_start': 526, 'text': '2003'}],\n",
       "   'question': \"When did Beyonce leave Destiny's Child and become a solo singer?\",\n",
       "   'id': '56be85543aeaaa14008c9066'},\n",
       "  {'answers': [{'answer_start': 166, 'text': 'Houston, Texas'}],\n",
       "   'question': 'In what city and state did Beyonce  grow up? ',\n",
       "   'id': '56bf6b0f3aeaaa14008c9601'},\n",
       "  {'answers': [{'answer_start': 276, 'text': 'late 1990s'}],\n",
       "   'question': 'In which decade did Beyonce become famous?',\n",
       "   'id': '56bf6b0f3aeaaa14008c9602'},\n",
       "  {'answers': [{'answer_start': 320, 'text': \"Destiny's Child\"}],\n",
       "   'question': 'In what R&B group was she the lead singer?',\n",
       "   'id': '56bf6b0f3aeaaa14008c9603'},\n",
       "  {'answers': [{'answer_start': 505, 'text': 'Dangerously in Love'}],\n",
       "   'question': 'What album made her a worldwide known artist?',\n",
       "   'id': '56bf6b0f3aeaaa14008c9604'},\n",
       "  {'answers': [{'answer_start': 360, 'text': 'Mathew Knowles'}],\n",
       "   'question': \"Who managed the Destiny's Child group?\",\n",
       "   'id': '56bf6b0f3aeaaa14008c9605'},\n",
       "  {'answers': [{'answer_start': 166, 'text': 'Houston'}],\n",
       "   'question': 'In what city did Beyonce grow up?',\n",
       "   'id': '56cef8faaab44d1400b88d67'},\n",
       "  {'answers': [{'answer_start': 505, 'text': 'Dangerously in Love'}],\n",
       "   'question': \"What was the name of Beyonce's first solo album?\",\n",
       "   'id': '56cef8faaab44d1400b88d68'},\n",
       "  {'answers': [{'answer_start': 64, 'text': 'September 4, 1981'}],\n",
       "   'question': 'On what date was Beyonce born?',\n",
       "   'id': '56cef8faaab44d1400b88d6a'},\n",
       "  {'answers': [{'answer_start': 0, 'text': 'Beyoncé Giselle Knowles-Carter'}],\n",
       "   'question': \"What is Beyonce's full name?\",\n",
       "   'id': '56cef8faaab44d1400b88d6b'},\n",
       "  {'answers': [{'answer_start': 276, 'text': 'late 1990s'}],\n",
       "   'question': 'When did Beyoncé rise to fame?',\n",
       "   'id': '56d43c5f2ccc5a1400d830a9'},\n",
       "  {'answers': [{'answer_start': 290, 'text': 'lead singer'}],\n",
       "   'question': \"What role did Beyoncé have in Destiny's Child?\",\n",
       "   'id': '56d43c5f2ccc5a1400d830aa'},\n",
       "  {'answers': [{'answer_start': 505, 'text': 'Dangerously in Love'}],\n",
       "   'question': 'What was the first album Beyoncé released as a solo artist?',\n",
       "   'id': '56d43c5f2ccc5a1400d830ab'},\n",
       "  {'answers': [{'answer_start': 526, 'text': '2003'}],\n",
       "   'question': 'When did Beyoncé release Dangerously in Love?',\n",
       "   'id': '56d43c5f2ccc5a1400d830ac'},\n",
       "  {'answers': [{'answer_start': 590, 'text': 'five'}],\n",
       "   'question': 'How many Grammy awards did Beyoncé win for her first solo album?',\n",
       "   'id': '56d43c5f2ccc5a1400d830ad'},\n",
       "  {'answers': [{'answer_start': 290, 'text': 'lead singer'}],\n",
       "   'question': \"What was Beyoncé's role in Destiny's Child?\",\n",
       "   'id': '56d43ce42ccc5a1400d830b4'},\n",
       "  {'answers': [{'answer_start': 505, 'text': 'Dangerously in Love'}],\n",
       "   'question': \"What was the name of Beyoncé's first solo album?\",\n",
       "   'id': '56d43ce42ccc5a1400d830b5'},\n",
       "  {'answers': [{'answer_start': 526, 'text': '2003'}],\n",
       "   'question': 'When did Beyoncé release her first solo album?',\n",
       "   'id': '56d43ce42ccc5a1400d830b6'}]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.iloc[1,0]['paragraphs'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models import InferSent\n",
    "V = 2\n",
    "MODEL_PATH = '/Users/devshreepatel/Desktop/LR/encoder/infersent%s.pkl' % V\n",
    "params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
    "                'pool_type': 'max', 'dpout_model': 0.0, 'version': V} # Model Parameters For Infersent Model\n",
    "infersent = InferSent(params_model) # Infersent Pre Trained encoder by Facebook\n",
    "infersent.load_state_dict(torch.load(MODEL_PATH)) # Load Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = []       \n",
    "questions = []\n",
    "answers_text = []\n",
    "answers_start = []\n",
    "for i in range(train.shape[0]):\n",
    "    topic = train.iloc[i,0]['paragraphs'] # To select each paragraph from data and gather corresponding info of it.\n",
    "    for sub_para in topic:\n",
    "        for q_a in sub_para['qas']:\n",
    "            questions.append(q_a['question']) # To get Question of each paragraph \n",
    "            answers_start.append(q_a['answers'][0]['answer_start']) # To get the start index of answer\n",
    "            answers_text.append(q_a['answers'][0]['text']) # To get the Answer for the question\n",
    "            contexts.append(sub_para['context']) # To get the context of the paragraph\n",
    "df = pd.DataFrame({\"context\":contexts, \"question\": questions, \"answer_start\": answers_start, \"text\": answers_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87599, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/train.csv\", index = None) # Change JSON file to CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dictionary of sentence embeddings for faster computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "paras = list(df[\"context\"].drop_duplicates().reset_index(drop= True)) # Create List of all Contexts and drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18891"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob = TextBlob(\" \".join(paras)) \n",
    "sentences = [item.raw for item in blob.sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92659"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2V_PATH = '/Users/devshreepatel/Desktop/LR/data/fastText/crawl-300d-2M.vec'\n",
    "infersent.set_w2v_path(W2V_PATH) # Set the Word To Vector Path for embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 89231(/109703) words with w2v vectors\n",
      "Vocab size : 89231\n"
     ]
    }
   ],
   "source": [
    "infersent.build_vocab(sentences, tokenize=True) # Using Function of InferSent Class to build vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-4d541e5dba1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdict_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfersent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Encode the sentences in dict_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/LR/models.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, sentences, bsize, tokenize, verbose)\u001b[0m\n\u001b[1;32m    220\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstidx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstidx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbsize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/LR/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sent_tuple)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# Handling padding in Recurrent Networks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0msent_packed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_len_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0msent_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_packed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# seqlen x batch x 2*nhid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0msent_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_packed_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    559\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[1;32m    560\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n\u001b[0m\u001b[1;32m    562\u001b[0m                               self.num_layers, self.dropout, self.training, self.bidirectional)\n\u001b[1;32m    563\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dict_embeddings = {}\n",
    "for i in range(len(sentences)):\n",
    "    print(i)\n",
    "    dict_embeddings[sentences[i]] = infersent.encode([sentences[i]], tokenize=True) # Encode the sentences in dict_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = list(df[\"question\"]) #Use the question column from df and create a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87599"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-014c2b803378>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdict_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquestions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfersent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquestions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Generate encoding for questions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/LR/models.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, sentences, bsize, tokenize, verbose)\u001b[0m\n\u001b[1;32m    220\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstidx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstidx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbsize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/LR/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sent_tuple)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# Handling padding in Recurrent Networks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0msent_packed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_len_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0msent_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_packed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# seqlen x batch x 2*nhid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0msent_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_packed_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    559\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[1;32m    560\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n\u001b[0m\u001b[1;32m    562\u001b[0m                               self.num_layers, self.dropout, self.training, self.bidirectional)\n\u001b[1;32m    563\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(len(questions)):\n",
    "    print(i)\n",
    "    dict_embeddings[questions[i]] = infersent.encode([questions[i]], tokenize=True) # Generate encoding for questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dictionary with each Sentence and corresponding word embeddings array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = {key:dict_embeddings[key] for i, key in enumerate(dict_embeddings) if i % 2 == 0} \n",
    "d2 = {key:dict_embeddings[key] for i, key in enumerate(dict_embeddings) if i % 2 == 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Architecturally, the school has a Catholic character.': array([[ 0.00746889, -0.05086312,  0.00736476, ...,  0.04118386,\n",
       "          0.01421822, -0.0138564 ]], dtype=float32),\n",
       " 'Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\".': array([[0.00746889, 0.00995223, 0.19095321, ..., 0.00119955, 0.04134529,\n",
       "         0.03366187]], dtype=float32),\n",
       " 'Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection.': array([[ 0.00746889, -0.0568299 ,  0.11969507, ..., -0.01752903,\n",
       "          0.01482134, -0.00012652]], dtype=float32),\n",
       " 'At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.': array([[ 0.00746889, -0.01367914,  0.1417282 , ...,  0.00988861,\n",
       "         -0.00070966,  0.06526423]], dtype=float32),\n",
       " 'The nine student-run outlets include three newspapers, both a radio and television station, and several magazines and journals.': array([[ 0.00746889, -0.04796241,  0.07264923, ...,  0.03530579,\n",
       "          0.04510233, -0.00038588]], dtype=float32),\n",
       " 'The other magazine, The Juggler, is released twice a year and focuses on student literature and artwork.': array([[ 0.00746889, -0.06945133,  0.13181168, ...,  0.04248592,\n",
       "          0.02065417, -0.01261841]], dtype=float32),\n",
       " \"The newspapers have varying publication interests, with The Observer published daily and mainly reporting university and other news, and staffed by students from both Notre Dame and Saint Mary's College.\": array([[ 0.00746889, -0.01281483,  0.07204209, ...,  0.0573613 ,\n",
       "          0.01229877, -0.01062984]], dtype=float32),\n",
       " 'In 1987, when some students believed that The Observer began to show a conservative bias, a liberal newspaper, Common Sense was published.': array([[ 0.00781301, -0.01500764,  0.03391016, ...,  0.01759525,\n",
       "          0.02070952,  0.00820774]], dtype=float32),\n",
       " 'Neither paper is published as often as The Observer; however, all three are distributed to all students.': array([[ 0.00746889,  0.04504532,  0.02792171, ...,  0.01247961,\n",
       "          0.0042558 , -0.00804492]], dtype=float32),\n",
       " 'The university is the major seat of the Congregation of Holy Cross (albeit not its official headquarters, which are in Rome).': array([[ 0.00746889, -0.03980925,  0.09276018, ...,  0.03126104,\n",
       "          0.01634196,  0.00973465]], dtype=float32),\n",
       " 'Old College, the oldest building on campus and located near the shore of St. Mary lake, houses undergraduate seminarians.': array([[ 0.00746889,  0.0091266 ,  0.11803459, ...,  0.01303004,\n",
       "         -0.00119626, -0.00416506]], dtype=float32),\n",
       " 'The university through the Moreau Seminary has ties to theologian Frederick Buechner.': array([[ 0.00746889,  0.04136043,  0.12646264, ...,  0.05841827,\n",
       "          0.04806095, -0.00954267]], dtype=float32),\n",
       " 'The College of Engineering was established in 1920, however, early courses in civil and mechanical engineering were a part of the College of Science since the 1870s.': array([[0.00746889, 0.00695036, 0.08987007, ..., 0.00723788, 0.00754538,\n",
       "         0.01113917]], dtype=float32),\n",
       " 'degrees offered.': array([[ 0.00746889, -0.14567915,  0.12703256, ...,  0.01041708,\n",
       "         -0.04434213, -0.0138564 ]], dtype=float32),\n",
       " 'and Master of Business Administration (MBA) degrees, respectively.': array([[ 0.00746889, -0.03469377,  0.13060714, ...,  0.04177964,\n",
       "         -0.00392607,  0.0112134 ]], dtype=float32),\n",
       " 'The First Year of Studies program was established in 1962 to guide incoming freshmen in their first year at the school before they have declared a major.': array([[ 0.00746889, -0.05850448,  0.11080081, ...,  0.03031485,\n",
       "         -0.01271816,  0.00850045]], dtype=float32),\n",
       " 'The program also includes a Learning Resource Center which provides time management, collaborative learning, and subject tutoring.': array([[ 0.00746889, -0.06945133,  0.13127132, ...,  0.0187227 ,\n",
       "         -0.03068226, -0.00473083]], dtype=float32),\n",
       " 'The university first offered graduate degrees, in the form of a Master of Arts (MA), in the 1854–1855 academic year.': array([[ 0.00746889, -0.05923435,  0.1428654 , ...,  0.0588249 ,\n",
       "         -0.00733571,  0.00044472]], dtype=float32),\n",
       " 'and Master of Civil Engineering in its early stages of growth, before a formal graduate school education was developed with a thesis not required to receive the degrees.': array([[ 0.00746889, -0.03469377,  0.13060714, ...,  0.02861905,\n",
       "          0.02539843,  0.00634693]], dtype=float32),\n",
       " 'Today each of the five colleges offer graduate education.': array([[ 0.00746889, -0.05605261,  0.12544382, ...,  0.04128855,\n",
       "          0.02234924,  0.03159412]], dtype=float32),\n",
       " 'program also exists.': array([[ 0.00746889, -0.11579645,  0.07624522, ..., -0.02836947,\n",
       "         -0.04434213, -0.01146343]], dtype=float32),\n",
       " 'The School of Architecture offers a Master of Architecture, while each of the departments of the College of Engineering offer PhD programs.': array([[ 0.00746889, -0.04462816,  0.15158054, ...,  0.02463973,\n",
       "         -0.01131338, -0.01114521]], dtype=float32),\n",
       " 'It also operates facilities in Chicago and Cincinnati for its executive MBA program.': array([[ 0.00746889, -0.06523705,  0.05011318, ...,  0.01453906,\n",
       "          0.03475435, -0.0138564 ]], dtype=float32),\n",
       " 'The Joan B. Kroc Institute for International Peace Studies at the University of Notre Dame is dedicated to research, education and outreach on the causes of violent conflict and the conditions for sustainable peace.': array([[ 0.00746889, -0.04407658,  0.17127456, ...,  0.01185475,\n",
       "          0.05270163, -0.0138564 ]], dtype=float32),\n",
       " \"It was founded in 1986 through the donations of Joan B. Kroc, the widow of McDonald's owner Ray Kroc.\": array([[ 0.00746889,  0.06351598,  0.13452797, ...,  0.02958886,\n",
       "          0.07627265, -0.00409858]], dtype=float32),\n",
       " 'Theodore M. Hesburgh CSC, President Emeritus of the University of Notre Dame.': array([[ 0.00746889, -0.01289263,  0.10980612, ...,  0.045949  ,\n",
       "          0.02540371, -0.0138564 ]], dtype=float32),\n",
       " 'The library system of the university is divided between the main library and each of the colleges and schools.': array([[ 0.00746889, -0.04266419,  0.06388525, ...,  0.03282906,\n",
       "          0.00740542,  0.0001418 ]], dtype=float32),\n",
       " 'The front of the library is adorned with the Word of Life mural designed by artist Millard Sheets.': array([[ 0.00746889, -0.06945133,  0.12051487, ..., -0.01929487,\n",
       "          0.05701393,  0.01379034]], dtype=float32),\n",
       " 'Notre Dame is known for its competitive admissions, with the incoming class enrolling in fall 2015 admitting 3,577 from a pool of 18,156 (19.7%).': array([[ 0.00746889, -0.02886533,  0.04536827, ...,  0.04803208,\n",
       "          0.06480951,  0.02531486]], dtype=float32),\n",
       " 'The university practices a non-restrictive early action policy that allows admitted students to consider admission to Notre Dame as well as any other colleges to which they were accepted.': array([[ 0.00746889, -0.05019838,  0.12120074, ...,  0.0920378 ,\n",
       "          0.01183403, -0.00874361]], dtype=float32),\n",
       " 'Admitted students came from 1,311 high schools and the average student traveled more than 750 miles to Notre Dame, making it arguably the most representative university in the United States.': array([[ 0.00746889, -0.05901236,  0.09063398, ...,  0.06671999,\n",
       "          0.05074641,  0.00282298]], dtype=float32),\n",
       " 'In 2015-2016, Notre Dame ranked 18th overall among \"national universities\" in the United States in U.S. News & World Report\\'s Best Colleges 2016.': array([[ 0.00746889, -0.02622681,  0.11909319, ...,  0.04402131,\n",
       "          0.02760862, -0.00121248]], dtype=float32),\n",
       " \"Forbes.com's America's Best Colleges ranks Notre Dame 13th among colleges in the United States in 2015, 8th among Research Universities, and 1st in the Midwest.\": array([[ 0.00746889, -0.02103034,  0.1186296 , ...,  0.05072717,\n",
       "          0.06204705, -0.00584517]], dtype=float32),\n",
       " 'BusinessWeek ranks Mendoza College of Business undergraduate school as 1st overall.': array([[ 0.00746889, -0.03763525,  0.09855548, ...,  0.04193189,\n",
       "          0.09590768, -0.01385639]], dtype=float32),\n",
       " \"The Philosophical Gourmet Report ranks Notre Dame's graduate philosophy program as 15th nationally, while ARCHITECT Magazine ranked the undergraduate architecture program as 12th nationally.\": array([[0.00746889, 0.03411497, 0.10283748, ..., 0.08295286, 0.06137797,\n",
       "         0.00088181]], dtype=float32),\n",
       " 'According to payscale.com, undergraduate alumni of University of Notre Dame have a mid-career median salary $110,000, making it the 24th highest among colleges and universities in the United States.': array([[ 0.00746889, -0.04069261,  0.12210719, ...,  0.07497196,\n",
       "          0.01057672,  0.01115221]], dtype=float32),\n",
       " 'Father Joseph Carrier, C.S.C.': array([[ 0.00746889,  0.13306412,  0.04797483, ...,  0.00227219,\n",
       "         -0.02712891, -0.01385639]], dtype=float32),\n",
       " 'Carrier taught that scientific research and its promise for progress were not antagonistic to the ideals of intellectual and moral culture endorsed by the Church.': array([[ 0.00746889, -0.01368759,  0.07775414, ...,  0.05001398,\n",
       "          0.05159443, -0.01036239]], dtype=float32),\n",
       " 'Zahm was active in the Catholic Summer School movement, which introduced Catholic laity to contemporary intellectual issues.': array([[ 0.00746889,  0.00402826,  0.1031303 , ...,  0.02423323,\n",
       "          0.03003588, -0.01385639]], dtype=float32),\n",
       " \"The intervention of Irish American Catholics in Rome prevented Zahm's censure by the Vatican.\": array([[0.00746889, 0.00659518, 0.05789064, ..., 0.01064422, 0.05036927,\n",
       "         0.01174656]], dtype=float32),\n",
       " \"In 1882, Albert Zahm (John Zahm's brother) built an early wind tunnel used to compare lift to drag of aeronautical models.\": array([[ 0.00746889,  0.13713878,  0.10880731, ...,  0.03585671,\n",
       "         -0.00994918,  0.0222488 ]], dtype=float32),\n",
       " 'In 1931, Father Julius Nieuwland performed early work on basic reactions that was used to create neoprene.': array([[ 0.00746889,  0.15577857,  0.15450579, ...,  0.01827547,\n",
       "         -0.03104604, -0.00922077]], dtype=float32),\n",
       " 'The Lobund Institute grew out of pioneering research in germ-free-life which began in 1928.': array([[ 0.00746889, -0.06945133,  0.12485177, ...,  0.00026835,\n",
       "         -0.0225818 , -0.01385639]], dtype=float32),\n",
       " 'Though others had taken up this idea, their research was short lived and inconclusive.': array([[ 0.00746889, -0.07036042,  0.05646488, ...,  0.01615443,\n",
       "         -0.01412128, -0.0138564 ]], dtype=float32),\n",
       " \"But the objective was not merely to answer Pasteur's question but also to produce the germ free animal as a new tool for biological and medical research.\": array([[ 0.00746889, -0.01058597,  0.0969854 , ...,  0.04258225,\n",
       "          0.05719625,  0.00210844]], dtype=float32),\n",
       " 'Today the work has spread to other universities.': array([[ 0.00746889, -0.05605261,  0.05420532, ...,  0.03133028,\n",
       "          0.01799517,  0.00414737]], dtype=float32),\n",
       " 'In the 1940s Lobund achieved independent status as a purely research organization and in 1950 was raised to the status of an Institute.': array([[ 0.00746889, -0.02578715,  0.11929949, ...,  0.02384967,\n",
       "          0.00364211,  0.01404227]], dtype=float32),\n",
       " 'The Review of Politics was founded in 1939 by Gurian, modeled after German Catholic journals.': array([[0.00746889, 0.06615608, 0.114291  , ..., 0.01966334, 0.05561492,\n",
       "         0.03002649]], dtype=float32),\n",
       " 'For 44 years, the Review was edited by Gurian, Matthew Fitzsimons, Frederick Crosson, and Thomas Stritch.': array([[0.00746889, 0.07585771, 0.12363724, ..., 0.02274249, 0.02921229,\n",
       "         0.02103494]], dtype=float32),\n",
       " 'It became a major forum for political ideas and modern political concerns, especially from a Catholic and scholastic tradition.': array([[ 0.00746889, -0.05279437,  0.10632031, ...,  0.03179824,\n",
       "          0.01003933, -0.00233491]], dtype=float32),\n",
       " 'The university president, John Jenkins, described his hope that Notre Dame would become \"one of the pre–eminent research institutions in the world\" in his inaugural address.': array([[ 0.00746889,  0.10895999,  0.07747042, ...,  0.07434669,\n",
       "          0.02956559, -0.0126728 ]], dtype=float32),\n",
       " 'Recent research includes work on family conflict and child development, genome mapping, the increasing trade deficit of the United States with China, studies in fluid mechanics, computational science and engineering, and marketing trends on the Internet.': array([[ 0.00746889, -0.06287918,  0.09614699, ...,  0.03689047,\n",
       "          0.04000059,  0.00610811]], dtype=float32),\n",
       " 'In 2014 the Notre Dame student body consisted of 12,179 students, with 8,448 undergraduates, 2,138 graduate and professional and 1,593 professional (Law, M.Div., Business, M.Ed.)': array([[ 0.00746889, -0.02061258,  0.09831659, ...,  0.01695207,\n",
       "          0.06951132,  0.01059516]], dtype=float32),\n",
       " 'Around 21–24% of students are children of alumni, and although 37% of students come from the Midwestern United States, the student body represents all 50 states and 100 countries.': array([[ 0.00746889, -0.03502593,  0.11360253, ...,  0.05338392,\n",
       "          0.1028404 ,  0.00943997]], dtype=float32),\n",
       " 'As of March 2015[update] The Princeton Review ranked Notre Dame as the ninth highest.': array([[ 0.00746889, -0.02317568,  0.11532791, ...,  0.00119202,\n",
       "          0.04781081,  0.02407013]], dtype=float32),\n",
       " 'It has also been commended by some diversity oriented publications; Hispanic Magazine in 2004 ranked the university ninth on its list of the top–25 colleges for Latinos, and The Journal of Blacks in Higher Education recognized the university in 2006 for raising enrollment of African-American students.': array([[ 0.00746889, -0.04348709,  0.17341624, ...,  0.04328664,\n",
       "          0.03428029, -0.00139962]], dtype=float32),\n",
       " \"The annual Bookstore Basketball tournament is the largest outdoor five-on-five tournament in the world with over 700 teams participating each year, while the Notre Dame Men's Boxing Club hosts the annual Bengal Bouts tournament that raises money for the Holy Cross Missions in Bangladesh.\": array([[ 0.00746889,  0.24994919,  0.11272144, ...,  0.0521933 ,\n",
       "          0.02884049, -0.00719388]], dtype=float32),\n",
       " 'The majority of the graduate students on campus live in one of four graduate housing complexes on campus, while all on-campus undergraduates live in one of the 29 residence halls.': array([[0.00746889, 0.03244872, 0.04762889, ..., 0.00308712, 0.03863527,\n",
       "         0.01353929]], dtype=float32),\n",
       " \"The university maintains a visiting policy (known as parietal hours) for those students who live in dormitories, specifying times when members of the opposite sex are allowed to visit other students' dorm rooms; however, all residence halls have 24-hour social spaces for students regardless of gender.\": array([[0.00746889, 0.02991413, 0.05201813, ..., 0.07779917, 0.03390772,\n",
       "         0.02204674]], dtype=float32),\n",
       " 'There are no traditional social fraternities or sororities at the university, but a majority of students live in the same residence hall for all four years.': array([[ 0.00746889,  0.06321944,  0.05501018, ...,  0.07414997,\n",
       "          0.02908874, -0.00737796]], dtype=float32),\n",
       " 'At the end of the intramural season, the championship game is played on the field in Notre Dame Stadium.': array([[ 0.00746889, -0.05595423,  0.04222571, ...,  0.0226332 ,\n",
       "          0.02247325,  0.00196628]], dtype=float32),\n",
       " 'While religious affiliation is not a criterion for admission, more than 93% of students identify as Christian, with over 80% of the total being Catholic.': array([[0.00746889, 0.0004513 , 0.03816226, ..., 0.05667157, 0.05137774,\n",
       "         0.00212138]], dtype=float32),\n",
       " 'There are multitudes of religious statues and artwork around campus, most prominent of which are the statue of Mary on the Main Building, the Notre Dame Grotto, and the Word of Life mural on Hesburgh Library depicting Christ as a teacher.': array([[ 0.00746889, -0.00438826,  0.14002848, ...,  0.02964163,\n",
       "          0.04284028,  0.02862951]], dtype=float32),\n",
       " 'There are many religious clubs (catholic and non-Catholic) at the school, including Council #1477 of the Knights of Columbus (KOC), Baptist Collegiate Ministry (BCM), Jewish Club, Muslim Student Association, Orthodox Christian Fellowship, The Mormon Club, and many more.': array([[ 0.00746889,  0.0494376 ,  0.10073157, ...,  0.05716039,\n",
       "          0.02817526, -0.00206219]], dtype=float32),\n",
       " 'Fifty-seven chapels are located throughout the campus.': array([[ 0.00746889, -0.11340452,  0.06519581, ..., -0.04419396,\n",
       "         -0.00266542, -0.00185361]], dtype=float32),\n",
       " 'The university founder, Fr.': array([[ 0.00746889,  0.11382534,  0.04054348, ...,  0.07316682,\n",
       "          0.04564752, -0.01051323]], dtype=float32),\n",
       " 'William Corby, immediately planned for the rebuilding of the structure that had housed virtually the entire University.': array([[0.00746889, 0.05848249, 0.07820141, ..., 0.00071696, 0.0252802 ,\n",
       "         0.00649237]], dtype=float32),\n",
       " 'The library collection was also rebuilt and stayed housed in the new Main Building for years afterwards.': array([[ 0.00746889, -0.06881724,  0.12122766, ...,  0.00788161,\n",
       "          0.00179163, -0.01385639]], dtype=float32),\n",
       " 'Eventually becoming known as Washington Hall, it hosted plays and musical acts put on by the school.': array([[ 0.00746889, -0.07898016,  0.14488319, ...,  0.00730676,\n",
       "          0.01870639, -0.01385639]], dtype=float32),\n",
       " 'The hall housed multiple classrooms and science labs needed for early research at the university.': array([[ 0.00746889, -0.06945133,  0.09032642, ...,  0.02997248,\n",
       "         -0.01495686, -0.00818464]], dtype=float32),\n",
       " 'What is in front of the Notre Dame Main Building?': array([[ 0.00746889, -0.03348338,  0.04054592, ..., -0.02341788,\n",
       "          0.02275974, -0.00952859]], dtype=float32),\n",
       " 'What is the Grotto at Notre Dame?': array([[ 0.00746889, -0.02113509,  0.08985032, ...,  0.00978156,\n",
       "          0.0465938 , -0.02454636]], dtype=float32),\n",
       " 'When did the Scholastic Magazine of Notre dame begin publishing?': array([[ 0.00746889, -0.03302433,  0.09289244, ...,  0.01606723,\n",
       "          0.03835963, -0.00657418]], dtype=float32),\n",
       " 'What is the daily student paper at Notre Dame called?': array([[ 0.00746889, -0.0324956 ,  0.06629253, ...,  0.00449075,\n",
       "          0.01920301, -0.01774   ]], dtype=float32),\n",
       " 'In what year did the student paper Common Sense begin publication at Notre Dame?': array([[ 0.00746889, -0.02891017,  0.05455929, ...,  0.04914213,\n",
       "          0.04443559, -0.01475404]], dtype=float32),\n",
       " 'What is the primary seminary of the Congregation of the Holy Cross?': array([[ 0.00746889, -0.01703593,  0.08965974, ...,  0.07007513,\n",
       "          0.00153861,  0.00036549]], dtype=float32),\n",
       " 'What individuals live at Fatima House at Notre Dame?': array([[ 0.00746889, -0.0372405 ,  0.06271604, ...,  0.00978156,\n",
       "          0.06424851, -0.02894759]], dtype=float32),\n",
       " 'How many BS level degrees are offered in the College of Engineering at Notre Dame?': array([[ 0.00746889, -0.03446827,  0.10656089, ...,  0.04954449,\n",
       "          0.03083676, -0.02208699]], dtype=float32),\n",
       " 'Before the creation of the College of Engineering similar studies were carried out at which Notre Dame college?': array([[ 0.00746889, -0.04028606,  0.0758063 , ...,  0.02348873,\n",
       "          0.0425625 , -0.00937898]], dtype=float32),\n",
       " 'The College of Science began to offer civil engineering courses beginning at what time at Notre Dame?': array([[ 0.00746889, -0.04968461,  0.1390745 , ...,  0.03620443,\n",
       "          0.03083676, -0.01888708]], dtype=float32),\n",
       " 'How many colleges for undergraduates are at Notre Dame?': array([[ 0.00746889, -0.02839426,  0.04101167, ...,  0.01976703,\n",
       "          0.03083676, -0.02894759]], dtype=float32),\n",
       " 'Which organization declared the First Year of Studies program at Notre Dame \"outstanding?\"': array([[ 0.00746889, -0.0346874 ,  0.15414032, ...,  0.03128888,\n",
       "          0.01950195, -0.01948333]], dtype=float32),\n",
       " 'What type of degree is an M.Div.?': array([[ 0.00746889, -0.05823731,  0.01139571, ...,  0.03373544,\n",
       "          0.09522612, -0.01709035]], dtype=float32),\n",
       " 'In what year was a Master of Arts course first offered at Notre Dame?': array([[ 0.00746889, -0.03371969,  0.15038876, ...,  0.04127797,\n",
       "          0.03083676, -0.0033253 ]], dtype=float32),\n",
       " 'What institute at Notre Dame studies  the reasons for violent conflict?': array([[ 0.00746889, -0.02470956,  0.03462701, ...,  0.07613046,\n",
       "         -0.0006332 , -0.0013717 ]], dtype=float32),\n",
       " 'In what year was the Joan B. Kroc Institute for International Peace Studies founded?': array([[ 0.00746889, -0.03403048,  0.16819099, ...,  0.03443929,\n",
       "          0.05506965, -0.01081187]], dtype=float32),\n",
       " 'What company did Ray Kroc own?': array([[ 0.00746889,  0.05765868, -0.01467852, ...,  0.03650914,\n",
       "          0.10190575, -0.0328564 ]], dtype=float32),\n",
       " 'What is the name of the main library at Notre Dame?': array([[ 0.00746889, -0.03746278,  0.04875365, ...,  0.01320337,\n",
       "          0.03083676, -0.01552814]], dtype=float32),\n",
       " 'Which artist created the mural on the Theodore M. Hesburgh Library?': array([[ 0.00746889, -0.016178  ,  0.10061903, ...,  0.02586384,\n",
       "          0.04762136, -0.00266146]], dtype=float32),\n",
       " 'How many incoming students did Notre Dame admit in fall 2015?': array([[ 0.00746889, -0.04596245,  0.03091678, ...,  0.05160764,\n",
       "          0.02021478,  0.00782873]], dtype=float32),\n",
       " 'Where does Notre Dame rank in terms of academic profile among research universities in the US?': array([[ 0.00746889, -0.02924412,  0.07677265, ...,  0.05709687,\n",
       "          0.02628081, -0.01288541]], dtype=float32),\n",
       " 'How many miles does the average student at Notre Dame travel to study there?': array([[ 0.00746889, -0.03664748,  0.06924798, ...,  0.06917804,\n",
       "          0.02017234, -0.01383365]], dtype=float32),\n",
       " 'Forbes.com placed Notre Dame at what position compared to other US research universities?': array([[ 0.00746889,  0.00897993,  0.06193391, ...,  0.04408635,\n",
       "          0.06281313, -0.00946541]], dtype=float32),\n",
       " 'In 2014 what entity named Notre Dame 10th best of all American universities?': array([[ 0.00746889, -0.02864725,  0.14881517, ...,  0.04408635,\n",
       "          0.08267792, -0.00946541]], dtype=float32),\n",
       " 'What person was the Director of the Science Museum at Notre Dame in the late 19th century?': array([[ 0.00746889, -0.02785532,  0.12482373, ...,  0.01516618,\n",
       "          0.03249896, -0.01414509]], dtype=float32),\n",
       " 'What program did John Augustine Zahm come to co-direct at Nore Dame?': array([[ 0.00746889,  0.11066419,  0.01120655, ...,  0.04927904,\n",
       "          0.02025451, -0.02894759]], dtype=float32),\n",
       " 'What professorship did Father Josh Carrier hold at Notre Dame?': array([[ 0.00746889,  0.16188827,  0.07492634, ...,  0.111545  ,\n",
       "          0.03083676, -0.02894759]], dtype=float32),\n",
       " 'Which professor sent the first wireless message in the USA?': array([[ 0.00746889, -0.04929662,  0.03690943, ...,  0.04306669,\n",
       "          0.0460338 , -0.01144039]], dtype=float32),\n",
       " 'Which individual worked on projects at Notre Dame that eventually created neoprene?': array([[ 0.00746889, -0.02006249,  0.07559655, ...,  0.01201947,\n",
       "          0.02168188, -0.02878987]], dtype=float32),\n",
       " 'Work on a germ-free-life ended up in the creation of which Notre Dame institute?': array([[ 0.00746889, -0.03822502,  0.0800937 , ...,  0.06439325,\n",
       "          0.02905383, -0.01654513]], dtype=float32),\n",
       " 'In what year did Lobund at Notre Dame become an Institute?': array([[ 0.00746889, -0.02696233,  0.0250694 , ...,  0.05618641,\n",
       "          0.03526094, -0.01922856]], dtype=float32),\n",
       " 'When did study of a germ-free-life begin at Notre Dame?': array([[ 0.00746889, -0.03789863,  0.02699653, ...,  0.04053538,\n",
       "          0.03083676, -0.01568062]], dtype=float32),\n",
       " 'What was the Review of Politics inspired by?': array([[ 0.00746889, -0.05823731,  0.04278293, ..., -0.03789991,\n",
       "          0.04135627, -0.02331193]], dtype=float32),\n",
       " 'Thomas Stritch was an editor of which publican from Notre Dame?': array([[ 0.00746889,  0.04913894,  0.09781592, ...,  0.01179953,\n",
       "          0.06644663, -0.01144144]], dtype=float32),\n",
       " 'The Kellogg Institute for International Studies is part of which university?': array([[ 0.00746889, -0.06945133,  0.05224474, ...,  0.04270795,\n",
       "         -0.00458944,  0.00655247]], dtype=float32),\n",
       " 'In what year did Notre Dame begin to host the Global Adaptation Index?': array([[ 0.00746889, -0.03403048,  0.01961547, ...,  0.04908524,\n",
       "          0.0055473 , -0.0193087 ]], dtype=float32),\n",
       " 'How many undergrads were attending Notre Dame in 2014?': array([[ 0.00746889, -0.01990862,  0.03648934, ..., -0.01308993,\n",
       "          0.0231524 , -0.00624262]], dtype=float32),\n",
       " 'How many teams participate in the Notre Dame Bookstore Basketball tournament?': array([[ 0.00746889, -0.02813184,  0.10712758, ...,  0.03652083,\n",
       "          0.01718631, -0.0328564 ]], dtype=float32),\n",
       " 'How many students in total were at Notre Dame in 2014?': array([[ 0.00746889, -0.02785887,  0.03287395, ...,  0.01023224,\n",
       "          0.0231524 , -0.00624261]], dtype=float32),\n",
       " \"How many student housing areas are reserved for Notre Dame's graduate students?\": array([[ 0.00746889, -0.05173535,  0.04067856, ...,  0.01379202,\n",
       "          0.01389107, -0.02007076]], dtype=float32),\n",
       " 'What amount of the graduate student body at Notre Dame live on the campus?': array([[ 0.00746889, -0.0371416 ,  0.04438407, ...,  0.02127676,\n",
       "          0.01013274, -0.01647669]], dtype=float32),\n",
       " 'What is Congregation of Holy Cross in Latin?': array([[ 0.00746889, -0.02973464,  0.09743706, ...,  0.0131976 ,\n",
       "          0.02463088, -0.01794858]], dtype=float32),\n",
       " 'How often is Catholic mass held at Notre Dame in a week?': array([[ 0.00746889, -0.01751873,  0.05284524, ...,  0.01370351,\n",
       "          0.01871812, -0.00679172]], dtype=float32),\n",
       " 'What amount of the student body of Notre Dame identifies as Catholic?': array([[ 0.00746889, -0.03650118,  0.05207566, ...,  0.01249221,\n",
       "          0.02177988, -0.02070744]], dtype=float32),\n",
       " 'In what year was the Main Building at Notre Dame razed in a fire?': array([[ 0.00746889, -0.02076855,  0.04093444, ...,  0.03817076,\n",
       "          0.04391238, -0.00544452]], dtype=float32),\n",
       " 'On what date was the rebuilding of The Main Building begun at Notre Dame after the fire that claimed the previous?': array([[ 0.00746889, -0.02040427,  0.03295892, ...,  0.01467578,\n",
       "          0.01884308,  0.01857617]], dtype=float32),\n",
       " 'What type of education was pushed at Notre Dame before its embracing of national standards?': array([[ 0.00746889, -0.02165048,  0.08528131, ...,  0.03449102,\n",
       "          0.01598102, -0.00088837]], dtype=float32),\n",
       " 'In 1919 a new president of Notre Dame was named, who was it?': array([[ 0.00746889, -0.02970909,  0.04769919, ...,  0.0646278 ,\n",
       "          0.01071784, -0.01534988]], dtype=float32),\n",
       " 'Those who attended a Jesuit college may have been forbidden from joining which Law School due to the curricula at the Jesuit institution?': array([[ 0.00746889,  0.02844515,  0.02600075, ...,  0.08158822,\n",
       "          0.02152558, -0.00978446]], dtype=float32),\n",
       " 'What was the amount of wins Knute Rockne attained at Notre Dame while head coach?': array([[ 0.00746889, -0.03035681,  0.13278662, ...,  0.04793294,\n",
       "          0.02569186, -0.01971718]], dtype=float32),\n",
       " 'How many years was Knute Rockne head coach at Notre Dame?': array([[ 0.00746889, -0.03850565,  0.12539884, ...,  0.05783675,\n",
       "          0.03083676, -0.0208456 ]], dtype=float32),\n",
       " 'Catholic people identified with Notre Dame, what religious group did people feel Yale represented?': array([[ 0.00746889, -0.02209795,  0.06042501, ...,  0.04559908,\n",
       "          0.02986021,  0.00248878]], dtype=float32),\n",
       " 'Which college president of Notre Dame is credited with preventing more confrontations between students and the KKK?': array([[ 0.00746889, -0.02573144,  0.108574  , ...,  0.06578565,\n",
       "          0.00322759, -0.02343128]], dtype=float32),\n",
       " 'Where did Notre Dame students and the KKK have their encounter?': array([[ 0.00746889, -0.02924412,  0.0276786 , ...,  0.04222579,\n",
       "          0.00669964, -0.02907055]], dtype=float32),\n",
       " 'Who was the president of Notre Dame in 1934?': array([[ 0.00746889, -0.02809111,  0.04901605, ...,  0.06075799,\n",
       "          0.02308474,  0.00147639]], dtype=float32),\n",
       " 'Which year was the Laetare Medal first given out at Notre Dame?': array([[ 0.00746889, -0.04716794,  0.13190906, ...,  0.00978156,\n",
       "          0.03083675, -0.01977634]], dtype=float32),\n",
       " 'Around the time that Rev. Cavanaugh became president of Notre Dame by how much did the undergrad student body of Notre Dame increase?': array([[ 0.00746889, -0.03099484,  0.05874563, ...,  0.06889748,\n",
       "          0.02881337, -0.01630138]], dtype=float32),\n",
       " \"What is O'Shaughnessy Hall of Notre Dame formerly known as?\": array([[ 0.00746889, -0.03125277,  0.05632982, ..., -0.00388747,\n",
       "          0.01793338, -0.01437634]], dtype=float32),\n",
       " 'Outside of an institute studying animals, what other institute did Cavanugh create at Notre Dame?': array([[ 0.00746889, -0.03875487,  0.10189147, ...,  0.08482949,\n",
       "          0.03083676, -0.02789392]], dtype=float32),\n",
       " 'In the time that Hesburgh was president of Notre Dame by what factor did the operating budget increase?': array([[ 0.00746889, -0.03629669,  0.06326973, ...,  0.06766914,\n",
       "          0.01810973, -0.01016424]], dtype=float32),\n",
       " 'During what years was Theodor Hesburgh president of Notre Dame?': array([[ 0.00746889, -0.00962188,  0.10658694, ...,  0.06685726,\n",
       "          0.03083676, -0.00741636]], dtype=float32),\n",
       " 'What type of educational institute is Hesburgh given credit for creating at Notre Dame?': array([[ 0.00746889, -0.03575664,  0.14572087, ...,  0.0645376 ,\n",
       "          0.03083676, -0.00555479]], dtype=float32),\n",
       " 'What title did Thomas Blantz have at Notre Dame?': array([[ 0.00746889,  0.06024232,  0.0126643 , ...,  0.04338562,\n",
       "          0.03083676, -0.02894759]], dtype=float32),\n",
       " 'With what institute did Notre Dame agree to an exchange program in the 1960s?': array([[ 0.00746889, -0.04164001,  0.09536715, ...,  0.0875727 ,\n",
       "          0.00917505, -0.00699064]], dtype=float32),\n",
       " 'What was the SAT score, on average, at Notre Dame when Edward Malloy became president?': array([[0.00746889, 0.01976051, 0.10480314, ..., 0.06317904, 0.0559235 ,\n",
       "         0.01901216]], dtype=float32),\n",
       " 'When Malloy reached the end of his time as president how much annuals funding for research did Notre Dame have?': array([[0.00746889, 0.03364968, 0.02125943, ..., 0.08938672, 0.02298217,\n",
       "         0.04054976]], dtype=float32),\n",
       " 'When did John Jenkins become the president of Notre Dame?': array([[ 0.00746889,  0.10641082,  0.03985216, ...,  0.06009026,\n",
       "          0.03083676, -0.02894759]], dtype=float32),\n",
       " 'Who was the Notre Dame president that preceded John Jenkins?': array([[ 0.00746889,  0.11933088,  0.0269052 , ...,  0.05650651,\n",
       "          0.01954538, -0.02476055]], dtype=float32),\n",
       " 'How much money was spent on enhancing Notre Dame Stadium under John Jenkins?': array([[ 0.00746889,  0.09882665,  0.14466661, ...,  0.02586999,\n",
       "          0.01439339, -0.01250162]], dtype=float32),\n",
       " 'What structure is found on the location of the original church of Father Sorin at Notre Dame?': array([[0.00746889, 0.14653462, 0.09822486, ..., 0.03051508, 0.03083676,\n",
       "         0.00305689]], dtype=float32),\n",
       " 'Which individual painted the inside of the Basilica of the Sacred Heart at Notre Dame?': array([[ 0.00746889, -0.04755112,  0.15691644, ...,  0.00978156,\n",
       "          0.03083676, -0.00695911]], dtype=float32),\n",
       " 'Which person oversaw the creation of a science hall at Notre Dame in 1883?': array([[ 0.00746889, -0.04108037,  0.1013907 , ...,  0.02323397,\n",
       "          0.02190747, -0.00938567]], dtype=float32),\n",
       " 'After which individual was the LaFortune Center Notre Dame named?': array([[ 0.00746889, -0.04405033,  0.10913444, ..., -0.0201329 ,\n",
       "          0.02254217, -0.00981468]], dtype=float32),\n",
       " \"What is the annual budget of Notre Dame's LaFortune Center?\": array([[ 0.00746889, -0.04248491,  0.11136586, ..., -0.02105238,\n",
       "          0.01674825, -0.01635798]], dtype=float32),\n",
       " 'Which library was built at Notre Dame in 1963?': array([[ 0.00746889, -0.01759705,  0.04516668, ...,  0.00932071,\n",
       "          0.0224239 ,  0.01000787]], dtype=float32),\n",
       " 'Construction for which hall started on March 8th 2007 at Notre Dame?': array([[ 0.00746889, -0.03332771,  0.06269735, ...,  0.00978156,\n",
       "          0.03083675, -0.01037716]], dtype=float32),\n",
       " 'In what year did Notre Dame create the Office of Sustainability?': array([[ 0.00746889, -0.03403048,  0.05620001, ...,  0.0473133 ,\n",
       "          0.00882926, -0.01896719]], dtype=float32),\n",
       " 'Notre Dame got a \"B\" for its sustainability practices from which entity?': array([[ 0.00746889, -0.02886533,  0.03867822, ...,  0.08181497,\n",
       "          0.00847422,  0.00637887]], dtype=float32),\n",
       " 'In what year did Notre Dame first have a facility in England?': array([[ 0.00746889, -0.03403048,  0.0169792 , ...,  0.05161941,\n",
       "          0.00811198, -0.00462423]], dtype=float32),\n",
       " 'Notre Dame has a center in Beijing, what is it referred to as?': array([[ 0.00746889, -0.02886533,  0.0830157 , ...,  0.02410353,\n",
       "          0.03002707,  0.00351521]], dtype=float32),\n",
       " \"What was Notre Dame's first college?\": array([[ 0.00746889, -0.02864076,  0.02440687, ...,  0.03786386,\n",
       "          0.01170451, -0.02032986]], dtype=float32),\n",
       " 'In what year did the College of Arts and Letters at Notre Dame grant its first degree?': array([[ 0.00746889, -0.02844605,  0.1376361 , ...,  0.05101108,\n",
       "          0.04466929, -0.01352173]], dtype=float32),\n",
       " 'How many BA majors does the College of Arts and Letters at Notre Dame offer?': array([[ 0.00746889, -0.03186386,  0.1382464 , ...,  0.05493123,\n",
       "          0.0530495 , -0.02980229]], dtype=float32),\n",
       " 'In what year was the Notre Dame College of Science formed?': array([[ 0.00746889, -0.02825755,  0.08014558, ...,  0.03580542,\n",
       "          0.00495198,  0.00344326]], dtype=float32),\n",
       " 'Which hall at Notre Dame contains the current College of Science?': array([[ 0.00746889, -0.0240424 ,  0.07021291, ...,  0.00755613,\n",
       "         -0.00240806,  0.00338926]], dtype=float32),\n",
       " 'In 1899 Notre Dame formed which college?': array([[ 0.00746889, -0.01366227,  0.0505464 , ...,  0.02348871,\n",
       "          0.01593718,  0.00781384]], dtype=float32),\n",
       " 'What length is the course of study at the Notre Dame School of Architecture?': array([[ 0.00746889, -0.0335194 ,  0.08323182, ..., -0.00261256,\n",
       "          0.00199022, -0.02005003]], dtype=float32),\n",
       " 'Which prestigious prize does the School of Architecture at Notre Dame give out?': array([[ 0.00746889, -0.02282041,  0.11354667, ...,  0.0332429 ,\n",
       "          0.02573718, -0.02761552]], dtype=float32),\n",
       " 'Where is the theology library at Notre Dame?': array([[ 0.00746889, -0.02924412,  0.06263556, ...,  0.04035725,\n",
       "          0.03083676, -0.02894759]], dtype=float32),\n",
       " \"Currently where does Notre Dame's library rank in the nation?\": array([[ 0.00746889, -0.04025814,  0.08504339, ...,  0.0480503 ,\n",
       "          0.03381392, -0.02800731]], dtype=float32),\n",
       " 'From where did Anton-Hermann Chroust come to reach Notre Dame?': array([[ 0.00746889, -0.0322109 ,  0.02149286, ...,  0.04636394,\n",
       "          0.08648323, -0.01909629]], dtype=float32),\n",
       " 'Who did Waldemar Gurian receive his tutelage under while seeking his doctorate?': array([[0.00746889, 0.05750531, 0.12061819, ..., 0.06904055, 0.02771274,\n",
       "         0.02379695]], dtype=float32)}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"Atop the Main Building's gold dome is a golden statue of the Virgin Mary.\": array([[ 0.00746889, -0.02137874,  0.17287011, ...,  0.00844874,\n",
       "          0.04903353,  0.04396459]], dtype=float32),\n",
       " 'Next to the Main Building is the Basilica of the Sacred Heart.': array([[ 0.00746889, -0.02694481,  0.16048309, ..., -0.03994385,\n",
       "         -0.00807925, -0.00656207]], dtype=float32),\n",
       " 'It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858.': array([[ 0.00746889, -0.04131714,  0.10167662, ...,  0.05108181,\n",
       "          0.04168973,  0.01518794]], dtype=float32),\n",
       " \"As at most other universities, Notre Dame's students run a number of news media outlets.\": array([[ 0.00746889, -0.03765845,  0.03816633, ...,  0.05854929,\n",
       "          0.02586767, -0.01350585]], dtype=float32),\n",
       " 'Begun as a one-page journal in September 1876, the Scholastic magazine is issued twice monthly and claims to be the oldest continuous collegiate publication in the United States.': array([[ 0.00746889, -0.08227891,  0.10700443, ...,  0.02604834,\n",
       "          0.00711613,  0.02184358]], dtype=float32),\n",
       " 'The Dome yearbook is published annually.': array([[ 0.00746889, -0.06945133,  0.04074246, ..., -0.00653984,\n",
       "          0.02827273,  0.06893884]], dtype=float32),\n",
       " 'Unlike Scholastic and The Dome, The Observer is an independent publication and does not have a faculty advisor or any editorial oversight from the University.': array([[ 0.00746889, -0.00868971,  0.04584178, ...,  0.07883545,\n",
       "          0.0108946 ,  0.05912701]], dtype=float32),\n",
       " 'Likewise, in 2003, when other students believed that the paper showed a liberal bias, the conservative paper Irish Rover went into production.': array([[ 0.00746889, -0.03754952,  0.07206297, ...,  0.03919014,\n",
       "         -0.01459047,  0.00715965]], dtype=float32),\n",
       " 'Finally, in Spring 2008 an undergraduate journal for political science research, Beyond Politics, made its debut.': array([[ 0.00746889, -0.08117481,  0.0767844 , ..., -0.00046703,\n",
       "         -0.0019121 ,  0.03459951]], dtype=float32),\n",
       " 'Its main seminary, Moreau Seminary, is located on the campus across St. Joseph lake from the Main Building.': array([[0.00746889, 0.07550844, 0.05978115, ..., 0.08552437, 0.00766757,\n",
       "         0.00031776]], dtype=float32),\n",
       " 'Retired priests and brothers reside in Fatima House (a former retreat center), Holy Cross House, as well as Columba Hall near the Grotto.': array([[0.00746889, 0.1820733 , 0.09297036, ..., 0.01853614, 0.0473526 ,\n",
       "         0.00340141]], dtype=float32),\n",
       " 'While not Catholic, Buechner has praised writers from Notre Dame and Moreau Seminary created a Buechner Prize for Preaching.': array([[ 0.00746889, -0.04176637,  0.1637811 , ...,  0.03942004,\n",
       "          0.046553  , -0.0138564 ]], dtype=float32),\n",
       " 'Today the college, housed in the Fitzpatrick, Cushing, and Stinson-Remick Halls of Engineering, includes five departments of study – aerospace and mechanical engineering, chemical and biomolecular engineering, civil engineering and geological sciences, computer science and engineering, and electrical engineering – with eight B.S.': array([[0.00746889, 0.00940546, 0.14497943, ..., 0.03633758, 0.04410801,\n",
       "         0.01700503]], dtype=float32),\n",
       " 'Additionally, the college offers five-year dual degree programs with the Colleges of Arts and Letters and of Business awarding additional B.A.': array([[ 0.00746889, -0.07104177,  0.143702  , ...,  0.03910587,\n",
       "          0.0181081 , -0.00461961]], dtype=float32),\n",
       " \"All of Notre Dame's undergraduate students are a part of one of the five undergraduate colleges at the school or are in the First Year of Studies program.\": array([[ 0.00746889,  0.03328385,  0.0642952 , ...,  0.0492411 ,\n",
       "          0.02251647, -0.00075209]], dtype=float32),\n",
       " 'Each student is given an academic advisor from the program who helps them to choose classes that give them exposure to any major in which they are interested.': array([[ 0.00746889, -0.05323166,  0.08923214, ...,  0.03438856,\n",
       "         -0.0020474 ,  0.01713784]], dtype=float32),\n",
       " 'This program has been recognized previously, by U.S. News & World Report, as outstanding.': array([[ 0.00746889, -0.0918988 ,  0.15955979, ...,  0.04207907,\n",
       "          0.02545061,  0.00627473]], dtype=float32),\n",
       " 'The program expanded to include Master of Laws (LL.M.)': array([[ 0.00746889, -0.03330582,  0.14000763, ...,  0.01198477,\n",
       "         -0.01954646, -0.00877506]], dtype=float32),\n",
       " 'This changed in 1924 with formal requirements developed for graduate degrees, including offering Doctorate (PhD) degrees.': array([[ 0.00746889, -0.04672971,  0.13976207, ...,  0.0363691 ,\n",
       "          0.01286281, -0.0138564 ]], dtype=float32),\n",
       " 'Most of the departments from the College of Arts and Letters offer PhD programs, while a professional Master of Divinity (M.Div.)': array([[ 0.00746889, -0.05189733,  0.15362355, ...,  0.03977236,\n",
       "          0.09015362, -0.00877506]], dtype=float32),\n",
       " 'All of the departments in the College of Science offer PhD programs, except for the Department of Pre-Professional Studies.': array([[ 0.00746889,  0.03328385,  0.15462303, ...,  0.01990374,\n",
       "         -0.0247236 , -0.00293499]], dtype=float32),\n",
       " 'The College of Business offers multiple professional programs including MBA and Master of Science in Accountancy programs.': array([[ 0.00746889, -0.04870484,  0.13229567, ...,  0.02480117,\n",
       "         -0.01267732, -0.01385639]], dtype=float32),\n",
       " 'Additionally, the Alliance for Catholic Education program offers a Master of Education program where students study at the university during the summer and teach in Catholic elementary schools, middle schools, and high schools across the Southern United States for two school years.': array([[ 0.00746889, -0.02873893,  0.14856733, ...,  0.06021661,\n",
       "         -0.00038707,  0.00165187]], dtype=float32),\n",
       " \"It offers PhD, Master's, and undergraduate degrees in peace studies.\": array([[ 0.00746889, -0.03883571,  0.17630017, ...,  0.03739916,\n",
       "          0.00887458, -0.00093834]], dtype=float32),\n",
       " 'The institute was inspired by the vision of the Rev.': array([[ 0.00746889, -0.06506519,  0.07094878, ...,  0.05572679,\n",
       "          0.01739113, -0.01238694]], dtype=float32),\n",
       " 'The institute has contributed to international policy discussions about peace building practices.': array([[ 0.00746889, -0.06945133,  0.12925576, ...,  0.07389921,\n",
       "          0.02326114, -0.01193932]], dtype=float32),\n",
       " 'The main building is the 14-story Theodore M. Hesburgh Library, completed in 1963, which is the third building to house the main collection of books.': array([[ 0.00746889, -0.00057826,  0.14213876, ...,  0.02757917,\n",
       "         -0.01346247,  0.01538247]], dtype=float32),\n",
       " 'This mural is popularly known as \"Touchdown Jesus\" because of its proximity to Notre Dame Stadium and Jesus\\' arms appearing to make the signal for a touchdown.': array([[ 0.00746889,  0.04835721,  0.11194215, ..., -0.00441161,\n",
       "          0.03789902,  0.02356623]], dtype=float32),\n",
       " 'The academic profile of the enrolled class continues to rate among the top 10 to 15 in the nation for national research universities.': array([[ 0.00746889, -0.06945133,  0.10018624, ...,  0.03133028,\n",
       "          0.02711735,  0.01084266]], dtype=float32),\n",
       " '1,400 of the 3,577 (39.1%) were admitted under the early action plan.': array([[ 0.00746889, -0.08201728,  0.06572732, ...,  0.02040935,\n",
       "          0.02101141,  0.00449545]], dtype=float32),\n",
       " 'While all entering students begin in the College of the First Year of Studies, 25% have indicated they plan to study in the liberal arts or social sciences, 24% in engineering, 24% in business, 24% in science, and 3% in architecture.': array([[ 0.00746889, -0.02637072,  0.1599175 , ...,  0.06593244,\n",
       "          0.03005763,  0.00705174]], dtype=float32),\n",
       " 'In 2014, USA Today ranked Notre Dame 10th overall for American universities based on data from College Factual.': array([[ 0.00746889, -0.02425078,  0.06734497, ...,  0.03919033,\n",
       "          0.04756414,  0.01954811]], dtype=float32),\n",
       " 'U.S. News & World Report also lists Notre Dame Law School as 22nd overall.': array([[ 0.00746889, -0.05366019,  0.05851858, ...,  0.01582344,\n",
       "          0.01507084, -0.0138564 ]], dtype=float32),\n",
       " 'It ranks the MBA program as 20th overall.': array([[ 0.00746889, -0.08601225,  0.06173663, ...,  0.02692425,\n",
       "          0.01696824, -0.0138564 ]], dtype=float32),\n",
       " 'Additionally, the study abroad program ranks sixth in highest participation percentage in the nation, with 57.6% of students choosing to study abroad in 17 countries.': array([[ 0.00746889, -0.09561538,  0.12592892, ...,  0.0587713 ,\n",
       "          0.03556156,  0.01061007]], dtype=float32),\n",
       " 'The median starting salary of $55,300 ranked 58th in the same peer group.': array([[0.00746889, 0.03345595, 0.09974945, ..., 0.06220954, 0.02058925,\n",
       "         0.00716132]], dtype=float32),\n",
       " 'was Director of the Science Museum and the Library and Professor of Chemistry and Physics until 1874.': array([[ 0.00746889, -0.06417917,  0.1293624 , ..., -0.00811615,\n",
       "         -0.02560744, -0.00945935]], dtype=float32),\n",
       " \"One of Carrier's students was Father John Augustine Zahm (1851–1921) who was made Professor and Co-Director of the Science Department at age 23 and by 1900 was a nationally prominent scientist and naturalist.\": array([[0.00746889, 0.18194008, 0.09765062, ..., 0.09101745, 0.02425553,\n",
       "         0.00358301]], dtype=float32),\n",
       " 'His book Evolution and Dogma (1896) defended certain aspects of evolutionary theory as true, and argued, moreover, that even the great Church teachers Thomas Aquinas and Augustine taught something like it.': array([[0.00746889, 0.13776019, 0.11558075, ..., 0.03065387, 0.03955825,\n",
       "         0.00704968]], dtype=float32),\n",
       " 'In 1913, Zahm and former President Theodore Roosevelt embarked on a major expedition through the Amazon.': array([[ 0.00746889,  0.02374011,  0.06610431, ...,  0.03726104,\n",
       "         -0.01089616, -0.0126924 ]], dtype=float32),\n",
       " 'Around 1899, Professor Jerome Green became the first American to send a wireless message.': array([[ 0.00746889,  0.0081201 ,  0.05048409, ...,  0.01085681,\n",
       "          0.06579891, -0.0138564 ]], dtype=float32),\n",
       " 'Study of nuclear physics at the university began with the building of a nuclear accelerator in 1936, and continues now partly through a partnership in the Joint Institute for Nuclear Astrophysics.': array([[ 0.00746889, -0.09022078,  0.1239248 , ...,  0.05208289,\n",
       "          0.03308126,  0.00400871]], dtype=float32),\n",
       " 'This area of research originated in a question posed by Pasteur as to whether animal life was possible without bacteria.': array([[ 0.00746889, -0.0194023 ,  0.05260702, ...,  0.00298189,\n",
       "          0.03493489,  0.01126307]], dtype=float32),\n",
       " 'Lobund was the first research organization to answer definitively, that such life is possible and that it can be prolonged through generations.': array([[ 0.00746889, -0.0393243 ,  0.06836958, ...,  0.02675898,\n",
       "          0.03209956, -0.01385639]], dtype=float32),\n",
       " 'This objective was reached and for years Lobund was a unique center for the study and production of germ free animals and for their use in biological and medical investigations.': array([[0.00746889, 0.02085518, 0.12203287, ..., 0.00193028, 0.04921304,\n",
       "         0.03424264]], dtype=float32),\n",
       " \"In the beginning it was under the Department of Biology and a program leading to the master's degree accompanied the research program.\": array([[ 0.00746889, -0.05091237,  0.1322094 , ...,  0.03143582,\n",
       "         -0.01744045, -0.01385639]], dtype=float32),\n",
       " 'In 1958 it was brought back into the Department of Biology as integral part of that department, but with its own program leading to the degree of PhD in Gnotobiotics.': array([[ 0.00746889, -0.07840765,  0.13568433, ...,  0.04244294,\n",
       "          0.00586794,  0.00780457]], dtype=float32),\n",
       " 'It quickly emerged as part of an international Catholic intellectual revival, offering an alternative vision to positivist philosophy.': array([[ 0.00746889,  0.11705339,  0.11638644, ...,  0.04934934,\n",
       "          0.05687793, -0.0080667 ]], dtype=float32),\n",
       " \"Intellectual leaders included Gurian, Jacques Maritain, Frank O'Malley, Leo Richard Ward, F. A. Hermens, and John U. Nef.\": array([[0.00746889, 0.11453307, 0.13181822, ..., 0.06915756, 0.001477  ,\n",
       "         0.00755003]], dtype=float32),\n",
       " 'As of 2012[update] research continued in many fields.': array([[ 0.00746889, -0.08351565,  0.05001136, ..., -0.012907  ,\n",
       "         -0.03575537, -0.01087091]], dtype=float32),\n",
       " 'The university has many multi-disciplinary institutes devoted to research in varying fields, including the Medieval Institute, the Kellogg Institute for International Studies, the Kroc Institute for International Peace studies, and the Center for Social Concerns.': array([[ 0.00746889, -0.06945133,  0.1615899 , ...,  0.06833797,\n",
       "          0.05345197,  0.00079866]], dtype=float32),\n",
       " 'As of 2013, the university is home to the Notre Dame Global Adaptation Index which ranks countries annually based on how vulnerable they are to climate change and how prepared they are to adapt.': array([[ 0.00746889, -0.034707  ,  0.0415285 , ...,  0.07674602,\n",
       "          0.00751369, -0.01018844]], dtype=float32),\n",
       " 'students.': array([[ 0.00746889, -0.13231756,  0.01555136, ..., -0.05920078,\n",
       "         -0.04434213, -0.0138564 ]], dtype=float32),\n",
       " \"As of March 2007[update] The Princeton Review ranked the school as the fifth highest 'dream school' for parents to send their children.\": array([[0.00746889, 0.01585644, 0.10074192, ..., 0.03371761, 0.03982302,\n",
       "         0.02105858]], dtype=float32),\n",
       " 'The school has been previously criticized for its lack of diversity, and The Princeton Review ranks the university highly among schools at which \"Alternative Lifestyles [are] Not an Alternative.\"': array([[ 0.00746889, -0.03007026,  0.08295311, ...,  0.05598167,\n",
       "          0.0149753 , -0.0137577 ]], dtype=float32),\n",
       " 'With 6,000 participants, the university\\'s intramural sports program was named in 2004 by Sports Illustrated as the best program in the country, while in 2007 The Princeton Review named it as the top school where \"Everyone Plays Intramural Sports.\"': array([[ 0.00746889, -0.05663411,  0.14988625, ...,  0.06029693,\n",
       "         -0.0061603 ,  0.00325034]], dtype=float32),\n",
       " 'About 80% of undergraduates and 20% of graduate students live on campus.': array([[ 0.00746889, -0.09700564,  0.04523597, ...,  0.04170553,\n",
       "          0.04609856,  0.00661732]], dtype=float32),\n",
       " 'Because of the religious affiliation of the university, all residence halls are single-sex, with 15 male dorms and 14 female dorms.': array([[ 0.00746889,  0.25497192,  0.04851457, ...,  0.05016318,\n",
       "          0.04243832, -0.00640982]], dtype=float32),\n",
       " 'Many residence halls have at least one nun and/or priest as a resident.': array([[ 0.00746889,  0.02504156,  0.03021   , ...,  0.09426585,\n",
       "          0.02699503, -0.0138564 ]], dtype=float32),\n",
       " 'Some intramural sports are based on residence hall teams, where the university offers the only non-military academy program of full-contact intramural American football.': array([[ 0.00746889, -0.00487626,  0.08493537, ...,  0.05580501,\n",
       "          0.0352921 , -0.01385639]], dtype=float32),\n",
       " 'The university is affiliated with the Congregation of Holy Cross (Latin: Congregatio a Sancta Cruce, abbreviated postnominals: \"CSC\").': array([[ 0.00746889, -0.03692098,  0.11871914, ...,  0.0433188 ,\n",
       "          0.00261235,  0.00131414]], dtype=float32),\n",
       " 'Collectively, Catholic Mass is celebrated over 100 times per week on campus, and a large campus ministry program provides for the faith needs of the community.': array([[ 0.00746889, -0.01537811,  0.14105509, ...,  0.07375652,\n",
       "          0.06113064, -0.00934059]], dtype=float32),\n",
       " 'Additionally, every classroom displays a crucifix.': array([[ 0.00746889, -0.0079357 ,  0.07550132, ..., -0.02271711,\n",
       "          0.02533777,  0.01104472]], dtype=float32),\n",
       " 'The Notre Dame KofC are known for being the first collegiate council of KofC, operating a charitable concession stand during every home football game and owning their own building on campus which can be used as a cigar lounge.': array([[0.00746889, 0.00658777, 0.13855124, ..., 0.06005629, 0.00545128,\n",
       "         0.01708827]], dtype=float32),\n",
       " 'This Main Building, and the library collection, was entirely destroyed by a fire in April 1879, and the school closed immediately and students were sent home.': array([[ 0.00746889, -0.02296377,  0.12319382, ...,  0.01656312,\n",
       "          0.03854515,  0.00694606]], dtype=float32),\n",
       " 'Sorin and the president at the time, the Rev.': array([[ 0.00746889, -0.04467461,  0.0823707 , ...,  0.07189906,\n",
       "         -0.01085076, -0.0126553 ]], dtype=float32),\n",
       " 'Construction was started on the 17th of May and by the incredible zeal of administrator and workers the building was completed before the fall semester of 1879.': array([[ 0.00746889, -0.02705424,  0.12729011, ...,  0.01355074,\n",
       "          0.00698159,  0.00856812]], dtype=float32),\n",
       " 'Around the time of the fire, a music hall was opened.': array([[ 0.00746889, -0.07012937,  0.15404797, ...,  0.00395871,\n",
       "         -0.01429992, -0.01385639]], dtype=float32),\n",
       " 'By 1880, a science program was established at the university, and a Science Hall (today LaFortune Student Center) was built in 1883.': array([[ 0.00746889, -0.06304183,  0.09698144, ...,  0.05650637,\n",
       "          0.00150177, -0.00055695]], dtype=float32),\n",
       " 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?': array([[ 0.00746889,  0.02421027,  0.06961633, ...,  0.04414295,\n",
       "          0.06608193, -0.00488943]], dtype=float32),\n",
       " 'The Basilica of the Sacred heart at Notre Dame is beside to which structure?': array([[ 0.00746889, -0.04394472,  0.1438594 , ...,  0.00888265,\n",
       "          0.00347126,  0.00399774]], dtype=float32),\n",
       " 'What sits on top of the Main Building at Notre Dame?': array([[ 0.00746889, -0.02515129,  0.08926863, ...,  0.00978156,\n",
       "          0.03083676,  0.01530967]], dtype=float32),\n",
       " \"How often is Notre Dame's the Juggler published?\": array([[ 0.00746889, -0.03577625,  0.11397861, ..., -0.01846896,\n",
       "          0.03299245,  0.00343772]], dtype=float32),\n",
       " 'How many student news papers are found at Notre Dame?': array([[ 0.00746889, -0.02829786,  0.04031913, ...,  0.00978156,\n",
       "          0.03083676, -0.0259642 ]], dtype=float32),\n",
       " 'Where is the headquarters of the Congregation of the Holy Cross?': array([[ 0.00746889, -0.02924412,  0.09020641, ...,  0.00697632,\n",
       "          0.00153862,  0.00036549]], dtype=float32),\n",
       " 'What is the oldest structure at Notre Dame?': array([[ 0.00746889, -0.03032651,  0.11140621, ...,  0.00978156,\n",
       "          0.03083676, -0.01948634]], dtype=float32),\n",
       " 'Which prize did Frederick Buechner create?': array([[ 0.00746889,  0.02486045,  0.1207828 , ...,  0.04025421,\n",
       "          0.06308156, -0.0328564 ]], dtype=float32),\n",
       " 'In what year was the College of Engineering at Notre Dame formed?': array([[ 0.00746889, -0.02852931,  0.07033195, ...,  0.03481072,\n",
       "          0.01052457,  0.00344326]], dtype=float32),\n",
       " 'How many departments are within the Stinson-Remick Hall of Engineering?': array([[ 0.00746889, -0.06241583,  0.02947674, ..., -0.00423836,\n",
       "         -0.00863434, -0.01926273]], dtype=float32),\n",
       " 'What entity provides help with the management of time for new students at Notre Dame?': array([[ 0.00746889, -0.04499252,  0.11414099, ...,  0.00978156,\n",
       "          0.03083676, -0.01247187]], dtype=float32),\n",
       " 'What was created at Notre Dame in 1962 to assist first year students?': array([[ 0.00746889, -0.01427094,  0.12657568, ...,  0.0163363 ,\n",
       "          0.00734404,  0.00228944]], dtype=float32),\n",
       " 'The granting of Doctorate degrees first occurred in what year at Notre Dame?': array([[ 0.00746889, -0.04017805,  0.12960155, ...,  0.0430699 ,\n",
       "          0.03083676, -0.01532799]], dtype=float32),\n",
       " 'Which program at Notre Dame offers a Master of Education degree?': array([[ 0.00746889, -0.02865481,  0.13284664, ...,  0.01981544,\n",
       "          0.00112695, -0.0328564 ]], dtype=float32),\n",
       " 'Which department at Notre Dame is the only one to not offer a PhD program?': array([[ 0.00746889, -0.02108997,  0.02791445, ...,  0.0337334 ,\n",
       "          0.00792095, -0.02576153]], dtype=float32),\n",
       " \"What is the title of Notre Dame's Theodore Hesburgh?\": array([[ 0.00746889, -0.01793301,  0.09491997, ...,  0.0538058 ,\n",
       "          0.00743666, -0.0257602 ]], dtype=float32),\n",
       " 'To whom was John B. Kroc married?': array([[ 0.00746889,  0.11240683,  0.01878939, ...,  0.01528319,\n",
       "          0.09173459, -0.02761156]], dtype=float32),\n",
       " 'How many stories tall is the main library at Notre Dame?': array([[ 0.00746889, -0.04014478,  0.14084311, ...,  0.01320337,\n",
       "          0.03083676,  0.03696061]], dtype=float32),\n",
       " 'In what year was the Theodore M. Hesburgh Library at Notre Dame finished?': array([[ 0.00746889, -0.00951161,  0.10405501, ...,  0.036369  ,\n",
       "          0.03940555, -0.00214273]], dtype=float32),\n",
       " 'What is a common name to reference the mural created by Millard Sheets at Notre Dame?': array([[ 0.00746889, -0.03639939,  0.06932765, ...,  0.00978156,\n",
       "          0.05833782,  0.00959178]], dtype=float32),\n",
       " 'What percentage of students were admitted to Notre Dame in fall 2015?': array([[ 0.00746889, -0.03672401,  0.02552836, ...,  0.02587894,\n",
       "          0.0122702 ,  0.00782873]], dtype=float32),\n",
       " 'What percentage of students at Notre Dame participated in the Early Action program?': array([[ 0.00746889, -0.03201317,  0.04604631, ...,  0.01468842,\n",
       "         -0.00288274, -0.01847472]], dtype=float32),\n",
       " 'Where did U.S. News & World Report rank Notre Dame in its 2015-2016 university rankings?': array([[ 0.00746889, -0.02924412,  0.06611128, ...,  0.04893788,\n",
       "          0.03205158, -0.00732894]], dtype=float32),\n",
       " 'The undergrad school at the Mendoza College of Business was ranked where according to BusinessWeek?': array([[ 0.00746889, -0.06945133,  0.09441994, ...,  0.03846572,\n",
       "          0.0946383 , -0.01567586]], dtype=float32),\n",
       " 'What percentage of Notre Dame students decide to study abroad?': array([[ 0.00746889, -0.04161699,  0.04268396, ...,  0.01445818,\n",
       "          0.04993702, -0.01019925]], dtype=float32),\n",
       " 'What was the lifespan of John Augustine Zahm?': array([[ 0.00746889,  0.10183143,  0.07826509, ..., -0.01116473,\n",
       "          0.01602325, -0.01753249]], dtype=float32),\n",
       " 'What book did John Zahm write in 1896?': array([[ 0.00746889,  0.11061182,  0.04652998, ...,  0.04756273,\n",
       "          0.02002252, -0.02438906]], dtype=float32),\n",
       " 'In what year did Albert Zahm begin comparing aeronatical models at Notre Dame?': array([[ 0.00746889,  0.0298842 ,  0.09122062, ...,  0.05571805,\n",
       "          0.03083675, -0.00535188]], dtype=float32),\n",
       " 'In what year did Jerome Green send his first wireless message?': array([[ 0.00746889,  0.03787824,  0.02608781, ...,  0.05238663,\n",
       "          0.06191304, -0.01541477]], dtype=float32),\n",
       " 'What did the brother of John Zahm construct at Notre Dame?': array([[ 0.00746889,  0.1302837 ,  0.03671796, ...,  0.04021671,\n",
       "          0.0318032 , -0.02506638]], dtype=float32),\n",
       " 'Around what time did Lobund of Notre Dame become independent?': array([[ 0.00746889, -0.03934406,  0.03138258, ...,  0.04406903,\n",
       "          0.03395665, -0.03285642]], dtype=float32),\n",
       " 'The Lobund Institute was merged into the Department of Biology at Notre Dame in what year?': array([[ 0.00746889, -0.03666215,  0.13908216, ...,  0.03859048,\n",
       "          0.01694525, -0.01260512]], dtype=float32),\n",
       " 'Gurian created what in 1939 at Notre Dame?': array([[0.00746889, 0.04119827, 0.09515359, ..., 0.00978156, 0.03083676,\n",
       "         0.03266636]], dtype=float32),\n",
       " 'Over how many years did Gurian edit the Review of Politics at Notre Dame?': array([[0.00746889, 0.03404715, 0.10809186, ..., 0.04358914, 0.03083676,\n",
       "         0.00835457]], dtype=float32),\n",
       " 'Who was the president of Notre Dame in 2012?': array([[ 0.00746889, -0.02809111,  0.04165757, ...,  0.06237934,\n",
       "          0.02245097, -0.01277552]], dtype=float32),\n",
       " 'What does the Kroc Institute at Notre Dame focus on?': array([[ 0.00746889, -0.02702881,  0.04188842, ...,  0.02741455,\n",
       "          0.07774848, -0.02305052]], dtype=float32),\n",
       " 'What threat does the Global Adaptation Index study?': array([[ 0.00746889, -0.05823731, -0.01057927, ...,  0.02447836,\n",
       "         -0.01896318, -0.01725734]], dtype=float32),\n",
       " 'What percentage of students at Notre Dame are the children of former Notre Dame students?': array([[ 0.00746889, -0.03201317,  0.04342808, ...,  0.00890699,\n",
       "          0.01999341, -0.01783857]], dtype=float32),\n",
       " 'For what cause is money raised at the Bengal Bouts tournament at Notre Dame?': array([[ 0.00746889, -0.02688975,  0.03612492, ...,  0.00978156,\n",
       "          0.03083676, -0.02664975]], dtype=float32),\n",
       " 'What percentage of undergrads live on the Notre Dame campus?': array([[ 0.00746889, -0.03200205,  0.02448026, ..., -0.00486517,\n",
       "          0.02415315, -0.01647669]], dtype=float32),\n",
       " 'How many dorms for males are on the Notre Dame campus?': array([[ 0.00746889,  0.24162279,  0.02947674, ..., -0.01178548,\n",
       "          0.02415315, -0.01647669]], dtype=float32),\n",
       " 'There are how many dorms for females at Notre Dame?': array([[ 0.00746889, -0.04988378,  0.03838776, ...,  0.00978156,\n",
       "          0.03083676, -0.01487821]], dtype=float32),\n",
       " 'What percentage of Notre Dame students feel they are Christian?': array([[ 0.00746889,  0.01084262,  0.04268396, ..., -0.01378006,\n",
       "          0.00972779, -0.0191562 ]], dtype=float32),\n",
       " 'How many chapels are on the Notre Dame campus?': array([[ 0.00746889, -0.0291071 ,  0.08744341, ..., -0.01178548,\n",
       "          0.02415315, -0.00300565]], dtype=float32),\n",
       " 'What was the music hall at Notre Dame called?': array([[ 0.00746889, -0.03046083,  0.1653848 , ...,  0.00449075,\n",
       "          0.01920301, -0.01774   ]], dtype=float32),\n",
       " 'Who was the president of Notre Dame in 1879?': array([[ 0.00746889, -0.02809111,  0.08900622, ...,  0.06036935,\n",
       "          0.0220595 , -0.02098578]], dtype=float32),\n",
       " 'What did the Science Hall at Notre Dame come to be known as?': array([[ 0.00746889, -0.02819136,  0.04613955, ...,  0.01474535,\n",
       "          0.00606541, -0.01437634]], dtype=float32),\n",
       " 'Which college did Notre Dame add in 1921?': array([[ 0.00746889, -0.03792632,  0.04603584, ...,  0.04102056,\n",
       "          0.023051  , -0.03285642]], dtype=float32),\n",
       " 'Over how many years did the change to national standards undertaken at Notre Dame in the early 20th century take place?': array([[ 0.00746889, -0.02756433,  0.03629762, ...,  0.03675203,\n",
       "          0.03054413,  0.00924837]], dtype=float32),\n",
       " 'The Notre Dame football team got a new head coach in 1918, who was it?': array([[ 0.00746889,  0.03666878,  0.04968944, ...,  0.04544953,\n",
       "          0.00543485, -0.02250037]], dtype=float32),\n",
       " 'In what year did the team lead by Knute Rockne win the Rose Bowl?': array([[ 0.00746889, -0.03403048,  0.11133788, ...,  0.04644712,\n",
       "          0.05408155, -0.00110556]], dtype=float32),\n",
       " 'How many national titles were won when Knute Rockne coached at Notre Dame?': array([[ 0.00746889, -0.03975635,  0.09032594, ...,  0.04049398,\n",
       "          0.03083676, -0.02894759]], dtype=float32),\n",
       " 'Notre Dame students had a showdown in 1924 with which anti-catholic group?': array([[ 0.00746889, -0.02886533,  0.06218977, ...,  0.0119959 ,\n",
       "          0.00370005, -0.00610876]], dtype=float32),\n",
       " 'What type of event did the Klan intend to have at Notre Dame in March of 1924?': array([[ 0.00746889, -0.03657004,  0.05053801, ...,  0.02039495,\n",
       "          0.00814689, -0.00243716]], dtype=float32),\n",
       " 'Which person became vice-president of Notre Dame in 1933?': array([[ 0.00746889, -0.01576834,  0.05593691, ...,  0.04443996,\n",
       "          0.02620146,  0.00623206]], dtype=float32),\n",
       " 'Irvin Abell was given what award by Notre Dame?': array([[ 0.00746889,  0.0195189 ,  0.13862814, ...,  0.00542824,\n",
       "          0.05771532, -0.01559607]], dtype=float32),\n",
       " \"For whos glory did Father O'Hara believed that the Notre Dame football team played?\": array([[ 0.00746889,  0.1573193 ,  0.05544842, ...,  0.05307491,\n",
       "          0.00663593, -0.00802205]], dtype=float32),\n",
       " 'Which institute involving animal life did Cavanaugh create at Notre Dame?': array([[ 0.00746889, -0.03513959,  0.04516904, ...,  0.04806768,\n",
       "          0.03083676, -0.01875789]], dtype=float32),\n",
       " 'Which president did Notre Dame have in 1947?': array([[ 0.00746889, -0.02612887,  0.03435066, ...,  0.08182936,\n",
       "          0.01435611, -0.01448653]], dtype=float32),\n",
       " 'What was the lifespan of Theodore Hesburgh?': array([[ 0.00746889, -0.02325978,  0.09906805, ...,  0.05380579,\n",
       "         -0.00605288, -0.01846725]], dtype=float32),\n",
       " 'What was the size of the Notre Dame endowment when Theodore Hesburgh became president?': array([[ 0.00746889, -0.03369594,  0.09312441, ...,  0.06317903,\n",
       "          0.00337314, -0.01202231]], dtype=float32),\n",
       " 'How many faculty members were at Notre Dame when Hesburgh left the role of president?': array([[ 0.00746889, -0.02357643,  0.10709711, ...,  0.06317903,\n",
       "          0.00376196, -0.02838633]], dtype=float32),\n",
       " 'Which role did Charles Sheedy have at Notre Dame?': array([[ 0.00746889,  0.05476302,  0.04635876, ...,  0.04214235,\n",
       "          0.03083676, -0.02894759]], dtype=float32),\n",
       " 'In what year did Notre Dame have its earliest undergraduate that was female?': array([[ 0.00746889,  0.04234634,  0.03504913, ...,  0.05010844,\n",
       "         -0.00092466, -0.00849015]], dtype=float32),\n",
       " 'During what years was Edward Malloy president of Notre Dame?': array([[ 0.00746889,  0.04087214,  0.11759114, ...,  0.06009026,\n",
       "          0.03083676, -0.00587898]], dtype=float32),\n",
       " 'When Malloy became president of Notre Dame what was the size of the endowment?': array([[ 0.00746889, -0.02486669,  0.08963089, ...,  0.06336912,\n",
       "          0.00702003, -0.00417425]], dtype=float32),\n",
       " 'The amount of professors at Notre Dame increased by what amount under Malloy?': array([[ 0.00746889, -0.02901333,  0.04467758, ...,  0.06502164,\n",
       "          0.0110885 , -0.00559668]], dtype=float32),\n",
       " 'In terms of the amount of presidents Notre Dame has had, where is John Jenkins on the list?': array([[ 0.00746889,  0.09027049,  0.03945574, ...,  0.08703002,\n",
       "          0.04137409, -0.01946839]], dtype=float32),\n",
       " 'Which arena was constructed under Jenkins at Notre Dame?': array([[ 0.00746889, -0.02583336,  0.06256244, ...,  0.00978156,\n",
       "          0.03083676,  0.01047458]], dtype=float32),\n",
       " 'Which congregation is in charge of the Old College at Notre Dame?': array([[ 0.00746889, -0.02708023,  0.0801167 , ...,  0.00978156,\n",
       "          0.03083675, -0.02894759]], dtype=float32),\n",
       " 'In which architectural style is the Basilica of the Sacred Heart at Notre Dame made?': array([[ 0.00746889, -0.04596859,  0.16487856, ...,  0.00247537,\n",
       "          0.01904789,  0.02944009]], dtype=float32),\n",
       " 'In what year was the Grotto of Our Lady of Lourdes at Notre Dame constructed?': array([[ 0.00746889, -0.02724864,  0.10104389, ...,  0.02773497,\n",
       "          0.0649344 ,  0.02938747]], dtype=float32),\n",
       " 'In what year did the student union building at Notre Dame get renamed to LaFortune Center?': array([[ 0.00746889,  0.02638008,  0.10283346, ...,  0.05237963,\n",
       "          0.02385461, -0.01229274]], dtype=float32),\n",
       " 'How large in square feet is the LaFortune Center at Notre Dame?': array([[ 0.00746889, -0.03690489,  0.11881611, ...,  0.00978156,\n",
       "          0.03083675, -0.00091587]], dtype=float32),\n",
       " 'How many halls are at Notre Dame that house students?': array([[ 0.00746889, -0.0253663 ,  0.0358339 , ...,  0.00589024,\n",
       "          0.02282035, -0.02007075]], dtype=float32),\n",
       " 'How many books are housed at the Theodore Hesburgh Library?': array([[ 0.00746889, -0.00914819,  0.10100544, ...,  0.02586384,\n",
       "         -0.0083923 , -0.03142101]], dtype=float32),\n",
       " 'Which baseball stadium is found at Notre Dame?': array([[ 0.00746889, -0.02855213,  0.03761207, ...,  0.0152971 ,\n",
       "          0.03083676, -0.00476031]], dtype=float32),\n",
       " 'What percentage of the food served at Notre Dame is locally grown?': array([[ 0.00746889, -0.01949697,  0.05998648, ...,  0.00857478,\n",
       "          0.00394444, -0.02125575]], dtype=float32),\n",
       " 'Gustavo Gutierrez is faculty of which institute?': array([[ 0.00746889,  0.00184472,  0.10162169, ...,  0.06439325,\n",
       "         -0.01485598, -0.00931185]], dtype=float32),\n",
       " 'At which location is the London Center operated by Notre Dame found?': array([[ 0.00746889, -0.01718752,  0.10048228, ...,  0.00500942,\n",
       "          0.04618065,  0.00447392]], dtype=float32),\n",
       " 'In what year did the Suffolk Street location start to house a Notre Dame facility?': array([[ 0.00746889, -0.03403048,  0.0375096 , ...,  0.04639708,\n",
       "          0.0342476 , -0.01526696]], dtype=float32),\n",
       " 'In what year was the The College of Arts and Letters at Notre Dame created?': array([[ 0.00746889, -0.02722234,  0.13951297, ...,  0.03268814,\n",
       "          0.05379793, -0.00424433]], dtype=float32),\n",
       " 'On which university did Notre Dame base its curriculum on?': array([[ 0.00746889, -0.04257369,  0.02994821, ...,  0.0627885 ,\n",
       "          0.00191284,  0.01524277]], dtype=float32),\n",
       " 'Which president at Notre Dame created the College of Science?': array([[ 0.00746889, -0.01312777,  0.08613379, ...,  0.07405501,\n",
       "          0.00665602, -0.01908303]], dtype=float32),\n",
       " 'How many years long was a scientific course under Patrick Dillon at Notre Dame?': array([[ 0.00746889, -0.00137596,  0.05267136, ...,  0.0363179 ,\n",
       "          0.03083676,  0.00057334]], dtype=float32),\n",
       " 'How many undergrad students attend the College of Science at Notre Dame today?': array([[ 0.00746889, -0.04393848,  0.07216544, ...,  0.00548971,\n",
       "          0.03236039, -0.00918912]], dtype=float32),\n",
       " 'In what building is the current School of Architecture housed at Notre Dame?': array([[ 0.00746889, -0.01478573,  0.08153841, ...,  0.00978156,\n",
       "          0.03083676, -0.00707509]], dtype=float32),\n",
       " 'In which location do students of the School of Architecture of Notre Dame spend their 3rd year?': array([[ 0.00746889, -0.03227853,  0.08473271, ...,  0.01927255,\n",
       "          0.0087087 , -0.00226784]], dtype=float32),\n",
       " 'In what year did the opening of a theology library at Notre Dame occur?': array([[ 0.00746889, -0.03403048,  0.07453269, ...,  0.04878908,\n",
       "          0.02481688, -0.01468862]], dtype=float32),\n",
       " 'How many books are held by the Notre Dame libraries?': array([[ 0.00746889, -0.02704467,  0.09521664, ...,  0.00826003,\n",
       "          0.03359633, -0.03285641]], dtype=float32),\n",
       " 'What caused many intellectual Catholics to leave europe in the 1930s?': array([[ 0.00746889, -0.0137989 , -0.00168686, ..., -0.01215739,\n",
       "         -0.01591346, -0.01338389]], dtype=float32),\n",
       " 'What field of study did Anton-Hermann Chroust specialize in?': array([[ 0.00746889, -0.05823731,  0.10605995, ...,  0.03462304,\n",
       "         -0.0356206 , -0.03285642]], dtype=float32)}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file and dump the word embeddings in d1 in file\n",
    "with open('data/dict_embeddings1.pickle', 'wb') as handle:\n",
    "    pickle.dump(d1, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file and dump the word embeddings in d2 in file\n",
    "with open('data/dict_embeddings2.pickle', 'wb') as handle:\n",
    "    pickle.dump(d2, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dict_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/train.csv\") # Load the train data CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87599, 4)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Embedding dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the pickle file 1 with word embeddings and load to d1\n",
    "with open(\"data/dict_1.pickle\", \"rb\") as f:\n",
    "    d1 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the pickle file 2 with word embeddings and load to d2\n",
    "with open(\"data/dict_2.pickle\", \"rb\") as f:\n",
    "    d2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the two dictionaries with word embeddings in one dict_emb\n",
    "dict_emb = dict(d1)\n",
    "dict_emb.update(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179862"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dict_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "del d1, d2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function sets the value of column Target which is the sentence number in the paragraph which contains the answer.\n",
    "def get_target(x):\n",
    "    idx = -1\n",
    "    for i in range(len(x[\"sentences\"])):\n",
    "        if x[\"text\"] in x[\"sentences\"][i]: idx = i\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
       "      <td>515</td>\n",
       "      <td>Saint Bernadette Soubirous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What is in front of the Notre Dame Main Building?</td>\n",
       "      <td>188</td>\n",
       "      <td>a copper statue of Christ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
       "      <td>279</td>\n",
       "      <td>the Main Building</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context  \\\n",
       "0  Architecturally, the school has a Catholic cha...   \n",
       "1  Architecturally, the school has a Catholic cha...   \n",
       "2  Architecturally, the school has a Catholic cha...   \n",
       "\n",
       "                                            question  answer_start  \\\n",
       "0  To whom did the Virgin Mary allegedly appear i...           515   \n",
       "1  What is in front of the Notre Dame Main Building?           188   \n",
       "2  The Basilica of the Sacred heart at Notre Dame...           279   \n",
       "\n",
       "                         text  \n",
       "0  Saint Bernadette Soubirous  \n",
       "1   a copper statue of Christ  \n",
       "2           the Main Building  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87599, 4)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use of dropna function to drop null values using inplace=True to make the changes in CSV file automatic \n",
    "train.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function to add the extra columns to the training data: sentences, target, sent_emb, quest_emb\n",
    "def process_data(train):\n",
    "    \n",
    "    print(\"step 1\")\n",
    "    train['sentences'] = train['context'].apply(lambda x: [item.raw for item in TextBlob(x).sentences])\n",
    "    \n",
    "    print(\"step 2\")\n",
    "    train[\"target\"] = train.apply(get_target, axis = 1)\n",
    "    \n",
    "    print(\"step 3\")\n",
    "    train['sent_emb'] = train['sentences'].apply(lambda x: [dict_emb[item][0] if item in\n",
    "                                                           dict_emb else np.zeros(4096) for item in x])\n",
    "    print(\"step 4\")\n",
    "    train['quest_emb'] = train['question'].apply(lambda x: dict_emb[x] if x in dict_emb else np.zeros(4096) )\n",
    "        \n",
    "    return train   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1\n",
      "step 2\n",
      "step 3\n",
      "step 4\n"
     ]
    }
   ],
   "source": [
    "# Run this to process data to add columns \n",
    "train = process_data(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87598, 8)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>text</th>\n",
       "      <th>sentences</th>\n",
       "      <th>target</th>\n",
       "      <th>sent_emb</th>\n",
       "      <th>quest_emb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
       "      <td>515</td>\n",
       "      <td>Saint Bernadette Soubirous</td>\n",
       "      <td>[Architecturally, the school has a Catholic ch...</td>\n",
       "      <td>5</td>\n",
       "      <td>[[0.0074688885, -0.050863117, 0.007364763, -0....</td>\n",
       "      <td>[[0.0074688885, 0.024210272, 0.069616325, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What is in front of the Notre Dame Main Building?</td>\n",
       "      <td>188</td>\n",
       "      <td>a copper statue of Christ</td>\n",
       "      <td>[Architecturally, the school has a Catholic ch...</td>\n",
       "      <td>2</td>\n",
       "      <td>[[0.0074688885, -0.050863117, 0.007364763, -0....</td>\n",
       "      <td>[[0.0074688885, -0.033483382, 0.040545918, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
       "      <td>279</td>\n",
       "      <td>the Main Building</td>\n",
       "      <td>[Architecturally, the school has a Catholic ch...</td>\n",
       "      <td>3</td>\n",
       "      <td>[[0.0074688885, -0.050863117, 0.007364763, -0....</td>\n",
       "      <td>[[0.0074688885, -0.043944724, 0.14385942, -0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context  \\\n",
       "0  Architecturally, the school has a Catholic cha...   \n",
       "1  Architecturally, the school has a Catholic cha...   \n",
       "2  Architecturally, the school has a Catholic cha...   \n",
       "\n",
       "                                            question  answer_start  \\\n",
       "0  To whom did the Virgin Mary allegedly appear i...           515   \n",
       "1  What is in front of the Notre Dame Main Building?           188   \n",
       "2  The Basilica of the Sacred heart at Notre Dame...           279   \n",
       "\n",
       "                         text  \\\n",
       "0  Saint Bernadette Soubirous   \n",
       "1   a copper statue of Christ   \n",
       "2           the Main Building   \n",
       "\n",
       "                                           sentences  target  \\\n",
       "0  [Architecturally, the school has a Catholic ch...       5   \n",
       "1  [Architecturally, the school has a Catholic ch...       2   \n",
       "2  [Architecturally, the school has a Catholic ch...       3   \n",
       "\n",
       "                                            sent_emb  \\\n",
       "0  [[0.0074688885, -0.050863117, 0.007364763, -0....   \n",
       "1  [[0.0074688885, -0.050863117, 0.007364763, -0....   \n",
       "2  [[0.0074688885, -0.050863117, 0.007364763, -0....   \n",
       "\n",
       "                                           quest_emb  \n",
       "0  [[0.0074688885, 0.024210272, 0.069616325, -0.0...  \n",
       "1  [[0.0074688885, -0.033483382, 0.040545918, -0....  \n",
       "2  [[0.0074688885, -0.043944724, 0.14385942, -0.0...  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicted Cosine & Euclidean Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the Cosine similarity Between Question and Answer \n",
    "def cosine_sim(x):\n",
    "    li = []\n",
    "    for item in x[\"sent_emb\"]:\n",
    "        li.append(spatial.distance.cosine(item,x[\"quest_emb\"][0]))\n",
    "    return li   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function is to find the argmin between all the distances of a single sentence array to find the predicted target ...\n",
    "#sentence number\n",
    "def pred_idx(distances):\n",
    "    return np.argmin(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The euclidean distance and cosine similarity are calculated and predictions are made using above function and stored \n",
    "def predictions(train):\n",
    "    \n",
    "    train[\"cosine_sim\"] = train.apply(cosine_sim, axis = 1)\n",
    "    train[\"diff\"] = (train[\"quest_emb\"] - train[\"sent_emb\"])**2\n",
    "    train[\"euclidean_dis\"] = train[\"diff\"].apply(lambda x: list(np.sum(x, axis = 1)))\n",
    "    del train[\"diff\"]\n",
    "    \n",
    "    print(\"cosine start\")\n",
    "    \n",
    "    train[\"pred_idx_cos\"] = train[\"cosine_sim\"].apply(lambda x: pred_idx(x))\n",
    "    train[\"pred_idx_euc\"] = train[\"euclidean_dis\"].apply(lambda x: pred_idx(x))\n",
    "    \n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine start\n"
     ]
    }
   ],
   "source": [
    "#Call to make the predictions\n",
    "predicted = predictions(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87598, 12)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>text</th>\n",
       "      <th>sentences</th>\n",
       "      <th>target</th>\n",
       "      <th>sent_emb</th>\n",
       "      <th>quest_emb</th>\n",
       "      <th>cosine_sim</th>\n",
       "      <th>euclidean_dis</th>\n",
       "      <th>pred_idx_cos</th>\n",
       "      <th>pred_idx_euc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
       "      <td>515</td>\n",
       "      <td>Saint Bernadette Soubirous</td>\n",
       "      <td>[Architecturally, the school has a Catholic ch...</td>\n",
       "      <td>5</td>\n",
       "      <td>[[0.0074688885, -0.050863117, 0.007364763, -0....</td>\n",
       "      <td>[[0.0074688885, 0.024210272, 0.069616325, -0.0...</td>\n",
       "      <td>[0.6835565567016602, 0.5527453124523163, 0.574...</td>\n",
       "      <td>[7.301305, 6.4764132, 7.214262, 6.9176197, 6.5...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What is in front of the Notre Dame Main Building?</td>\n",
       "      <td>188</td>\n",
       "      <td>a copper statue of Christ</td>\n",
       "      <td>[Architecturally, the school has a Catholic ch...</td>\n",
       "      <td>2</td>\n",
       "      <td>[[0.0074688885, -0.050863117, 0.007364763, -0....</td>\n",
       "      <td>[[0.0074688885, -0.033483382, 0.040545918, -0....</td>\n",
       "      <td>[0.6624992787837982, 0.5191770792007446, 0.592...</td>\n",
       "      <td>[5.717004, 5.076439, 6.3408985, 3.5867877, 4.8...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
       "      <td>279</td>\n",
       "      <td>the Main Building</td>\n",
       "      <td>[Architecturally, the school has a Catholic ch...</td>\n",
       "      <td>3</td>\n",
       "      <td>[[0.0074688885, -0.050863117, 0.007364763, -0....</td>\n",
       "      <td>[[0.0074688885, -0.043944724, 0.14385942, -0.0...</td>\n",
       "      <td>[0.6164608299732208, 0.4940056800842285, 0.521...</td>\n",
       "      <td>[5.3495297, 4.8541417, 5.6286764, 2.8828857, 3...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context  \\\n",
       "0  Architecturally, the school has a Catholic cha...   \n",
       "1  Architecturally, the school has a Catholic cha...   \n",
       "2  Architecturally, the school has a Catholic cha...   \n",
       "\n",
       "                                            question  answer_start  \\\n",
       "0  To whom did the Virgin Mary allegedly appear i...           515   \n",
       "1  What is in front of the Notre Dame Main Building?           188   \n",
       "2  The Basilica of the Sacred heart at Notre Dame...           279   \n",
       "\n",
       "                         text  \\\n",
       "0  Saint Bernadette Soubirous   \n",
       "1   a copper statue of Christ   \n",
       "2           the Main Building   \n",
       "\n",
       "                                           sentences  target  \\\n",
       "0  [Architecturally, the school has a Catholic ch...       5   \n",
       "1  [Architecturally, the school has a Catholic ch...       2   \n",
       "2  [Architecturally, the school has a Catholic ch...       3   \n",
       "\n",
       "                                            sent_emb  \\\n",
       "0  [[0.0074688885, -0.050863117, 0.007364763, -0....   \n",
       "1  [[0.0074688885, -0.050863117, 0.007364763, -0....   \n",
       "2  [[0.0074688885, -0.050863117, 0.007364763, -0....   \n",
       "\n",
       "                                           quest_emb  \\\n",
       "0  [[0.0074688885, 0.024210272, 0.069616325, -0.0...   \n",
       "1  [[0.0074688885, -0.033483382, 0.040545918, -0....   \n",
       "2  [[0.0074688885, -0.043944724, 0.14385942, -0.0...   \n",
       "\n",
       "                                          cosine_sim  \\\n",
       "0  [0.6835565567016602, 0.5527453124523163, 0.574...   \n",
       "1  [0.6624992787837982, 0.5191770792007446, 0.592...   \n",
       "2  [0.6164608299732208, 0.4940056800842285, 0.521...   \n",
       "\n",
       "                                       euclidean_dis  pred_idx_cos  \\\n",
       "0  [7.301305, 6.4764132, 7.214262, 6.9176197, 6.5...             5   \n",
       "1  [5.717004, 5.076439, 6.3408985, 3.5867877, 4.8...             3   \n",
       "2  [5.3495297, 4.8541417, 5.6286764, 2.8828857, 3...             3   \n",
       "\n",
       "   pred_idx_euc  \n",
       "0             5  \n",
       "1             3  \n",
       "2             3  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6835565567016602,\n",
       " 0.5527453124523163,\n",
       " 0.5747938454151154,\n",
       " 0.624308854341507,\n",
       " 0.619026243686676,\n",
       " 0.25835102796554565,\n",
       " 0.5743658542633057]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted[\"cosine_sim\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7.301305, 6.4764132, 7.214262, 6.9176197, 6.519748, 3.6410067, 6.9080434]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted[\"euclidean_dis\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy between predicted and target values\n",
    "def accuracy(target, predicted):\n",
    "    acc = (target==predicted).sum()/len(target)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy for  euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.488778282609192\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(predicted[\"target\"], predicted[\"pred_idx_euc\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy for Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5972282472202561\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(predicted[\"target\"], predicted[\"pred_idx_cos\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write back the final data with predicted values to a CSV file\n",
    "predicted.to_csv(\"train_detect_sent.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the new CSV file\n",
    "predicted = pd.read_csv(\"train_detect_sent.csv\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use spaCy Open source Library to load the data needed for to process for English Language\n",
    "en_nlp=spacy.load('en')\n",
    "doc = en_nlp(predicted.iloc[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>text</th>\n",
       "      <th>sentences</th>\n",
       "      <th>target</th>\n",
       "      <th>sent_emb</th>\n",
       "      <th>quest_emb</th>\n",
       "      <th>cosine_sim</th>\n",
       "      <th>euclidean_dis</th>\n",
       "      <th>pred_idx_cos</th>\n",
       "      <th>pred_idx_euc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
       "      <td>515</td>\n",
       "      <td>Saint Bernadette Soubirous</td>\n",
       "      <td>['Architecturally, the school has a Catholic c...</td>\n",
       "      <td>5</td>\n",
       "      <td>[array([ 0.00746889, -0.05086312,  0.00736476,...</td>\n",
       "      <td>[[ 0.00746889  0.02421027  0.06961633 ...  0.0...</td>\n",
       "      <td>[0.6835565567016602, 0.5527453124523163, 0.574...</td>\n",
       "      <td>[7.301305, 6.4764132, 7.214262, 6.9176197, 6.5...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What is in front of the Notre Dame Main Building?</td>\n",
       "      <td>188</td>\n",
       "      <td>a copper statue of Christ</td>\n",
       "      <td>['Architecturally, the school has a Catholic c...</td>\n",
       "      <td>2</td>\n",
       "      <td>[array([ 0.00746889, -0.05086312,  0.00736476,...</td>\n",
       "      <td>[[ 0.00746889 -0.03348338  0.04054592 ... -0.0...</td>\n",
       "      <td>[0.6624992787837982, 0.5191770792007446, 0.592...</td>\n",
       "      <td>[5.717004, 5.076439, 6.3408985, 3.5867877, 4.8...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
       "      <td>279</td>\n",
       "      <td>the Main Building</td>\n",
       "      <td>['Architecturally, the school has a Catholic c...</td>\n",
       "      <td>3</td>\n",
       "      <td>[array([ 0.00746889, -0.05086312,  0.00736476,...</td>\n",
       "      <td>[[ 0.00746889 -0.04394472  0.14385942 ...  0.0...</td>\n",
       "      <td>[0.6164608299732208, 0.4940056800842285, 0.521...</td>\n",
       "      <td>[5.3495297, 4.8541417, 5.6286764, 2.8828857, 3...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What is the Grotto at Notre Dame?</td>\n",
       "      <td>381</td>\n",
       "      <td>a Marian place of prayer and reflection</td>\n",
       "      <td>['Architecturally, the school has a Catholic c...</td>\n",
       "      <td>4</td>\n",
       "      <td>[array([ 0.00746889, -0.05086312,  0.00736476,...</td>\n",
       "      <td>[[ 0.00746889 -0.02113509  0.08985032 ...  0.0...</td>\n",
       "      <td>[0.6685060262680054, 0.6087148487567902, 0.695...</td>\n",
       "      <td>[6.2550764, 6.3554144, 7.8789406, 4.5443306, 4...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What sits on top of the Main Building at Notre...</td>\n",
       "      <td>92</td>\n",
       "      <td>a golden statue of the Virgin Mary</td>\n",
       "      <td>['Architecturally, the school has a Catholic c...</td>\n",
       "      <td>1</td>\n",
       "      <td>[array([ 0.00746889, -0.05086312,  0.00736476,...</td>\n",
       "      <td>[[ 0.00746889 -0.02515129  0.08926863 ...  0.0...</td>\n",
       "      <td>[0.7792749553918839, 0.528475284576416, 0.5735...</td>\n",
       "      <td>[7.5543714, 5.692802, 6.6871767, 4.9444075, 5....</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87593</th>\n",
       "      <td>Kathmandu Metropolitan City (KMC), in order to...</td>\n",
       "      <td>In what US state did Kathmandu first establish...</td>\n",
       "      <td>229</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>['Kathmandu Metropolitan City (KMC), in order ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[array([ 0.00746889, -0.05648201,  0.10629363,...</td>\n",
       "      <td>[[ 0.00746889 -0.03403048  0.06046114 ...  0.0...</td>\n",
       "      <td>[0.41991251707077026, 0.3826998472213745, 0.48...</td>\n",
       "      <td>[4.760933, 4.116004, 6.391081, 5.7619123]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87594</th>\n",
       "      <td>Kathmandu Metropolitan City (KMC), in order to...</td>\n",
       "      <td>What was Yangon previously known as?</td>\n",
       "      <td>414</td>\n",
       "      <td>Rangoon</td>\n",
       "      <td>['Kathmandu Metropolitan City (KMC), in order ...</td>\n",
       "      <td>2</td>\n",
       "      <td>[array([ 0.00746889, -0.05648201,  0.10629363,...</td>\n",
       "      <td>[[ 0.00746889 -0.05823731 -0.00707996 ... -0.0...</td>\n",
       "      <td>[0.7255948781967163, 0.698612630367279, 0.7624...</td>\n",
       "      <td>[7.763542, 7.0988116, 9.420061, 9.603617]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87595</th>\n",
       "      <td>Kathmandu Metropolitan City (KMC), in order to...</td>\n",
       "      <td>With what Belorussian city does Kathmandu have...</td>\n",
       "      <td>476</td>\n",
       "      <td>Minsk</td>\n",
       "      <td>['Kathmandu Metropolitan City (KMC), in order ...</td>\n",
       "      <td>2</td>\n",
       "      <td>[array([ 0.00746889, -0.05648201,  0.10629363,...</td>\n",
       "      <td>[[ 0.00746889 -0.05044248  0.06999452 ...  0.0...</td>\n",
       "      <td>[0.4843701124191284, 0.5179441869258881, 0.530...</td>\n",
       "      <td>[5.394357, 5.4583673, 6.8717723, 6.434966]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87596</th>\n",
       "      <td>Kathmandu Metropolitan City (KMC), in order to...</td>\n",
       "      <td>In what year did Kathmandu create its initial ...</td>\n",
       "      <td>199</td>\n",
       "      <td>1975</td>\n",
       "      <td>['Kathmandu Metropolitan City (KMC), in order ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[array([ 0.00746889, -0.05648201,  0.10629363,...</td>\n",
       "      <td>[[ 0.00746889 -0.03403048  0.05538646 ...  0.0...</td>\n",
       "      <td>[0.49502629041671753, 0.4206138253211975, 0.52...</td>\n",
       "      <td>[5.523418, 4.4599686, 6.844032, 6.0133524]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87597</th>\n",
       "      <td>Kathmandu Metropolitan City (KMC), in order to...</td>\n",
       "      <td>What is KMC an initialism of?</td>\n",
       "      <td>0</td>\n",
       "      <td>Kathmandu Metropolitan City</td>\n",
       "      <td>['Kathmandu Metropolitan City (KMC), in order ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[array([ 0.00746889, -0.05648201,  0.10629363,...</td>\n",
       "      <td>[[ 0.00746889 -0.05823731  0.08288754 ... -0.0...</td>\n",
       "      <td>[0.6605812907218933, 0.7128670513629913, 0.849...</td>\n",
       "      <td>[7.2158666, 7.3852386, 10.605681, 9.032009]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>87598 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 context  \\\n",
       "0      Architecturally, the school has a Catholic cha...   \n",
       "1      Architecturally, the school has a Catholic cha...   \n",
       "2      Architecturally, the school has a Catholic cha...   \n",
       "3      Architecturally, the school has a Catholic cha...   \n",
       "4      Architecturally, the school has a Catholic cha...   \n",
       "...                                                  ...   \n",
       "87593  Kathmandu Metropolitan City (KMC), in order to...   \n",
       "87594  Kathmandu Metropolitan City (KMC), in order to...   \n",
       "87595  Kathmandu Metropolitan City (KMC), in order to...   \n",
       "87596  Kathmandu Metropolitan City (KMC), in order to...   \n",
       "87597  Kathmandu Metropolitan City (KMC), in order to...   \n",
       "\n",
       "                                                question  answer_start  \\\n",
       "0      To whom did the Virgin Mary allegedly appear i...           515   \n",
       "1      What is in front of the Notre Dame Main Building?           188   \n",
       "2      The Basilica of the Sacred heart at Notre Dame...           279   \n",
       "3                      What is the Grotto at Notre Dame?           381   \n",
       "4      What sits on top of the Main Building at Notre...            92   \n",
       "...                                                  ...           ...   \n",
       "87593  In what US state did Kathmandu first establish...           229   \n",
       "87594               What was Yangon previously known as?           414   \n",
       "87595  With what Belorussian city does Kathmandu have...           476   \n",
       "87596  In what year did Kathmandu create its initial ...           199   \n",
       "87597                      What is KMC an initialism of?             0   \n",
       "\n",
       "                                          text  \\\n",
       "0                   Saint Bernadette Soubirous   \n",
       "1                    a copper statue of Christ   \n",
       "2                            the Main Building   \n",
       "3      a Marian place of prayer and reflection   \n",
       "4           a golden statue of the Virgin Mary   \n",
       "...                                        ...   \n",
       "87593                                   Oregon   \n",
       "87594                                  Rangoon   \n",
       "87595                                    Minsk   \n",
       "87596                                     1975   \n",
       "87597              Kathmandu Metropolitan City   \n",
       "\n",
       "                                               sentences  target  \\\n",
       "0      ['Architecturally, the school has a Catholic c...       5   \n",
       "1      ['Architecturally, the school has a Catholic c...       2   \n",
       "2      ['Architecturally, the school has a Catholic c...       3   \n",
       "3      ['Architecturally, the school has a Catholic c...       4   \n",
       "4      ['Architecturally, the school has a Catholic c...       1   \n",
       "...                                                  ...     ...   \n",
       "87593  ['Kathmandu Metropolitan City (KMC), in order ...       1   \n",
       "87594  ['Kathmandu Metropolitan City (KMC), in order ...       2   \n",
       "87595  ['Kathmandu Metropolitan City (KMC), in order ...       2   \n",
       "87596  ['Kathmandu Metropolitan City (KMC), in order ...       1   \n",
       "87597  ['Kathmandu Metropolitan City (KMC), in order ...       0   \n",
       "\n",
       "                                                sent_emb  \\\n",
       "0      [array([ 0.00746889, -0.05086312,  0.00736476,...   \n",
       "1      [array([ 0.00746889, -0.05086312,  0.00736476,...   \n",
       "2      [array([ 0.00746889, -0.05086312,  0.00736476,...   \n",
       "3      [array([ 0.00746889, -0.05086312,  0.00736476,...   \n",
       "4      [array([ 0.00746889, -0.05086312,  0.00736476,...   \n",
       "...                                                  ...   \n",
       "87593  [array([ 0.00746889, -0.05648201,  0.10629363,...   \n",
       "87594  [array([ 0.00746889, -0.05648201,  0.10629363,...   \n",
       "87595  [array([ 0.00746889, -0.05648201,  0.10629363,...   \n",
       "87596  [array([ 0.00746889, -0.05648201,  0.10629363,...   \n",
       "87597  [array([ 0.00746889, -0.05648201,  0.10629363,...   \n",
       "\n",
       "                                               quest_emb  \\\n",
       "0      [[ 0.00746889  0.02421027  0.06961633 ...  0.0...   \n",
       "1      [[ 0.00746889 -0.03348338  0.04054592 ... -0.0...   \n",
       "2      [[ 0.00746889 -0.04394472  0.14385942 ...  0.0...   \n",
       "3      [[ 0.00746889 -0.02113509  0.08985032 ...  0.0...   \n",
       "4      [[ 0.00746889 -0.02515129  0.08926863 ...  0.0...   \n",
       "...                                                  ...   \n",
       "87593  [[ 0.00746889 -0.03403048  0.06046114 ...  0.0...   \n",
       "87594  [[ 0.00746889 -0.05823731 -0.00707996 ... -0.0...   \n",
       "87595  [[ 0.00746889 -0.05044248  0.06999452 ...  0.0...   \n",
       "87596  [[ 0.00746889 -0.03403048  0.05538646 ...  0.0...   \n",
       "87597  [[ 0.00746889 -0.05823731  0.08288754 ... -0.0...   \n",
       "\n",
       "                                              cosine_sim  \\\n",
       "0      [0.6835565567016602, 0.5527453124523163, 0.574...   \n",
       "1      [0.6624992787837982, 0.5191770792007446, 0.592...   \n",
       "2      [0.6164608299732208, 0.4940056800842285, 0.521...   \n",
       "3      [0.6685060262680054, 0.6087148487567902, 0.695...   \n",
       "4      [0.7792749553918839, 0.528475284576416, 0.5735...   \n",
       "...                                                  ...   \n",
       "87593  [0.41991251707077026, 0.3826998472213745, 0.48...   \n",
       "87594  [0.7255948781967163, 0.698612630367279, 0.7624...   \n",
       "87595  [0.4843701124191284, 0.5179441869258881, 0.530...   \n",
       "87596  [0.49502629041671753, 0.4206138253211975, 0.52...   \n",
       "87597  [0.6605812907218933, 0.7128670513629913, 0.849...   \n",
       "\n",
       "                                           euclidean_dis  pred_idx_cos  \\\n",
       "0      [7.301305, 6.4764132, 7.214262, 6.9176197, 6.5...             5   \n",
       "1      [5.717004, 5.076439, 6.3408985, 3.5867877, 4.8...             3   \n",
       "2      [5.3495297, 4.8541417, 5.6286764, 2.8828857, 3...             3   \n",
       "3      [6.2550764, 6.3554144, 7.8789406, 4.5443306, 4...             3   \n",
       "4      [7.5543714, 5.692802, 6.6871767, 4.9444075, 5....             3   \n",
       "...                                                  ...           ...   \n",
       "87593          [4.760933, 4.116004, 6.391081, 5.7619123]             1   \n",
       "87594          [7.763542, 7.0988116, 9.420061, 9.603617]             1   \n",
       "87595         [5.394357, 5.4583673, 6.8717723, 6.434966]             0   \n",
       "87596         [5.523418, 4.4599686, 6.844032, 6.0133524]             1   \n",
       "87597        [7.2158666, 7.3852386, 10.605681, 9.032009]             0   \n",
       "\n",
       "       pred_idx_euc  \n",
       "0                 5  \n",
       "1                 3  \n",
       "2                 3  \n",
       "3                 3  \n",
       "4                 3  \n",
       "...             ...  \n",
       "87593             1  \n",
       "87594             1  \n",
       "87595             0  \n",
       "87596             1  \n",
       "87597             0  \n",
       "\n",
       "[87598 rows x 12 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87598, 12)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Architecturally, the school has a Catholic character.',\n",
       " \"Atop the Main Building's gold dome is a golden statue of the Virgin Mary.\",\n",
       " 'Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\".',\n",
       " 'Next to the Main Building is the Basilica of the Sacred Heart.',\n",
       " 'Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection.',\n",
       " 'It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858.',\n",
       " 'At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ast.literal_eval to print sentence-wise\n",
    "ast.literal_eval(predicted[\"sentences\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here all the paragraphs with number of sentences 10 or less are taken from data\n",
    "predicted = predicted[predicted[\"sentences\"].apply(lambda x: len(ast.literal_eval(x)))<11].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Every array value of cosine similarity and euclidean distance for each sentence are separated in columns as we have to work\n",
    "#on the numerical distance data and every Question Answer pair has array for euclidean distance and cosine similarity\n",
    "def create_features(predicted):\n",
    "    train = pd.DataFrame()\n",
    "     \n",
    "    for k in range(len(predicted[\"euclidean_dis\"])):\n",
    "        dis = ast.literal_eval(predicted[\"euclidean_dis\"][k])\n",
    "        for i in range(len(dis)):\n",
    "            train.loc[k, \"column_euc_\"+\"%s\"%i] = dis[i]\n",
    "    \n",
    "    print(\"Finished\")\n",
    "    \n",
    "    for k in range(len(predicted[\"cosine_sim\"])):\n",
    "        dis = ast.literal_eval(predicted[\"cosine_sim\"][k].replace(\"nan\",\"1\"))\n",
    "        for i in range(len(dis)):\n",
    "            train.loc[k, \"column_cos_\"+\"%s\"%i] = dis[i]\n",
    "            \n",
    "    train[\"target\"] = predicted[\"target\"]\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n"
     ]
    }
   ],
   "source": [
    "# Call to create features and convert distances to columns\n",
    "train = create_features(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_euc_0</th>\n",
       "      <th>column_euc_1</th>\n",
       "      <th>column_euc_2</th>\n",
       "      <th>column_euc_3</th>\n",
       "      <th>column_euc_4</th>\n",
       "      <th>column_euc_5</th>\n",
       "      <th>column_euc_6</th>\n",
       "      <th>column_euc_7</th>\n",
       "      <th>column_euc_8</th>\n",
       "      <th>column_euc_9</th>\n",
       "      <th>...</th>\n",
       "      <th>column_cos_1</th>\n",
       "      <th>column_cos_2</th>\n",
       "      <th>column_cos_3</th>\n",
       "      <th>column_cos_4</th>\n",
       "      <th>column_cos_5</th>\n",
       "      <th>column_cos_6</th>\n",
       "      <th>column_cos_7</th>\n",
       "      <th>column_cos_8</th>\n",
       "      <th>column_cos_9</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.301305</td>\n",
       "      <td>6.476413</td>\n",
       "      <td>7.214262</td>\n",
       "      <td>6.917620</td>\n",
       "      <td>6.519748</td>\n",
       "      <td>3.641007</td>\n",
       "      <td>6.908043</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.552745</td>\n",
       "      <td>0.574794</td>\n",
       "      <td>0.624309</td>\n",
       "      <td>0.619026</td>\n",
       "      <td>0.258351</td>\n",
       "      <td>0.574366</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.717004</td>\n",
       "      <td>5.076439</td>\n",
       "      <td>6.340898</td>\n",
       "      <td>3.586788</td>\n",
       "      <td>4.826955</td>\n",
       "      <td>7.294750</td>\n",
       "      <td>6.392016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.519177</td>\n",
       "      <td>0.592157</td>\n",
       "      <td>0.395893</td>\n",
       "      <td>0.572930</td>\n",
       "      <td>0.594844</td>\n",
       "      <td>0.632305</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.349530</td>\n",
       "      <td>4.854142</td>\n",
       "      <td>5.628676</td>\n",
       "      <td>2.882886</td>\n",
       "      <td>3.573071</td>\n",
       "      <td>6.292664</td>\n",
       "      <td>5.433945</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.494006</td>\n",
       "      <td>0.521783</td>\n",
       "      <td>0.316489</td>\n",
       "      <td>0.421474</td>\n",
       "      <td>0.506190</td>\n",
       "      <td>0.533841</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.255076</td>\n",
       "      <td>6.355414</td>\n",
       "      <td>7.878941</td>\n",
       "      <td>4.544331</td>\n",
       "      <td>4.577210</td>\n",
       "      <td>7.192090</td>\n",
       "      <td>7.930581</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.608715</td>\n",
       "      <td>0.695520</td>\n",
       "      <td>0.465277</td>\n",
       "      <td>0.498301</td>\n",
       "      <td>0.558794</td>\n",
       "      <td>0.736132</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.554371</td>\n",
       "      <td>5.692802</td>\n",
       "      <td>6.687177</td>\n",
       "      <td>4.944407</td>\n",
       "      <td>5.559352</td>\n",
       "      <td>7.806414</td>\n",
       "      <td>6.598026</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.528475</td>\n",
       "      <td>0.573556</td>\n",
       "      <td>0.488644</td>\n",
       "      <td>0.583152</td>\n",
       "      <td>0.594557</td>\n",
       "      <td>0.594187</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85114</th>\n",
       "      <td>4.760933</td>\n",
       "      <td>4.116004</td>\n",
       "      <td>6.391081</td>\n",
       "      <td>5.761912</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.382700</td>\n",
       "      <td>0.485660</td>\n",
       "      <td>0.461544</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85115</th>\n",
       "      <td>7.763542</td>\n",
       "      <td>7.098812</td>\n",
       "      <td>9.420061</td>\n",
       "      <td>9.603617</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.698613</td>\n",
       "      <td>0.762461</td>\n",
       "      <td>0.821555</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85116</th>\n",
       "      <td>5.394357</td>\n",
       "      <td>5.458367</td>\n",
       "      <td>6.871772</td>\n",
       "      <td>6.434966</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.517944</td>\n",
       "      <td>0.530936</td>\n",
       "      <td>0.525017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85117</th>\n",
       "      <td>5.523418</td>\n",
       "      <td>4.459969</td>\n",
       "      <td>6.844032</td>\n",
       "      <td>6.013352</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.420614</td>\n",
       "      <td>0.527569</td>\n",
       "      <td>0.487704</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85118</th>\n",
       "      <td>7.215867</td>\n",
       "      <td>7.385239</td>\n",
       "      <td>10.605681</td>\n",
       "      <td>9.032009</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.712867</td>\n",
       "      <td>0.849905</td>\n",
       "      <td>0.757005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>85119 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       column_euc_0  column_euc_1  column_euc_2  column_euc_3  column_euc_4  \\\n",
       "0          7.301305      6.476413      7.214262      6.917620      6.519748   \n",
       "1          5.717004      5.076439      6.340898      3.586788      4.826955   \n",
       "2          5.349530      4.854142      5.628676      2.882886      3.573071   \n",
       "3          6.255076      6.355414      7.878941      4.544331      4.577210   \n",
       "4          7.554371      5.692802      6.687177      4.944407      5.559352   \n",
       "...             ...           ...           ...           ...           ...   \n",
       "85114      4.760933      4.116004      6.391081      5.761912           NaN   \n",
       "85115      7.763542      7.098812      9.420061      9.603617           NaN   \n",
       "85116      5.394357      5.458367      6.871772      6.434966           NaN   \n",
       "85117      5.523418      4.459969      6.844032      6.013352           NaN   \n",
       "85118      7.215867      7.385239     10.605681      9.032009           NaN   \n",
       "\n",
       "       column_euc_5  column_euc_6  column_euc_7  column_euc_8  column_euc_9  \\\n",
       "0          3.641007      6.908043           NaN           NaN           NaN   \n",
       "1          7.294750      6.392016           NaN           NaN           NaN   \n",
       "2          6.292664      5.433945           NaN           NaN           NaN   \n",
       "3          7.192090      7.930581           NaN           NaN           NaN   \n",
       "4          7.806414      6.598026           NaN           NaN           NaN   \n",
       "...             ...           ...           ...           ...           ...   \n",
       "85114           NaN           NaN           NaN           NaN           NaN   \n",
       "85115           NaN           NaN           NaN           NaN           NaN   \n",
       "85116           NaN           NaN           NaN           NaN           NaN   \n",
       "85117           NaN           NaN           NaN           NaN           NaN   \n",
       "85118           NaN           NaN           NaN           NaN           NaN   \n",
       "\n",
       "       ...  column_cos_1  column_cos_2  column_cos_3  column_cos_4  \\\n",
       "0      ...      0.552745      0.574794      0.624309      0.619026   \n",
       "1      ...      0.519177      0.592157      0.395893      0.572930   \n",
       "2      ...      0.494006      0.521783      0.316489      0.421474   \n",
       "3      ...      0.608715      0.695520      0.465277      0.498301   \n",
       "4      ...      0.528475      0.573556      0.488644      0.583152   \n",
       "...    ...           ...           ...           ...           ...   \n",
       "85114  ...      0.382700      0.485660      0.461544           NaN   \n",
       "85115  ...      0.698613      0.762461      0.821555           NaN   \n",
       "85116  ...      0.517944      0.530936      0.525017           NaN   \n",
       "85117  ...      0.420614      0.527569      0.487704           NaN   \n",
       "85118  ...      0.712867      0.849905      0.757005           NaN   \n",
       "\n",
       "       column_cos_5  column_cos_6  column_cos_7  column_cos_8  column_cos_9  \\\n",
       "0          0.258351      0.574366           NaN           NaN           NaN   \n",
       "1          0.594844      0.632305           NaN           NaN           NaN   \n",
       "2          0.506190      0.533841           NaN           NaN           NaN   \n",
       "3          0.558794      0.736132           NaN           NaN           NaN   \n",
       "4          0.594557      0.594187           NaN           NaN           NaN   \n",
       "...             ...           ...           ...           ...           ...   \n",
       "85114           NaN           NaN           NaN           NaN           NaN   \n",
       "85115           NaN           NaN           NaN           NaN           NaN   \n",
       "85116           NaN           NaN           NaN           NaN           NaN   \n",
       "85117           NaN           NaN           NaN           NaN           NaN   \n",
       "85118           NaN           NaN           NaN           NaN           NaN   \n",
       "\n",
       "       target  \n",
       "0           5  \n",
       "1           2  \n",
       "2           3  \n",
       "3           4  \n",
       "4           1  \n",
       "...       ...  \n",
       "85114       1  \n",
       "85115       2  \n",
       "85116       2  \n",
       "85117       1  \n",
       "85118       0  \n",
       "\n",
       "[85119 rows x 21 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "column_euc_0    29.927914\n",
       "column_euc_1    40.290561\n",
       "column_euc_2    33.286404\n",
       "column_euc_3    34.140771\n",
       "column_euc_4    36.115491\n",
       "column_euc_5    35.623532\n",
       "column_euc_6    39.640774\n",
       "column_euc_7          NaN\n",
       "column_euc_8          NaN\n",
       "column_euc_9          NaN\n",
       "column_cos_0     1.491517\n",
       "column_cos_1     1.489619\n",
       "column_cos_2     1.455926\n",
       "column_cos_3     1.490736\n",
       "column_cos_4     1.415583\n",
       "column_cos_5     1.355459\n",
       "column_cos_6     1.418385\n",
       "column_cos_7          NaN\n",
       "column_cos_8          NaN\n",
       "column_cos_9          NaN\n",
       "target           9.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finding Maximum for all the columns\n",
    "train.apply(max, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As for all the sentences the number of array values in cosine similarity and euclidean distance array are not same so we\n",
    "#replace those values accordingly by 60 and 1\n",
    "subset1 = train.iloc[:,:10].fillna(60)\n",
    "subset2 = train.iloc[:,10:].fillna(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_euc_0</th>\n",
       "      <th>column_euc_1</th>\n",
       "      <th>column_euc_2</th>\n",
       "      <th>column_euc_3</th>\n",
       "      <th>column_euc_4</th>\n",
       "      <th>column_euc_5</th>\n",
       "      <th>column_euc_6</th>\n",
       "      <th>column_euc_7</th>\n",
       "      <th>column_euc_8</th>\n",
       "      <th>column_euc_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.301305</td>\n",
       "      <td>6.476413</td>\n",
       "      <td>7.214262</td>\n",
       "      <td>6.917620</td>\n",
       "      <td>6.519748</td>\n",
       "      <td>3.641007</td>\n",
       "      <td>6.908043</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.717004</td>\n",
       "      <td>5.076439</td>\n",
       "      <td>6.340898</td>\n",
       "      <td>3.586788</td>\n",
       "      <td>4.826955</td>\n",
       "      <td>7.294750</td>\n",
       "      <td>6.392016</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.349530</td>\n",
       "      <td>4.854142</td>\n",
       "      <td>5.628676</td>\n",
       "      <td>2.882886</td>\n",
       "      <td>3.573071</td>\n",
       "      <td>6.292664</td>\n",
       "      <td>5.433945</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   column_euc_0  column_euc_1  column_euc_2  column_euc_3  column_euc_4  \\\n",
       "0      7.301305      6.476413      7.214262      6.917620      6.519748   \n",
       "1      5.717004      5.076439      6.340898      3.586788      4.826955   \n",
       "2      5.349530      4.854142      5.628676      2.882886      3.573071   \n",
       "\n",
       "   column_euc_5  column_euc_6  column_euc_7  column_euc_8  column_euc_9  \n",
       "0      3.641007      6.908043          60.0          60.0          60.0  \n",
       "1      7.294750      6.392016          60.0          60.0          60.0  \n",
       "2      6.292664      5.433945          60.0          60.0          60.0  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " subset1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the update of NaN values in both euclidean and cosine subsets they are concatenated for final use\n",
    "train2 = pd.concat([subset1, subset2],axis=1, join_axes=[subset1.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "column_euc_0    29.927914\n",
       "column_euc_1    60.000000\n",
       "column_euc_2    60.000000\n",
       "column_euc_3    60.000000\n",
       "column_euc_4    60.000000\n",
       "column_euc_5    60.000000\n",
       "column_euc_6    60.000000\n",
       "column_euc_7    60.000000\n",
       "column_euc_8    60.000000\n",
       "column_euc_9    60.000000\n",
       "column_cos_0     1.491517\n",
       "column_cos_1     1.489619\n",
       "column_cos_2     1.455926\n",
       "column_cos_3     1.490736\n",
       "column_cos_4     1.415583\n",
       "column_cos_5     1.355459\n",
       "column_cos_6     1.418385\n",
       "column_cos_7     1.354246\n",
       "column_cos_8     1.315012\n",
       "column_cos_9     1.159472\n",
       "target           9.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Maximum for all columns\n",
    "train2.apply(max, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization of final train data between 0 to 1\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(train2.iloc[:,:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.23388115, 0.10277595, 0.1112316 , ..., 0.71921987, 0.74207714,\n",
       "        0.84381084],\n",
       "       [0.18023799, 0.07930797, 0.09652653, ..., 0.71921987, 0.74207714,\n",
       "        0.84381084],\n",
       "       [0.1677956 , 0.07558157, 0.08453465, ..., 0.71921987, 0.74207714,\n",
       "        0.84381084],\n",
       "       ...,\n",
       "       [0.16931342, 0.0857103 , 0.10546501, ..., 0.71921987, 0.74207714,\n",
       "        0.84381084],\n",
       "       [0.17368332, 0.06897399, 0.10499793, ..., 0.71921987, 0.74207714,\n",
       "        0.84381084],\n",
       "       [0.23098827, 0.11801073, 0.16833388, ..., 0.71921987, 0.74207714,\n",
       "        0.84381084]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training data and testing data with 80:20 ratio\n",
    "train_x, test_x, train_y, test_y = train_test_split(X,\n",
    "train.iloc[:,-1], train_size=0.8, random_state = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Logistic regression Train Accuracy :  0.6255525368969822\n",
      "Multinomial Logistic regression Test Accuracy :  0.6261748120300752\n"
     ]
    }
   ],
   "source": [
    "#Applying Logistic Regression using sklearn library\n",
    "mul_lr = linear_model.LogisticRegression(multi_class='multinomial', solver='saga',max_iter=200)\n",
    "mul_lr.fit(train_x, train_y)\n",
    "\n",
    "print(\"Multinomial Logistic regression Train Accuracy : \", metrics.accuracy_score(train_y, mul_lr.predict(train_x)))\n",
    "print(\"Multinomial Logistic regression Test Accuracy : \", metrics.accuracy_score(test_y, mul_lr.predict(test_x)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Logistic Regression Using Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Logistic regression Train Accuracy :  0.6255525368969822\n",
      "Multinomial Logistic regression Test Accuracy :  0.6261748120300752\n"
     ]
    }
   ],
   "source": [
    "#Applying Logistic Regression using sklearn library L2 Regularization\n",
    "mul_lr = linear_model.LogisticRegression(multi_class='multinomial', solver='saga',max_iter=200,penalty='l2')\n",
    "mul_lr.fit(train_x, train_y)\n",
    "\n",
    "print(\"Multinomial Logistic regression Train Accuracy : \", metrics.accuracy_score(train_y, mul_lr.predict(train_x)))\n",
    "print(\"Multinomial Logistic regression Test Accuracy : \", metrics.accuracy_score(test_y, mul_lr.predict(test_x)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Using Keras for single neuron model applying softmax for Logistic Regression\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Activation \n",
    "from keras import regularizers\n",
    "output_dim = nb_classes = 11\n",
    "model = Sequential() \n",
    "input_dim = 20\n",
    "model.add(Dense(output_dim, input_dim=input_dim,activation='softmax')) \n",
    "batch_size = 256 \n",
    "nb_epoch = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 68095 samples, validate on 17024 samples\n",
      "Epoch 1/1000\n",
      "68095/68095 [==============================] - 1s 18us/step - loss: 1.9106 - acc: 0.2363 - val_loss: 1.7850 - val_acc: 0.2400\n",
      "Epoch 2/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.7672 - acc: 0.2501 - val_loss: 1.7385 - val_acc: 0.2643\n",
      "Epoch 3/1000\n",
      "68095/68095 [==============================] - 1s 18us/step - loss: 1.7328 - acc: 0.2636 - val_loss: 1.7109 - val_acc: 0.2625\n",
      "Epoch 4/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.7090 - acc: 0.2687 - val_loss: 1.6898 - val_acc: 0.2696\n",
      "Epoch 5/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.6899 - acc: 0.2735 - val_loss: 1.6723 - val_acc: 0.2734\n",
      "Epoch 6/1000\n",
      "68095/68095 [==============================] - 1s 18us/step - loss: 1.6737 - acc: 0.2765 - val_loss: 1.6572 - val_acc: 0.2794\n",
      "Epoch 7/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.6595 - acc: 0.2806 - val_loss: 1.6440 - val_acc: 0.2837\n",
      "Epoch 8/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.6470 - acc: 0.2830 - val_loss: 1.6319 - val_acc: 0.2851\n",
      "Epoch 9/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.6356 - acc: 0.2870 - val_loss: 1.6212 - val_acc: 0.2899\n",
      "Epoch 10/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.6253 - acc: 0.2925 - val_loss: 1.6113 - val_acc: 0.2906\n",
      "Epoch 11/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.6159 - acc: 0.2955 - val_loss: 1.6023 - val_acc: 0.2964\n",
      "Epoch 12/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.6070 - acc: 0.2988 - val_loss: 1.5940 - val_acc: 0.3074\n",
      "Epoch 13/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.5990 - acc: 0.3058 - val_loss: 1.5861 - val_acc: 0.3030\n",
      "Epoch 14/1000\n",
      "68095/68095 [==============================] - 1s 19us/step - loss: 1.5914 - acc: 0.3086 - val_loss: 1.5788 - val_acc: 0.3046\n",
      "Epoch 15/1000\n",
      "68095/68095 [==============================] - 1s 19us/step - loss: 1.5844 - acc: 0.3113 - val_loss: 1.5719 - val_acc: 0.3178\n",
      "Epoch 16/1000\n",
      "68095/68095 [==============================] - 1s 20us/step - loss: 1.5777 - acc: 0.3214 - val_loss: 1.5655 - val_acc: 0.3169\n",
      "Epoch 17/1000\n",
      "68095/68095 [==============================] - 1s 18us/step - loss: 1.5715 - acc: 0.3253 - val_loss: 1.5594 - val_acc: 0.3216\n",
      "Epoch 18/1000\n",
      "68095/68095 [==============================] - 1s 19us/step - loss: 1.5655 - acc: 0.3294 - val_loss: 1.5537 - val_acc: 0.3357\n",
      "Epoch 19/1000\n",
      "68095/68095 [==============================] - 1s 18us/step - loss: 1.5599 - acc: 0.3377 - val_loss: 1.5482 - val_acc: 0.3398\n",
      "Epoch 20/1000\n",
      "68095/68095 [==============================] - 2s 24us/step - loss: 1.5545 - acc: 0.3425 - val_loss: 1.5430 - val_acc: 0.3524\n",
      "Epoch 21/1000\n",
      "68095/68095 [==============================] - 1s 22us/step - loss: 1.5494 - acc: 0.3494 - val_loss: 1.5380 - val_acc: 0.3561\n",
      "Epoch 22/1000\n",
      "68095/68095 [==============================] - 1s 22us/step - loss: 1.5445 - acc: 0.3573 - val_loss: 1.5333 - val_acc: 0.3466\n",
      "Epoch 23/1000\n",
      "68095/68095 [==============================] - 1s 19us/step - loss: 1.5399 - acc: 0.3588 - val_loss: 1.5286 - val_acc: 0.3596\n",
      "Epoch 24/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.5353 - acc: 0.3669 - val_loss: 1.5242 - val_acc: 0.3630\n",
      "Epoch 25/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.5310 - acc: 0.3719 - val_loss: 1.5201 - val_acc: 0.3614\n",
      "Epoch 26/1000\n",
      "68095/68095 [==============================] - 1s 18us/step - loss: 1.5269 - acc: 0.3761 - val_loss: 1.5160 - val_acc: 0.3709\n",
      "Epoch 27/1000\n",
      "68095/68095 [==============================] - 1s 21us/step - loss: 1.5229 - acc: 0.3815 - val_loss: 1.5121 - val_acc: 0.3768\n",
      "Epoch 28/1000\n",
      "68095/68095 [==============================] - 1s 19us/step - loss: 1.5190 - acc: 0.3860 - val_loss: 1.5082 - val_acc: 0.3916\n",
      "Epoch 29/1000\n",
      "68095/68095 [==============================] - 1s 21us/step - loss: 1.5152 - acc: 0.3959 - val_loss: 1.5047 - val_acc: 0.3765\n",
      "Epoch 30/1000\n",
      "68095/68095 [==============================] - 1s 18us/step - loss: 1.5117 - acc: 0.3965 - val_loss: 1.5011 - val_acc: 0.3854\n",
      "Epoch 31/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.5081 - acc: 0.4002 - val_loss: 1.4976 - val_acc: 0.4102\n",
      "Epoch 32/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.5047 - acc: 0.4072 - val_loss: 1.4943 - val_acc: 0.4168\n",
      "Epoch 33/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.5014 - acc: 0.4142 - val_loss: 1.4911 - val_acc: 0.4050\n",
      "Epoch 34/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.4982 - acc: 0.4165 - val_loss: 1.4879 - val_acc: 0.4155\n",
      "Epoch 35/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.4951 - acc: 0.4196 - val_loss: 1.4848 - val_acc: 0.4310\n",
      "Epoch 36/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.4921 - acc: 0.4269 - val_loss: 1.4818 - val_acc: 0.4201\n",
      "Epoch 37/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.4891 - acc: 0.4295 - val_loss: 1.4789 - val_acc: 0.4289\n",
      "Epoch 38/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.4862 - acc: 0.4304 - val_loss: 1.4761 - val_acc: 0.4473\n",
      "Epoch 39/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.4834 - acc: 0.4376 - val_loss: 1.4733 - val_acc: 0.4514\n",
      "Epoch 40/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.4806 - acc: 0.4419 - val_loss: 1.4706 - val_acc: 0.4525\n",
      "Epoch 41/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.4780 - acc: 0.4456 - val_loss: 1.4679 - val_acc: 0.4476\n",
      "Epoch 42/1000\n",
      "68095/68095 [==============================] - 1s 18us/step - loss: 1.4753 - acc: 0.4464 - val_loss: 1.4654 - val_acc: 0.4592\n",
      "Epoch 43/1000\n",
      "68095/68095 [==============================] - 1s 19us/step - loss: 1.4728 - acc: 0.4526 - val_loss: 1.4628 - val_acc: 0.4521\n",
      "Epoch 44/1000\n",
      "68095/68095 [==============================] - 1s 19us/step - loss: 1.4702 - acc: 0.4515 - val_loss: 1.4604 - val_acc: 0.4699\n",
      "Epoch 45/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.4678 - acc: 0.4590 - val_loss: 1.4579 - val_acc: 0.4645\n",
      "Epoch 46/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.4654 - acc: 0.4597 - val_loss: 1.4555 - val_acc: 0.4678\n",
      "Epoch 47/1000\n",
      "68095/68095 [==============================] - 1s 18us/step - loss: 1.4630 - acc: 0.4633 - val_loss: 1.4532 - val_acc: 0.4707\n",
      "Epoch 48/1000\n",
      "68095/68095 [==============================] - 1s 20us/step - loss: 1.4607 - acc: 0.4646 - val_loss: 1.4509 - val_acc: 0.4794\n",
      "Epoch 49/1000\n",
      "68095/68095 [==============================] - 1s 21us/step - loss: 1.4583 - acc: 0.4707 - val_loss: 1.4488 - val_acc: 0.4672\n",
      "Epoch 50/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.4562 - acc: 0.4714 - val_loss: 1.4465 - val_acc: 0.4769\n",
      "Epoch 51/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.4540 - acc: 0.4740 - val_loss: 1.4443 - val_acc: 0.4866\n",
      "Epoch 52/1000\n",
      "68095/68095 [==============================] - 1s 18us/step - loss: 1.4518 - acc: 0.4770 - val_loss: 1.4422 - val_acc: 0.4884\n",
      "Epoch 53/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.4497 - acc: 0.4813 - val_loss: 1.4401 - val_acc: 0.4857\n",
      "Epoch 54/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.4476 - acc: 0.4830 - val_loss: 1.4380 - val_acc: 0.4857\n",
      "Epoch 55/1000\n",
      "68095/68095 [==============================] - 1s 18us/step - loss: 1.4456 - acc: 0.4840 - val_loss: 1.4360 - val_acc: 0.4913\n",
      "Epoch 56/1000\n",
      "68095/68095 [==============================] - 1s 18us/step - loss: 1.4436 - acc: 0.4875 - val_loss: 1.4340 - val_acc: 0.4932\n",
      "Epoch 57/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.4416 - acc: 0.4894 - val_loss: 1.4321 - val_acc: 0.4921\n",
      "Epoch 58/1000\n",
      "68095/68095 [==============================] - 1s 18us/step - loss: 1.4396 - acc: 0.4888 - val_loss: 1.4302 - val_acc: 0.4979\n",
      "Epoch 59/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.4377 - acc: 0.4931 - val_loss: 1.4283 - val_acc: 0.4993\n",
      "Epoch 60/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.4358 - acc: 0.4937 - val_loss: 1.4264 - val_acc: 0.5051\n",
      "Epoch 61/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.4339 - acc: 0.4980 - val_loss: 1.4246 - val_acc: 0.4992\n",
      "Epoch 62/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.4321 - acc: 0.4959 - val_loss: 1.4228 - val_acc: 0.5170\n",
      "Epoch 63/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.4303 - acc: 0.5026 - val_loss: 1.4209 - val_acc: 0.5072\n",
      "Epoch 64/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.4285 - acc: 0.5011 - val_loss: 1.4192 - val_acc: 0.5109\n",
      "Epoch 65/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.4268 - acc: 0.5047 - val_loss: 1.4175 - val_acc: 0.5065\n",
      "Epoch 66/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.4250 - acc: 0.5044 - val_loss: 1.4158 - val_acc: 0.5127\n",
      "Epoch 67/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.4233 - acc: 0.5064 - val_loss: 1.4140 - val_acc: 0.5198\n",
      "Epoch 68/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.4216 - acc: 0.5100 - val_loss: 1.4124 - val_acc: 0.5123\n",
      "Epoch 69/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.4200 - acc: 0.5105 - val_loss: 1.4108 - val_acc: 0.5121\n",
      "Epoch 70/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.4183 - acc: 0.5108 - val_loss: 1.4092 - val_acc: 0.5161\n",
      "Epoch 71/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.4167 - acc: 0.5127 - val_loss: 1.4075 - val_acc: 0.5242\n",
      "Epoch 72/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.4151 - acc: 0.5166 - val_loss: 1.4059 - val_acc: 0.5161\n",
      "Epoch 73/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.4135 - acc: 0.5153 - val_loss: 1.4044 - val_acc: 0.5233\n",
      "Epoch 74/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.4120 - acc: 0.5166 - val_loss: 1.4028 - val_acc: 0.5285\n",
      "Epoch 75/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.4104 - acc: 0.5193 - val_loss: 1.4013 - val_acc: 0.5278\n",
      "Epoch 76/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.4089 - acc: 0.5193 - val_loss: 1.3998 - val_acc: 0.5290\n",
      "Epoch 77/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.4074 - acc: 0.5203 - val_loss: 1.3983 - val_acc: 0.5309\n",
      "Epoch 78/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.4059 - acc: 0.5225 - val_loss: 1.3968 - val_acc: 0.5342\n",
      "Epoch 79/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.4044 - acc: 0.5241 - val_loss: 1.3954 - val_acc: 0.5292\n",
      "Epoch 80/1000\n",
      "68095/68095 [==============================] - 1s 18us/step - loss: 1.4030 - acc: 0.5241 - val_loss: 1.3940 - val_acc: 0.5331\n",
      "Epoch 81/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.4016 - acc: 0.5258 - val_loss: 1.3925 - val_acc: 0.5368\n",
      "Epoch 82/1000\n",
      "68095/68095 [==============================] - 1s 18us/step - loss: 1.4001 - acc: 0.5266 - val_loss: 1.3912 - val_acc: 0.5285\n",
      "Epoch 83/1000\n",
      "68095/68095 [==============================] - 1s 18us/step - loss: 1.3987 - acc: 0.5261 - val_loss: 1.3897 - val_acc: 0.5401\n",
      "Epoch 84/1000\n",
      "68095/68095 [==============================] - 1s 18us/step - loss: 1.3973 - acc: 0.5295 - val_loss: 1.3883 - val_acc: 0.5382\n",
      "Epoch 85/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.3959 - acc: 0.5303 - val_loss: 1.3870 - val_acc: 0.5322\n",
      "Epoch 86/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.3946 - acc: 0.5291 - val_loss: 1.3856 - val_acc: 0.5432\n",
      "Epoch 87/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.3932 - acc: 0.5319 - val_loss: 1.3843 - val_acc: 0.5422\n",
      "Epoch 88/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.3919 - acc: 0.5329 - val_loss: 1.3829 - val_acc: 0.5438\n",
      "Epoch 89/1000\n",
      "68095/68095 [==============================] - 1s 19us/step - loss: 1.3906 - acc: 0.5344 - val_loss: 1.3816 - val_acc: 0.5418\n",
      "Epoch 90/1000\n",
      "68095/68095 [==============================] - 1s 20us/step - loss: 1.3893 - acc: 0.5341 - val_loss: 1.3803 - val_acc: 0.5439\n",
      "Epoch 91/1000\n",
      "68095/68095 [==============================] - 1s 21us/step - loss: 1.3880 - acc: 0.5355 - val_loss: 1.3791 - val_acc: 0.5442\n",
      "Epoch 92/1000\n",
      "68095/68095 [==============================] - 1s 19us/step - loss: 1.3867 - acc: 0.5368 - val_loss: 1.3778 - val_acc: 0.5438\n",
      "Epoch 93/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.3854 - acc: 0.5375 - val_loss: 1.3765 - val_acc: 0.5448\n",
      "Epoch 94/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.3842 - acc: 0.5387 - val_loss: 1.3754 - val_acc: 0.5428\n",
      "Epoch 95/1000\n",
      "68095/68095 [==============================] - 1s 18us/step - loss: 1.3829 - acc: 0.5373 - val_loss: 1.3742 - val_acc: 0.5526\n",
      "Epoch 96/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.3817 - acc: 0.5406 - val_loss: 1.3729 - val_acc: 0.5451\n",
      "Epoch 97/1000\n",
      "68095/68095 [==============================] - 1s 18us/step - loss: 1.3805 - acc: 0.5401 - val_loss: 1.3717 - val_acc: 0.5499\n",
      "Epoch 98/1000\n",
      "68095/68095 [==============================] - 1s 19us/step - loss: 1.3793 - acc: 0.5426 - val_loss: 1.3705 - val_acc: 0.5448\n",
      "Epoch 99/1000\n",
      "68095/68095 [==============================] - 1s 21us/step - loss: 1.3781 - acc: 0.5409 - val_loss: 1.3693 - val_acc: 0.5514\n",
      "Epoch 100/1000\n",
      "68095/68095 [==============================] - 2s 24us/step - loss: 1.3769 - acc: 0.5435 - val_loss: 1.3681 - val_acc: 0.5471\n",
      "Epoch 101/1000\n",
      "68095/68095 [==============================] - 1s 18us/step - loss: 1.3757 - acc: 0.5436 - val_loss: 1.3670 - val_acc: 0.5453\n",
      "Epoch 102/1000\n",
      "68095/68095 [==============================] - 1s 21us/step - loss: 1.3746 - acc: 0.5425 - val_loss: 1.3658 - val_acc: 0.5527\n",
      "Epoch 103/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.3734 - acc: 0.5441 - val_loss: 1.3647 - val_acc: 0.5546\n",
      "Epoch 104/1000\n",
      "68095/68095 [==============================] - 1s 19us/step - loss: 1.3723 - acc: 0.5469 - val_loss: 1.3635 - val_acc: 0.5513\n",
      "Epoch 105/1000\n",
      "68095/68095 [==============================] - 2s 26us/step - loss: 1.3711 - acc: 0.5459 - val_loss: 1.3624 - val_acc: 0.5525\n",
      "Epoch 106/1000\n",
      "68095/68095 [==============================] - 2s 29us/step - loss: 1.3700 - acc: 0.5454 - val_loss: 1.3613 - val_acc: 0.5580\n",
      "Epoch 107/1000\n",
      "68095/68095 [==============================] - 2s 26us/step - loss: 1.3689 - acc: 0.5493 - val_loss: 1.3602 - val_acc: 0.5490\n",
      "Epoch 108/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.3678 - acc: 0.5469 - val_loss: 1.3591 - val_acc: 0.5551\n",
      "Epoch 109/1000\n",
      "68095/68095 [==============================] - 1s 18us/step - loss: 1.3667 - acc: 0.5485 - val_loss: 1.3580 - val_acc: 0.5504\n",
      "Epoch 110/1000\n",
      "68095/68095 [==============================] - 1s 18us/step - loss: 1.3656 - acc: 0.5475 - val_loss: 1.3569 - val_acc: 0.5539\n",
      "Epoch 111/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.3646 - acc: 0.5497 - val_loss: 1.3558 - val_acc: 0.5542\n",
      "Epoch 112/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.3635 - acc: 0.5494 - val_loss: 1.3548 - val_acc: 0.5560\n",
      "Epoch 113/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.3625 - acc: 0.5512 - val_loss: 1.3538 - val_acc: 0.5569\n",
      "Epoch 114/1000\n",
      "68095/68095 [==============================] - 1s 18us/step - loss: 1.3614 - acc: 0.5509 - val_loss: 1.3527 - val_acc: 0.5571\n",
      "Epoch 115/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.3603 - acc: 0.5528 - val_loss: 1.3518 - val_acc: 0.5520\n",
      "Epoch 116/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.3593 - acc: 0.5511 - val_loss: 1.3507 - val_acc: 0.5585\n",
      "Epoch 117/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.3583 - acc: 0.5518 - val_loss: 1.3497 - val_acc: 0.5620\n",
      "Epoch 118/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.3573 - acc: 0.5541 - val_loss: 1.3486 - val_acc: 0.5588\n",
      "Epoch 119/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.3563 - acc: 0.5529 - val_loss: 1.3477 - val_acc: 0.5560\n",
      "Epoch 120/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.3553 - acc: 0.5540 - val_loss: 1.3467 - val_acc: 0.5580\n",
      "Epoch 121/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.3543 - acc: 0.5540 - val_loss: 1.3457 - val_acc: 0.5609\n",
      "Epoch 122/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.3533 - acc: 0.5549 - val_loss: 1.3447 - val_acc: 0.5626\n",
      "Epoch 123/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.3523 - acc: 0.5563 - val_loss: 1.3437 - val_acc: 0.5608\n",
      "Epoch 124/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.3514 - acc: 0.5557 - val_loss: 1.3428 - val_acc: 0.5629\n",
      "Epoch 125/1000\n",
      "68095/68095 [==============================] - 1s 18us/step - loss: 1.3504 - acc: 0.5559 - val_loss: 1.3418 - val_acc: 0.5632\n",
      "Epoch 126/1000\n",
      "68095/68095 [==============================] - 1s 18us/step - loss: 1.3495 - acc: 0.5567 - val_loss: 1.3409 - val_acc: 0.5641\n",
      "Epoch 127/1000\n",
      "68095/68095 [==============================] - 1s 19us/step - loss: 1.3485 - acc: 0.5580 - val_loss: 1.3400 - val_acc: 0.5610\n",
      "Epoch 128/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.3476 - acc: 0.5578 - val_loss: 1.3391 - val_acc: 0.5583\n",
      "Epoch 129/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.3467 - acc: 0.5581 - val_loss: 1.3382 - val_acc: 0.5582\n",
      "Epoch 130/1000\n",
      "68095/68095 [==============================] - 1s 18us/step - loss: 1.3457 - acc: 0.5590 - val_loss: 1.3372 - val_acc: 0.5586\n",
      "Epoch 131/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.3448 - acc: 0.5577 - val_loss: 1.3363 - val_acc: 0.5664\n",
      "Epoch 132/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.3439 - acc: 0.5603 - val_loss: 1.3354 - val_acc: 0.5616\n",
      "Epoch 133/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.3430 - acc: 0.5595 - val_loss: 1.3345 - val_acc: 0.5668\n",
      "Epoch 134/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.3421 - acc: 0.5608 - val_loss: 1.3336 - val_acc: 0.5639\n",
      "Epoch 135/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.3412 - acc: 0.5601 - val_loss: 1.3327 - val_acc: 0.5657\n",
      "Epoch 136/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.3403 - acc: 0.5612 - val_loss: 1.3320 - val_acc: 0.5636\n",
      "Epoch 137/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.3395 - acc: 0.5607 - val_loss: 1.3310 - val_acc: 0.5658\n",
      "Epoch 138/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.3386 - acc: 0.5607 - val_loss: 1.3302 - val_acc: 0.5706\n",
      "Epoch 139/1000\n",
      "68095/68095 [==============================] - 1s 20us/step - loss: 1.3378 - acc: 0.5626 - val_loss: 1.3293 - val_acc: 0.5692\n",
      "Epoch 140/1000\n",
      "68095/68095 [==============================] - 1s 21us/step - loss: 1.3369 - acc: 0.5625 - val_loss: 1.3284 - val_acc: 0.5703\n",
      "Epoch 141/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.3360 - acc: 0.5638 - val_loss: 1.3276 - val_acc: 0.5670\n",
      "Epoch 142/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.3352 - acc: 0.5629 - val_loss: 1.3267 - val_acc: 0.5686\n",
      "Epoch 143/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.3344 - acc: 0.5641 - val_loss: 1.3259 - val_acc: 0.5674\n",
      "Epoch 144/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.3335 - acc: 0.5637 - val_loss: 1.3251 - val_acc: 0.5698\n",
      "Epoch 145/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.3327 - acc: 0.5639 - val_loss: 1.3242 - val_acc: 0.5705\n",
      "Epoch 146/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.3319 - acc: 0.5641 - val_loss: 1.3235 - val_acc: 0.5740\n",
      "Epoch 147/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.3311 - acc: 0.5652 - val_loss: 1.3226 - val_acc: 0.5735\n",
      "Epoch 148/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.3303 - acc: 0.5651 - val_loss: 1.3219 - val_acc: 0.5723\n",
      "Epoch 149/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.3295 - acc: 0.5657 - val_loss: 1.3210 - val_acc: 0.5730\n",
      "Epoch 150/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.3287 - acc: 0.5659 - val_loss: 1.3202 - val_acc: 0.5738\n",
      "Epoch 151/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.3279 - acc: 0.5669 - val_loss: 1.3195 - val_acc: 0.5694\n",
      "Epoch 152/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.3271 - acc: 0.5666 - val_loss: 1.3187 - val_acc: 0.5704\n",
      "Epoch 153/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.3263 - acc: 0.5674 - val_loss: 1.3179 - val_acc: 0.5692\n",
      "Epoch 154/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.3255 - acc: 0.5672 - val_loss: 1.3172 - val_acc: 0.5718\n",
      "Epoch 155/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.3248 - acc: 0.5677 - val_loss: 1.3164 - val_acc: 0.5734\n",
      "Epoch 156/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.3240 - acc: 0.5680 - val_loss: 1.3156 - val_acc: 0.5736\n",
      "Epoch 157/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.3232 - acc: 0.5681 - val_loss: 1.3148 - val_acc: 0.5747\n",
      "Epoch 158/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.3225 - acc: 0.5687 - val_loss: 1.3141 - val_acc: 0.5740\n",
      "Epoch 159/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.3217 - acc: 0.5688 - val_loss: 1.3133 - val_acc: 0.5735\n",
      "Epoch 160/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.3210 - acc: 0.5688 - val_loss: 1.3126 - val_acc: 0.5741\n",
      "Epoch 161/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.3202 - acc: 0.5694 - val_loss: 1.3119 - val_acc: 0.5747\n",
      "Epoch 162/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.3195 - acc: 0.5697 - val_loss: 1.3111 - val_acc: 0.5751\n",
      "Epoch 163/1000\n",
      "68095/68095 [==============================] - 1s 18us/step - loss: 1.3188 - acc: 0.5701 - val_loss: 1.3104 - val_acc: 0.5748\n",
      "Epoch 164/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.3180 - acc: 0.5696 - val_loss: 1.3097 - val_acc: 0.5773\n",
      "Epoch 165/1000\n",
      "68095/68095 [==============================] - 1s 18us/step - loss: 1.3173 - acc: 0.5701 - val_loss: 1.3090 - val_acc: 0.5762\n",
      "Epoch 166/1000\n",
      "68095/68095 [==============================] - 1s 20us/step - loss: 1.3166 - acc: 0.5707 - val_loss: 1.3083 - val_acc: 0.5749\n",
      "Epoch 167/1000\n",
      "68095/68095 [==============================] - 1s 20us/step - loss: 1.3159 - acc: 0.5706 - val_loss: 1.3075 - val_acc: 0.5767\n",
      "Epoch 168/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.3152 - acc: 0.5709 - val_loss: 1.3068 - val_acc: 0.5776\n",
      "Epoch 169/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.3145 - acc: 0.5709 - val_loss: 1.3062 - val_acc: 0.5765\n",
      "Epoch 170/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.3138 - acc: 0.5715 - val_loss: 1.3055 - val_acc: 0.5756\n",
      "Epoch 171/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.3131 - acc: 0.5713 - val_loss: 1.3048 - val_acc: 0.5758\n",
      "Epoch 172/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.3124 - acc: 0.5713 - val_loss: 1.3040 - val_acc: 0.5803\n",
      "Epoch 173/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.3117 - acc: 0.5719 - val_loss: 1.3034 - val_acc: 0.5789\n",
      "Epoch 174/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.3110 - acc: 0.5728 - val_loss: 1.3027 - val_acc: 0.5781\n",
      "Epoch 175/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.3104 - acc: 0.5727 - val_loss: 1.3020 - val_acc: 0.5785\n",
      "Epoch 176/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.3097 - acc: 0.5724 - val_loss: 1.3014 - val_acc: 0.5768\n",
      "Epoch 177/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.3090 - acc: 0.5729 - val_loss: 1.3007 - val_acc: 0.5797\n",
      "Epoch 178/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.3083 - acc: 0.5732 - val_loss: 1.3000 - val_acc: 0.5802\n",
      "Epoch 179/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.3077 - acc: 0.5741 - val_loss: 1.2994 - val_acc: 0.5771\n",
      "Epoch 180/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.3070 - acc: 0.5732 - val_loss: 1.2988 - val_acc: 0.5804\n",
      "Epoch 181/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.3064 - acc: 0.5734 - val_loss: 1.2981 - val_acc: 0.5808\n",
      "Epoch 182/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.3057 - acc: 0.5749 - val_loss: 1.2974 - val_acc: 0.5819\n",
      "Epoch 183/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.3051 - acc: 0.5744 - val_loss: 1.2968 - val_acc: 0.5799\n",
      "Epoch 184/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.3044 - acc: 0.5741 - val_loss: 1.2962 - val_acc: 0.5824\n",
      "Epoch 185/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.3038 - acc: 0.5750 - val_loss: 1.2955 - val_acc: 0.5810\n",
      "Epoch 186/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.3031 - acc: 0.5742 - val_loss: 1.2949 - val_acc: 0.5841\n",
      "Epoch 187/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.3025 - acc: 0.5751 - val_loss: 1.2943 - val_acc: 0.5821\n",
      "Epoch 188/1000\n",
      "68095/68095 [==============================] - 2s 22us/step - loss: 1.3019 - acc: 0.5756 - val_loss: 1.2936 - val_acc: 0.5816\n",
      "Epoch 189/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.3013 - acc: 0.5747 - val_loss: 1.2930 - val_acc: 0.5831\n",
      "Epoch 190/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.3006 - acc: 0.5757 - val_loss: 1.2924 - val_acc: 0.5841\n",
      "Epoch 191/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.3000 - acc: 0.5750 - val_loss: 1.2918 - val_acc: 0.5834\n",
      "Epoch 192/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.2994 - acc: 0.5759 - val_loss: 1.2912 - val_acc: 0.5846\n",
      "Epoch 193/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.2988 - acc: 0.5762 - val_loss: 1.2905 - val_acc: 0.5838\n",
      "Epoch 194/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.2982 - acc: 0.5765 - val_loss: 1.2899 - val_acc: 0.5843\n",
      "Epoch 195/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.2976 - acc: 0.5768 - val_loss: 1.2894 - val_acc: 0.5812\n",
      "Epoch 196/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.2970 - acc: 0.5760 - val_loss: 1.2887 - val_acc: 0.5846\n",
      "Epoch 197/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.2964 - acc: 0.5763 - val_loss: 1.2882 - val_acc: 0.5822\n",
      "Epoch 198/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.2958 - acc: 0.5760 - val_loss: 1.2876 - val_acc: 0.5836\n",
      "Epoch 199/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.2952 - acc: 0.5763 - val_loss: 1.2870 - val_acc: 0.5859\n",
      "Epoch 200/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.2946 - acc: 0.5771 - val_loss: 1.2864 - val_acc: 0.5846\n",
      "Epoch 201/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.2941 - acc: 0.5774 - val_loss: 1.2858 - val_acc: 0.5848\n",
      "Epoch 202/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.2935 - acc: 0.5770 - val_loss: 1.2853 - val_acc: 0.5857\n",
      "Epoch 203/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.2929 - acc: 0.5776 - val_loss: 1.2847 - val_acc: 0.5836\n",
      "Epoch 204/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.2923 - acc: 0.5765 - val_loss: 1.2841 - val_acc: 0.5872\n",
      "Epoch 205/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.2918 - acc: 0.5776 - val_loss: 1.2836 - val_acc: 0.5844\n",
      "Epoch 206/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.2912 - acc: 0.5780 - val_loss: 1.2831 - val_acc: 0.5838\n",
      "Epoch 207/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.2906 - acc: 0.5780 - val_loss: 1.2825 - val_acc: 0.5859\n",
      "Epoch 208/1000\n",
      "68095/68095 [==============================] - 1s 18us/step - loss: 1.2901 - acc: 0.5786 - val_loss: 1.2819 - val_acc: 0.5876\n",
      "Epoch 209/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.2895 - acc: 0.5782 - val_loss: 1.2814 - val_acc: 0.5865\n",
      "Epoch 210/1000\n",
      "68095/68095 [==============================] - 1s 19us/step - loss: 1.2890 - acc: 0.5785 - val_loss: 1.2808 - val_acc: 0.5852\n",
      "Epoch 211/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.2884 - acc: 0.5791 - val_loss: 1.2803 - val_acc: 0.5854\n",
      "Epoch 212/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.2879 - acc: 0.5788 - val_loss: 1.2797 - val_acc: 0.5871\n",
      "Epoch 213/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.2873 - acc: 0.5784 - val_loss: 1.2791 - val_acc: 0.5859\n",
      "Epoch 214/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.2868 - acc: 0.5793 - val_loss: 1.2786 - val_acc: 0.5881\n",
      "Epoch 215/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.2863 - acc: 0.5794 - val_loss: 1.2781 - val_acc: 0.5883\n",
      "Epoch 216/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.2857 - acc: 0.5796 - val_loss: 1.2775 - val_acc: 0.5885\n",
      "Epoch 217/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.2852 - acc: 0.5786 - val_loss: 1.2770 - val_acc: 0.5899\n",
      "Epoch 218/1000\n",
      "68095/68095 [==============================] - 2s 22us/step - loss: 1.2846 - acc: 0.5798 - val_loss: 1.2765 - val_acc: 0.5896\n",
      "Epoch 219/1000\n",
      "68095/68095 [==============================] - 2s 24us/step - loss: 1.2841 - acc: 0.5805 - val_loss: 1.2759 - val_acc: 0.5892\n",
      "Epoch 220/1000\n",
      "68095/68095 [==============================] - 2s 24us/step - loss: 1.2836 - acc: 0.5803 - val_loss: 1.2754 - val_acc: 0.5881\n",
      "Epoch 221/1000\n",
      "68095/68095 [==============================] - 1s 18us/step - loss: 1.2831 - acc: 0.5808 - val_loss: 1.2749 - val_acc: 0.5891\n",
      "Epoch 222/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.2826 - acc: 0.5805 - val_loss: 1.2744 - val_acc: 0.5884\n",
      "Epoch 223/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.2820 - acc: 0.5802 - val_loss: 1.2739 - val_acc: 0.5889\n",
      "Epoch 224/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.2815 - acc: 0.5807 - val_loss: 1.2734 - val_acc: 0.5871\n",
      "Epoch 225/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.2810 - acc: 0.5810 - val_loss: 1.2729 - val_acc: 0.5894\n",
      "Epoch 226/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.2805 - acc: 0.5813 - val_loss: 1.2724 - val_acc: 0.5900\n",
      "Epoch 227/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.2800 - acc: 0.5807 - val_loss: 1.2719 - val_acc: 0.5908\n",
      "Epoch 228/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.2795 - acc: 0.5813 - val_loss: 1.2713 - val_acc: 0.5907\n",
      "Epoch 229/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.2790 - acc: 0.5817 - val_loss: 1.2709 - val_acc: 0.5893\n",
      "Epoch 230/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.2785 - acc: 0.5815 - val_loss: 1.2704 - val_acc: 0.5919\n",
      "Epoch 231/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.2780 - acc: 0.5819 - val_loss: 1.2699 - val_acc: 0.5897\n",
      "Epoch 232/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.2775 - acc: 0.5816 - val_loss: 1.2694 - val_acc: 0.5919\n",
      "Epoch 233/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.2770 - acc: 0.5822 - val_loss: 1.2689 - val_acc: 0.5898\n",
      "Epoch 234/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.2766 - acc: 0.5824 - val_loss: 1.2685 - val_acc: 0.5881\n",
      "Epoch 235/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.2761 - acc: 0.5818 - val_loss: 1.2680 - val_acc: 0.5895\n",
      "Epoch 236/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.2756 - acc: 0.5825 - val_loss: 1.2675 - val_acc: 0.5920\n",
      "Epoch 237/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.2751 - acc: 0.5827 - val_loss: 1.2670 - val_acc: 0.5919\n",
      "Epoch 238/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.2746 - acc: 0.5827 - val_loss: 1.2666 - val_acc: 0.5910\n",
      "Epoch 239/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.2742 - acc: 0.5832 - val_loss: 1.2660 - val_acc: 0.5924\n",
      "Epoch 240/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.2737 - acc: 0.5830 - val_loss: 1.2656 - val_acc: 0.5921\n",
      "Epoch 241/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.2732 - acc: 0.5836 - val_loss: 1.2651 - val_acc: 0.5918\n",
      "Epoch 242/1000\n",
      "68095/68095 [==============================] - 1s 18us/step - loss: 1.2728 - acc: 0.5831 - val_loss: 1.2646 - val_acc: 0.5922\n",
      "Epoch 243/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.2723 - acc: 0.5836 - val_loss: 1.2642 - val_acc: 0.5934\n",
      "Epoch 244/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.2718 - acc: 0.5840 - val_loss: 1.2637 - val_acc: 0.5925\n",
      "Epoch 245/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.2714 - acc: 0.5833 - val_loss: 1.2633 - val_acc: 0.5925\n",
      "Epoch 246/1000\n",
      "68095/68095 [==============================] - 1s 21us/step - loss: 1.2709 - acc: 0.5843 - val_loss: 1.2628 - val_acc: 0.5929\n",
      "Epoch 247/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.2704 - acc: 0.5843 - val_loss: 1.2623 - val_acc: 0.5927\n",
      "Epoch 248/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.2700 - acc: 0.5839 - val_loss: 1.2619 - val_acc: 0.5936\n",
      "Epoch 249/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.2695 - acc: 0.5843 - val_loss: 1.2615 - val_acc: 0.5929\n",
      "Epoch 250/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.2691 - acc: 0.5843 - val_loss: 1.2610 - val_acc: 0.5932\n",
      "Epoch 251/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.2686 - acc: 0.5849 - val_loss: 1.2606 - val_acc: 0.5916\n",
      "Epoch 252/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.2682 - acc: 0.5841 - val_loss: 1.2601 - val_acc: 0.5929\n",
      "Epoch 253/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.2677 - acc: 0.5849 - val_loss: 1.2597 - val_acc: 0.5926\n",
      "Epoch 254/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.2673 - acc: 0.5847 - val_loss: 1.2592 - val_acc: 0.5936\n",
      "Epoch 255/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.2669 - acc: 0.5854 - val_loss: 1.2588 - val_acc: 0.5932\n",
      "Epoch 256/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.2665 - acc: 0.5853 - val_loss: 1.2583 - val_acc: 0.5942\n",
      "Epoch 257/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.2660 - acc: 0.5852 - val_loss: 1.2579 - val_acc: 0.5927\n",
      "Epoch 258/1000\n",
      "68095/68095 [==============================] - ETA: 0s - loss: 1.2655 - acc: 0.585 - 1s 18us/step - loss: 1.2656 - acc: 0.5853 - val_loss: 1.2575 - val_acc: 0.5938\n",
      "Epoch 259/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.2652 - acc: 0.5853 - val_loss: 1.2571 - val_acc: 0.5935\n",
      "Epoch 260/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.2647 - acc: 0.5855 - val_loss: 1.2567 - val_acc: 0.5923\n",
      "Epoch 261/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.2643 - acc: 0.5855 - val_loss: 1.2562 - val_acc: 0.5938\n",
      "Epoch 262/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.2639 - acc: 0.5860 - val_loss: 1.2558 - val_acc: 0.5935\n",
      "Epoch 263/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.2635 - acc: 0.5860 - val_loss: 1.2554 - val_acc: 0.5939\n",
      "Epoch 264/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.2630 - acc: 0.5863 - val_loss: 1.2549 - val_acc: 0.5942\n",
      "Epoch 265/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.2626 - acc: 0.5862 - val_loss: 1.2545 - val_acc: 0.5947\n",
      "Epoch 266/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.2622 - acc: 0.5861 - val_loss: 1.2542 - val_acc: 0.5946\n",
      "Epoch 267/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.2618 - acc: 0.5866 - val_loss: 1.2537 - val_acc: 0.5940\n",
      "Epoch 268/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.2614 - acc: 0.5860 - val_loss: 1.2533 - val_acc: 0.5946\n",
      "Epoch 269/1000\n",
      "68095/68095 [==============================] - 1s 19us/step - loss: 1.2609 - acc: 0.5870 - val_loss: 1.2530 - val_acc: 0.5928\n",
      "Epoch 270/1000\n",
      "68095/68095 [==============================] - 2s 26us/step - loss: 1.2605 - acc: 0.5865 - val_loss: 1.2526 - val_acc: 0.5945\n",
      "Epoch 271/1000\n",
      "68095/68095 [==============================] - 2s 24us/step - loss: 1.2602 - acc: 0.5868 - val_loss: 1.2521 - val_acc: 0.5947\n",
      "Epoch 272/1000\n",
      "68095/68095 [==============================] - 1s 20us/step - loss: 1.2597 - acc: 0.5869 - val_loss: 1.2517 - val_acc: 0.5951\n",
      "Epoch 273/1000\n",
      "68095/68095 [==============================] - 1s 20us/step - loss: 1.2594 - acc: 0.5870 - val_loss: 1.2513 - val_acc: 0.5954\n",
      "Epoch 274/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.2589 - acc: 0.5868 - val_loss: 1.2509 - val_acc: 0.5945\n",
      "Epoch 275/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.2586 - acc: 0.5868 - val_loss: 1.2505 - val_acc: 0.5955\n",
      "Epoch 276/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.2582 - acc: 0.5870 - val_loss: 1.2501 - val_acc: 0.5955\n",
      "Epoch 277/1000\n",
      "68095/68095 [==============================] - 1s 19us/step - loss: 1.2578 - acc: 0.5877 - val_loss: 1.2497 - val_acc: 0.5947\n",
      "Epoch 278/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.2574 - acc: 0.5871 - val_loss: 1.2493 - val_acc: 0.5956\n",
      "Epoch 279/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.2570 - acc: 0.5878 - val_loss: 1.2490 - val_acc: 0.5945\n",
      "Epoch 280/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.2566 - acc: 0.5878 - val_loss: 1.2485 - val_acc: 0.5957\n",
      "Epoch 281/1000\n",
      "68095/68095 [==============================] - 1s 21us/step - loss: 1.2562 - acc: 0.5878 - val_loss: 1.2482 - val_acc: 0.5964\n",
      "Epoch 282/1000\n",
      "68095/68095 [==============================] - 1s 21us/step - loss: 1.2558 - acc: 0.5877 - val_loss: 1.2478 - val_acc: 0.5952\n",
      "Epoch 283/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.2554 - acc: 0.5878 - val_loss: 1.2475 - val_acc: 0.5950\n",
      "Epoch 284/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.2551 - acc: 0.5884 - val_loss: 1.2471 - val_acc: 0.5952\n",
      "Epoch 285/1000\n",
      "68095/68095 [==============================] - 2s 28us/step - loss: 1.2547 - acc: 0.5876 - val_loss: 1.2467 - val_acc: 0.5961\n",
      "Epoch 286/1000\n",
      "68095/68095 [==============================] - 2s 24us/step - loss: 1.2543 - acc: 0.5875 - val_loss: 1.2463 - val_acc: 0.5972\n",
      "Epoch 287/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.2539 - acc: 0.5887 - val_loss: 1.2459 - val_acc: 0.5966\n",
      "Epoch 288/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.2535 - acc: 0.5883 - val_loss: 1.2455 - val_acc: 0.5966\n",
      "Epoch 289/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.2532 - acc: 0.5888 - val_loss: 1.2451 - val_acc: 0.5965\n",
      "Epoch 290/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.2528 - acc: 0.5882 - val_loss: 1.2448 - val_acc: 0.5963\n",
      "Epoch 291/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.2524 - acc: 0.5888 - val_loss: 1.2444 - val_acc: 0.5979\n",
      "Epoch 292/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.2521 - acc: 0.5893 - val_loss: 1.2441 - val_acc: 0.5963\n",
      "Epoch 293/1000\n",
      "68095/68095 [==============================] - 1s 10us/step - loss: 1.2517 - acc: 0.5890 - val_loss: 1.2437 - val_acc: 0.5973\n",
      "Epoch 294/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.2513 - acc: 0.5893 - val_loss: 1.2433 - val_acc: 0.5957\n",
      "Epoch 295/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.2510 - acc: 0.5890 - val_loss: 1.2430 - val_acc: 0.5963\n",
      "Epoch 296/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.2506 - acc: 0.5890 - val_loss: 1.2426 - val_acc: 0.5984\n",
      "Epoch 297/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.2503 - acc: 0.5899 - val_loss: 1.2423 - val_acc: 0.5971\n",
      "Epoch 298/1000\n",
      "68095/68095 [==============================] - 1s 10us/step - loss: 1.2499 - acc: 0.5892 - val_loss: 1.2419 - val_acc: 0.5982\n",
      "Epoch 299/1000\n",
      "68095/68095 [==============================] - 1s 10us/step - loss: 1.2495 - acc: 0.5895 - val_loss: 1.2415 - val_acc: 0.5979\n",
      "Epoch 300/1000\n",
      "68095/68095 [==============================] - 1s 10us/step - loss: 1.2492 - acc: 0.5898 - val_loss: 1.2412 - val_acc: 0.5982\n",
      "Epoch 301/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.2488 - acc: 0.5898 - val_loss: 1.2408 - val_acc: 0.5984\n",
      "Epoch 302/1000\n",
      "68095/68095 [==============================] - 2s 27us/step - loss: 1.2485 - acc: 0.5899 - val_loss: 1.2405 - val_acc: 0.5969\n",
      "Epoch 303/1000\n",
      "68095/68095 [==============================] - 1s 20us/step - loss: 1.2481 - acc: 0.5901 - val_loss: 1.2401 - val_acc: 0.5977\n",
      "Epoch 304/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.2478 - acc: 0.5895 - val_loss: 1.2398 - val_acc: 0.5987\n",
      "Epoch 305/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.2474 - acc: 0.5903 - val_loss: 1.2394 - val_acc: 0.5984\n",
      "Epoch 306/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.2471 - acc: 0.5900 - val_loss: 1.2391 - val_acc: 0.5987\n",
      "Epoch 307/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.2467 - acc: 0.5904 - val_loss: 1.2387 - val_acc: 0.5976\n",
      "Epoch 308/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.2464 - acc: 0.5904 - val_loss: 1.2384 - val_acc: 0.5976\n",
      "Epoch 309/1000\n",
      "68095/68095 [==============================] - 1s 21us/step - loss: 1.2461 - acc: 0.5903 - val_loss: 1.2381 - val_acc: 0.5984\n",
      "Epoch 310/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.2457 - acc: 0.5905 - val_loss: 1.2377 - val_acc: 0.5982\n",
      "Epoch 311/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.2454 - acc: 0.5906 - val_loss: 1.2374 - val_acc: 0.5988\n",
      "Epoch 312/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.2450 - acc: 0.5903 - val_loss: 1.2370 - val_acc: 0.5982\n",
      "Epoch 313/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.2447 - acc: 0.5906 - val_loss: 1.2367 - val_acc: 0.5988\n",
      "Epoch 314/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.2444 - acc: 0.5904 - val_loss: 1.2364 - val_acc: 0.5987\n",
      "Epoch 315/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.2441 - acc: 0.5907 - val_loss: 1.2361 - val_acc: 0.5992\n",
      "Epoch 316/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.2437 - acc: 0.5913 - val_loss: 1.2357 - val_acc: 0.5987\n",
      "Epoch 317/1000\n",
      "68095/68095 [==============================] - 1s 10us/step - loss: 1.2434 - acc: 0.5906 - val_loss: 1.2354 - val_acc: 0.5989\n",
      "Epoch 318/1000\n",
      "68095/68095 [==============================] - 1s 20us/step - loss: 1.2431 - acc: 0.5907 - val_loss: 1.2351 - val_acc: 0.5993\n",
      "Epoch 319/1000\n",
      "68095/68095 [==============================] - 2s 25us/step - loss: 1.2427 - acc: 0.5914 - val_loss: 1.2347 - val_acc: 0.5986\n",
      "Epoch 320/1000\n",
      "68095/68095 [==============================] - 2s 22us/step - loss: 1.2424 - acc: 0.5914 - val_loss: 1.2344 - val_acc: 0.5993\n",
      "Epoch 321/1000\n",
      "68095/68095 [==============================] - 2s 26us/step - loss: 1.2421 - acc: 0.5912 - val_loss: 1.2341 - val_acc: 0.5984\n",
      "Epoch 322/1000\n",
      "68095/68095 [==============================] - 2s 25us/step - loss: 1.2418 - acc: 0.5911 - val_loss: 1.2338 - val_acc: 0.5998\n",
      "Epoch 323/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.2414 - acc: 0.5913 - val_loss: 1.2335 - val_acc: 0.5988\n",
      "Epoch 324/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.2411 - acc: 0.5915 - val_loss: 1.2331 - val_acc: 0.5993\n",
      "Epoch 325/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.2408 - acc: 0.5916 - val_loss: 1.2328 - val_acc: 0.5989\n",
      "Epoch 326/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.2405 - acc: 0.5913 - val_loss: 1.2326 - val_acc: 0.5988\n",
      "Epoch 327/1000\n",
      "68095/68095 [==============================] - 1s 22us/step - loss: 1.2402 - acc: 0.5915 - val_loss: 1.2322 - val_acc: 0.6002\n",
      "Epoch 328/1000\n",
      "68095/68095 [==============================] - 2s 23us/step - loss: 1.2399 - acc: 0.5912 - val_loss: 1.2319 - val_acc: 0.5992\n",
      "Epoch 329/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.2395 - acc: 0.5918 - val_loss: 1.2316 - val_acc: 0.5997\n",
      "Epoch 330/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.2392 - acc: 0.5921 - val_loss: 1.2313 - val_acc: 0.5998\n",
      "Epoch 331/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.2389 - acc: 0.5915 - val_loss: 1.2310 - val_acc: 0.6011\n",
      "Epoch 332/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.2386 - acc: 0.5916 - val_loss: 1.2306 - val_acc: 0.6006\n",
      "Epoch 333/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.2383 - acc: 0.5922 - val_loss: 1.2304 - val_acc: 0.6006\n",
      "Epoch 334/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.2380 - acc: 0.5920 - val_loss: 1.2300 - val_acc: 0.6000\n",
      "Epoch 335/1000\n",
      "68095/68095 [==============================] - 1s 10us/step - loss: 1.2377 - acc: 0.5917 - val_loss: 1.2297 - val_acc: 0.6000\n",
      "Epoch 336/1000\n",
      "68095/68095 [==============================] - 1s 10us/step - loss: 1.2374 - acc: 0.5925 - val_loss: 1.2294 - val_acc: 0.6002\n",
      "Epoch 337/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.2371 - acc: 0.5925 - val_loss: 1.2291 - val_acc: 0.6006\n",
      "Epoch 338/1000\n",
      "68095/68095 [==============================] - 1s 10us/step - loss: 1.2368 - acc: 0.5925 - val_loss: 1.2288 - val_acc: 0.6002\n",
      "Epoch 339/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.2365 - acc: 0.5924 - val_loss: 1.2285 - val_acc: 0.6006\n",
      "Epoch 340/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.2362 - acc: 0.5927 - val_loss: 1.2283 - val_acc: 0.6000\n",
      "Epoch 341/1000\n",
      "68095/68095 [==============================] - 1s 10us/step - loss: 1.2359 - acc: 0.5925 - val_loss: 1.2279 - val_acc: 0.6009\n",
      "Epoch 342/1000\n",
      "68095/68095 [==============================] - 1s 10us/step - loss: 1.2356 - acc: 0.5927 - val_loss: 1.2277 - val_acc: 0.6004\n",
      "Epoch 343/1000\n",
      "68095/68095 [==============================] - 1s 9us/step - loss: 1.2353 - acc: 0.5928 - val_loss: 1.2273 - val_acc: 0.6003\n",
      "Epoch 344/1000\n",
      "68095/68095 [==============================] - 1s 10us/step - loss: 1.2350 - acc: 0.5929 - val_loss: 1.2271 - val_acc: 0.5997\n",
      "Epoch 345/1000\n",
      "68095/68095 [==============================] - 1s 10us/step - loss: 1.2347 - acc: 0.5923 - val_loss: 1.2268 - val_acc: 0.6009\n",
      "Epoch 346/1000\n",
      "68095/68095 [==============================] - 1s 10us/step - loss: 1.2344 - acc: 0.5930 - val_loss: 1.2265 - val_acc: 0.6003\n",
      "Epoch 347/1000\n",
      "68095/68095 [==============================] - 1s 10us/step - loss: 1.2341 - acc: 0.5927 - val_loss: 1.2262 - val_acc: 0.6013\n",
      "Epoch 348/1000\n",
      "68095/68095 [==============================] - 1s 10us/step - loss: 1.2338 - acc: 0.5928 - val_loss: 1.2259 - val_acc: 0.6013\n",
      "Epoch 349/1000\n",
      "68095/68095 [==============================] - 1s 9us/step - loss: 1.2335 - acc: 0.5928 - val_loss: 1.2256 - val_acc: 0.6019\n",
      "Epoch 350/1000\n",
      "68095/68095 [==============================] - 1s 10us/step - loss: 1.2332 - acc: 0.5928 - val_loss: 1.2253 - val_acc: 0.6012\n",
      "Epoch 351/1000\n",
      "68095/68095 [==============================] - 1s 10us/step - loss: 1.2330 - acc: 0.5930 - val_loss: 1.2250 - val_acc: 0.6016\n",
      "Epoch 352/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.2327 - acc: 0.5928 - val_loss: 1.2247 - val_acc: 0.6019\n",
      "Epoch 353/1000\n",
      "68095/68095 [==============================] - 1s 20us/step - loss: 1.2324 - acc: 0.5934 - val_loss: 1.2245 - val_acc: 0.6004\n",
      "Epoch 354/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.2321 - acc: 0.5932 - val_loss: 1.2242 - val_acc: 0.6011\n",
      "Epoch 355/1000\n",
      "68095/68095 [==============================] - 2s 22us/step - loss: 1.2318 - acc: 0.5932 - val_loss: 1.2239 - val_acc: 0.6020\n",
      "Epoch 356/1000\n",
      "68095/68095 [==============================] - 1s 22us/step - loss: 1.2316 - acc: 0.5933 - val_loss: 1.2236 - val_acc: 0.6010\n",
      "Epoch 357/1000\n",
      "68095/68095 [==============================] - 2s 24us/step - loss: 1.2313 - acc: 0.5936 - val_loss: 1.2233 - val_acc: 0.6016\n",
      "Epoch 358/1000\n",
      "68095/68095 [==============================] - 1s 19us/step - loss: 1.2310 - acc: 0.5935 - val_loss: 1.2230 - val_acc: 0.6020\n",
      "Epoch 359/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.2307 - acc: 0.5935 - val_loss: 1.2228 - val_acc: 0.6021\n",
      "Epoch 360/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.2305 - acc: 0.5938 - val_loss: 1.2225 - val_acc: 0.6028\n",
      "Epoch 361/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.2302 - acc: 0.5938 - val_loss: 1.2223 - val_acc: 0.6020\n",
      "Epoch 362/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.2299 - acc: 0.5937 - val_loss: 1.2220 - val_acc: 0.6027\n",
      "Epoch 363/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.2296 - acc: 0.5937 - val_loss: 1.2217 - val_acc: 0.6022\n",
      "Epoch 364/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.2293 - acc: 0.5939 - val_loss: 1.2214 - val_acc: 0.6027\n",
      "Epoch 365/1000\n",
      "68095/68095 [==============================] - 2s 27us/step - loss: 1.2291 - acc: 0.5939 - val_loss: 1.2212 - val_acc: 0.6033\n",
      "Epoch 366/1000\n",
      "68095/68095 [==============================] - 2s 35us/step - loss: 1.2288 - acc: 0.5938 - val_loss: 1.2209 - val_acc: 0.6027\n",
      "Epoch 367/1000\n",
      "68095/68095 [==============================] - 2s 29us/step - loss: 1.2286 - acc: 0.5941 - val_loss: 1.2206 - val_acc: 0.6027\n",
      "Epoch 368/1000\n",
      "68095/68095 [==============================] - 1s 19us/step - loss: 1.2283 - acc: 0.5940 - val_loss: 1.2203 - val_acc: 0.6026\n",
      "Epoch 369/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.2280 - acc: 0.5937 - val_loss: 1.2202 - val_acc: 0.6022\n",
      "Epoch 370/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.2278 - acc: 0.5940 - val_loss: 1.2198 - val_acc: 0.6024\n",
      "Epoch 371/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.2275 - acc: 0.5940 - val_loss: 1.2196 - val_acc: 0.6027\n",
      "Epoch 372/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.2272 - acc: 0.5943 - val_loss: 1.2193 - val_acc: 0.6030\n",
      "Epoch 373/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.2270 - acc: 0.5944 - val_loss: 1.2190 - val_acc: 0.6031\n",
      "Epoch 374/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.2267 - acc: 0.5943 - val_loss: 1.2188 - val_acc: 0.6029\n",
      "Epoch 375/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.2264 - acc: 0.5947 - val_loss: 1.2185 - val_acc: 0.6028\n",
      "Epoch 376/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.2262 - acc: 0.5948 - val_loss: 1.2183 - val_acc: 0.6029\n",
      "Epoch 377/1000\n",
      "68095/68095 [==============================] - 1s 21us/step - loss: 1.2259 - acc: 0.5942 - val_loss: 1.2180 - val_acc: 0.6033\n",
      "Epoch 378/1000\n",
      "68095/68095 [==============================] - 1s 19us/step - loss: 1.2257 - acc: 0.5945 - val_loss: 1.2178 - val_acc: 0.6023\n",
      "Epoch 379/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.2254 - acc: 0.5948 - val_loss: 1.2175 - val_acc: 0.6029\n",
      "Epoch 380/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.2252 - acc: 0.5945 - val_loss: 1.2173 - val_acc: 0.6034\n",
      "Epoch 381/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.2249 - acc: 0.5949 - val_loss: 1.2170 - val_acc: 0.6039\n",
      "Epoch 382/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.2246 - acc: 0.5947 - val_loss: 1.2169 - val_acc: 0.6026\n",
      "Epoch 383/1000\n",
      "68095/68095 [==============================] - 3s 40us/step - loss: 1.2244 - acc: 0.5949 - val_loss: 1.2165 - val_acc: 0.6034\n",
      "Epoch 384/1000\n",
      "68095/68095 [==============================] - 2s 35us/step - loss: 1.2242 - acc: 0.5948 - val_loss: 1.2162 - val_acc: 0.6036\n",
      "Epoch 385/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.2239 - acc: 0.5947 - val_loss: 1.2160 - val_acc: 0.6033\n",
      "Epoch 386/1000\n",
      "68095/68095 [==============================] - 3s 41us/step - loss: 1.2236 - acc: 0.5957 - val_loss: 1.2157 - val_acc: 0.6035\n",
      "Epoch 387/1000\n",
      "68095/68095 [==============================] - 3s 43us/step - loss: 1.2234 - acc: 0.5952 - val_loss: 1.2155 - val_acc: 0.6037\n",
      "Epoch 388/1000\n",
      "68095/68095 [==============================] - 1s 18us/step - loss: 1.2231 - acc: 0.5952 - val_loss: 1.2152 - val_acc: 0.6039\n",
      "Epoch 389/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.2229 - acc: 0.5952 - val_loss: 1.2150 - val_acc: 0.6044\n",
      "Epoch 390/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.2226 - acc: 0.5954 - val_loss: 1.2147 - val_acc: 0.6032\n",
      "Epoch 391/1000\n",
      "68095/68095 [==============================] - 1s 18us/step - loss: 1.2224 - acc: 0.5955 - val_loss: 1.2145 - val_acc: 0.6039\n",
      "Epoch 392/1000\n",
      "68095/68095 [==============================] - 1s 22us/step - loss: 1.2222 - acc: 0.5951 - val_loss: 1.2143 - val_acc: 0.6043\n",
      "Epoch 393/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.2219 - acc: 0.5953 - val_loss: 1.2140 - val_acc: 0.6042\n",
      "Epoch 394/1000\n",
      "68095/68095 [==============================] - 2s 35us/step - loss: 1.2217 - acc: 0.5955 - val_loss: 1.2138 - val_acc: 0.6047\n",
      "Epoch 395/1000\n",
      "68095/68095 [==============================] - 3s 39us/step - loss: 1.2214 - acc: 0.5958 - val_loss: 1.2135 - val_acc: 0.6039\n",
      "Epoch 396/1000\n",
      "68095/68095 [==============================] - 2s 37us/step - loss: 1.2212 - acc: 0.5955 - val_loss: 1.2133 - val_acc: 0.6038\n",
      "Epoch 397/1000\n",
      "68095/68095 [==============================] - 2s 24us/step - loss: 1.2209 - acc: 0.5957 - val_loss: 1.2132 - val_acc: 0.6042\n",
      "Epoch 398/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.2207 - acc: 0.5959 - val_loss: 1.2128 - val_acc: 0.6045\n",
      "Epoch 399/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.2205 - acc: 0.5962 - val_loss: 1.2126 - val_acc: 0.6043\n",
      "Epoch 400/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.2203 - acc: 0.5956 - val_loss: 1.2123 - val_acc: 0.6051\n",
      "Epoch 401/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.2200 - acc: 0.5957 - val_loss: 1.2121 - val_acc: 0.6047\n",
      "Epoch 402/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.2198 - acc: 0.5957 - val_loss: 1.2120 - val_acc: 0.6035\n",
      "Epoch 403/1000\n",
      "68095/68095 [==============================] - 1s 21us/step - loss: 1.2196 - acc: 0.5956 - val_loss: 1.2117 - val_acc: 0.6044\n",
      "Epoch 404/1000\n",
      "68095/68095 [==============================] - 3s 42us/step - loss: 1.2193 - acc: 0.5960 - val_loss: 1.2115 - val_acc: 0.6033\n",
      "Epoch 405/1000\n",
      "68095/68095 [==============================] - 3s 41us/step - loss: 1.2191 - acc: 0.5961 - val_loss: 1.2112 - val_acc: 0.6039\n",
      "Epoch 406/1000\n",
      "68095/68095 [==============================] - 2s 24us/step - loss: 1.2188 - acc: 0.5964 - val_loss: 1.2110 - val_acc: 0.6055\n",
      "Epoch 407/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.2186 - acc: 0.5961 - val_loss: 1.2107 - val_acc: 0.6043\n",
      "Epoch 408/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.2184 - acc: 0.5962 - val_loss: 1.2105 - val_acc: 0.6050\n",
      "Epoch 409/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.2182 - acc: 0.5964 - val_loss: 1.2102 - val_acc: 0.6049\n",
      "Epoch 410/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.2179 - acc: 0.5961 - val_loss: 1.2101 - val_acc: 0.6045\n",
      "Epoch 411/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.2177 - acc: 0.5969 - val_loss: 1.2098 - val_acc: 0.6054\n",
      "Epoch 412/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.2175 - acc: 0.5965 - val_loss: 1.2096 - val_acc: 0.6050\n",
      "Epoch 413/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.2173 - acc: 0.5967 - val_loss: 1.2093 - val_acc: 0.6057\n",
      "Epoch 414/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.2170 - acc: 0.5966 - val_loss: 1.2091 - val_acc: 0.6048\n",
      "Epoch 415/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.2168 - acc: 0.5967 - val_loss: 1.2089 - val_acc: 0.6053\n",
      "Epoch 416/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.2166 - acc: 0.5969 - val_loss: 1.2088 - val_acc: 0.6047\n",
      "Epoch 417/1000\n",
      "68095/68095 [==============================] - 2s 32us/step - loss: 1.2164 - acc: 0.5969 - val_loss: 1.2085 - val_acc: 0.6056\n",
      "Epoch 418/1000\n",
      "68095/68095 [==============================] - 2s 32us/step - loss: 1.2161 - acc: 0.5964 - val_loss: 1.2083 - val_acc: 0.6054\n",
      "Epoch 419/1000\n",
      "68095/68095 [==============================] - 3s 40us/step - loss: 1.2159 - acc: 0.5969 - val_loss: 1.2081 - val_acc: 0.6049\n",
      "Epoch 420/1000\n",
      "68095/68095 [==============================] - 1s 18us/step - loss: 1.2157 - acc: 0.5966 - val_loss: 1.2079 - val_acc: 0.6047\n",
      "Epoch 421/1000\n",
      "68095/68095 [==============================] - 1s 19us/step - loss: 1.2155 - acc: 0.5967 - val_loss: 1.2076 - val_acc: 0.6057\n",
      "Epoch 422/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.2153 - acc: 0.5967 - val_loss: 1.2073 - val_acc: 0.6055\n",
      "Epoch 423/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.2150 - acc: 0.5970 - val_loss: 1.2072 - val_acc: 0.6058\n",
      "Epoch 424/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.2148 - acc: 0.5969 - val_loss: 1.2069 - val_acc: 0.6059\n",
      "Epoch 425/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.2146 - acc: 0.5962 - val_loss: 1.2067 - val_acc: 0.6053\n",
      "Epoch 426/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.2144 - acc: 0.5973 - val_loss: 1.2065 - val_acc: 0.6051\n",
      "Epoch 427/1000\n",
      "68095/68095 [==============================] - 2s 24us/step - loss: 1.2142 - acc: 0.5971 - val_loss: 1.2063 - val_acc: 0.6060\n",
      "Epoch 428/1000\n",
      "68095/68095 [==============================] - 3s 38us/step - loss: 1.2139 - acc: 0.5972 - val_loss: 1.2061 - val_acc: 0.6048\n",
      "Epoch 429/1000\n",
      "68095/68095 [==============================] - 2s 35us/step - loss: 1.2137 - acc: 0.5971 - val_loss: 1.2059 - val_acc: 0.6053\n",
      "Epoch 430/1000\n",
      "68095/68095 [==============================] - 3s 37us/step - loss: 1.2135 - acc: 0.5972 - val_loss: 1.2057 - val_acc: 0.6057\n",
      "Epoch 431/1000\n",
      "68095/68095 [==============================] - 3s 40us/step - loss: 1.2133 - acc: 0.5972 - val_loss: 1.2055 - val_acc: 0.6051\n",
      "Epoch 432/1000\n",
      "68095/68095 [==============================] - 2s 36us/step - loss: 1.2131 - acc: 0.5975 - val_loss: 1.2053 - val_acc: 0.6061\n",
      "Epoch 433/1000\n",
      "68095/68095 [==============================] - 1s 18us/step - loss: 1.2129 - acc: 0.5980 - val_loss: 1.2050 - val_acc: 0.6057\n",
      "Epoch 434/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.2127 - acc: 0.5970 - val_loss: 1.2049 - val_acc: 0.6053\n",
      "Epoch 435/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.2125 - acc: 0.5973 - val_loss: 1.2046 - val_acc: 0.6059\n",
      "Epoch 436/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.2122 - acc: 0.5978 - val_loss: 1.2045 - val_acc: 0.6056\n",
      "Epoch 437/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.2121 - acc: 0.5977 - val_loss: 1.2042 - val_acc: 0.6056\n",
      "Epoch 438/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.2119 - acc: 0.5975 - val_loss: 1.2040 - val_acc: 0.6063\n",
      "Epoch 439/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.2117 - acc: 0.5977 - val_loss: 1.2038 - val_acc: 0.6056\n",
      "Epoch 440/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.2115 - acc: 0.5978 - val_loss: 1.2036 - val_acc: 0.6058\n",
      "Epoch 441/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.2113 - acc: 0.5980 - val_loss: 1.2034 - val_acc: 0.6056\n",
      "Epoch 442/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.2110 - acc: 0.5975 - val_loss: 1.2032 - val_acc: 0.6051\n",
      "Epoch 443/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.2109 - acc: 0.5983 - val_loss: 1.2031 - val_acc: 0.6054\n",
      "Epoch 444/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.2107 - acc: 0.5980 - val_loss: 1.2028 - val_acc: 0.6060\n",
      "Epoch 445/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.2104 - acc: 0.5980 - val_loss: 1.2026 - val_acc: 0.6058\n",
      "Epoch 446/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.2102 - acc: 0.5980 - val_loss: 1.2024 - val_acc: 0.6057\n",
      "Epoch 447/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.2100 - acc: 0.5980 - val_loss: 1.2022 - val_acc: 0.6058\n",
      "Epoch 448/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.2099 - acc: 0.5982 - val_loss: 1.2020 - val_acc: 0.6064\n",
      "Epoch 449/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.2097 - acc: 0.5981 - val_loss: 1.2018 - val_acc: 0.6057\n",
      "Epoch 450/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.2095 - acc: 0.5980 - val_loss: 1.2016 - val_acc: 0.6059\n",
      "Epoch 451/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.2093 - acc: 0.5986 - val_loss: 1.2014 - val_acc: 0.6057\n",
      "Epoch 452/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.2091 - acc: 0.5986 - val_loss: 1.2012 - val_acc: 0.6064\n",
      "Epoch 453/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.2088 - acc: 0.5980 - val_loss: 1.2011 - val_acc: 0.6048\n",
      "Epoch 454/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.2087 - acc: 0.5984 - val_loss: 1.2008 - val_acc: 0.6059\n",
      "Epoch 455/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.2085 - acc: 0.5984 - val_loss: 1.2006 - val_acc: 0.6056\n",
      "Epoch 456/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.2083 - acc: 0.5985 - val_loss: 1.2005 - val_acc: 0.6059\n",
      "Epoch 457/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.2081 - acc: 0.5984 - val_loss: 1.2003 - val_acc: 0.6060\n",
      "Epoch 458/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.2079 - acc: 0.5985 - val_loss: 1.2000 - val_acc: 0.6061\n",
      "Epoch 459/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.2077 - acc: 0.5984 - val_loss: 1.1998 - val_acc: 0.6059\n",
      "Epoch 460/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.2075 - acc: 0.5988 - val_loss: 1.1997 - val_acc: 0.6061\n",
      "Epoch 461/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.2073 - acc: 0.5985 - val_loss: 1.1995 - val_acc: 0.6057\n",
      "Epoch 462/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.2071 - acc: 0.5990 - val_loss: 1.1993 - val_acc: 0.6060\n",
      "Epoch 463/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.2069 - acc: 0.5988 - val_loss: 1.1991 - val_acc: 0.6063\n",
      "Epoch 464/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.2067 - acc: 0.5988 - val_loss: 1.1990 - val_acc: 0.6049\n",
      "Epoch 465/1000\n",
      "68095/68095 [==============================] - 2s 23us/step - loss: 1.2066 - acc: 0.5989 - val_loss: 1.1988 - val_acc: 0.6061\n",
      "Epoch 466/1000\n",
      "68095/68095 [==============================] - 1s 21us/step - loss: 1.2064 - acc: 0.5991 - val_loss: 1.1985 - val_acc: 0.6064\n",
      "Epoch 467/1000\n",
      "68095/68095 [==============================] - 1s 19us/step - loss: 1.2062 - acc: 0.5989 - val_loss: 1.1983 - val_acc: 0.6065\n",
      "Epoch 468/1000\n",
      "68095/68095 [==============================] - 1s 22us/step - loss: 1.2060 - acc: 0.5990 - val_loss: 1.1981 - val_acc: 0.6062\n",
      "Epoch 469/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.2058 - acc: 0.5993 - val_loss: 1.1980 - val_acc: 0.6056\n",
      "Epoch 470/1000\n",
      "68095/68095 [==============================] - 1s 21us/step - loss: 1.2056 - acc: 0.5990 - val_loss: 1.1978 - val_acc: 0.6063\n",
      "Epoch 471/1000\n",
      "68095/68095 [==============================] - 1s 21us/step - loss: 1.2054 - acc: 0.5989 - val_loss: 1.1976 - val_acc: 0.6059\n",
      "Epoch 472/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.2053 - acc: 0.5992 - val_loss: 1.1975 - val_acc: 0.6060\n",
      "Epoch 473/1000\n",
      "68095/68095 [==============================] - 1s 21us/step - loss: 1.2051 - acc: 0.5995 - val_loss: 1.1973 - val_acc: 0.6055\n",
      "Epoch 474/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.2049 - acc: 0.5992 - val_loss: 1.1970 - val_acc: 0.6061\n",
      "Epoch 475/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.2047 - acc: 0.5996 - val_loss: 1.1969 - val_acc: 0.6067\n",
      "Epoch 476/1000\n",
      "68095/68095 [==============================] - 1s 19us/step - loss: 1.2045 - acc: 0.5992 - val_loss: 1.1967 - val_acc: 0.6067\n",
      "Epoch 477/1000\n",
      "68095/68095 [==============================] - 1s 20us/step - loss: 1.2043 - acc: 0.5989 - val_loss: 1.1966 - val_acc: 0.6061\n",
      "Epoch 478/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.2042 - acc: 0.5992 - val_loss: 1.1964 - val_acc: 0.6064\n",
      "Epoch 479/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.2040 - acc: 0.5995 - val_loss: 1.1962 - val_acc: 0.6074\n",
      "Epoch 480/1000\n",
      "68095/68095 [==============================] - 1s 10us/step - loss: 1.2038 - acc: 0.5993 - val_loss: 1.1960 - val_acc: 0.6075\n",
      "Epoch 481/1000\n",
      "68095/68095 [==============================] - 1s 10us/step - loss: 1.2037 - acc: 0.5996 - val_loss: 1.1958 - val_acc: 0.6072\n",
      "Epoch 482/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.2035 - acc: 0.5994 - val_loss: 1.1957 - val_acc: 0.6066\n",
      "Epoch 483/1000\n",
      "68095/68095 [==============================] - 1s 18us/step - loss: 1.2033 - acc: 0.5999 - val_loss: 1.1955 - val_acc: 0.6066\n",
      "Epoch 484/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.2031 - acc: 0.5997 - val_loss: 1.1953 - val_acc: 0.6072\n",
      "Epoch 485/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.2029 - acc: 0.5997 - val_loss: 1.1951 - val_acc: 0.6073\n",
      "Epoch 486/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.2028 - acc: 0.5996 - val_loss: 1.1949 - val_acc: 0.6074\n",
      "Epoch 487/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.2026 - acc: 0.5995 - val_loss: 1.1948 - val_acc: 0.6073\n",
      "Epoch 488/1000\n",
      "68095/68095 [==============================] - 1s 18us/step - loss: 1.2024 - acc: 0.5989 - val_loss: 1.1946 - val_acc: 0.6078\n",
      "Epoch 489/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.2022 - acc: 0.6000 - val_loss: 1.1944 - val_acc: 0.6073\n",
      "Epoch 490/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.2021 - acc: 0.6000 - val_loss: 1.1942 - val_acc: 0.6073\n",
      "Epoch 491/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.2019 - acc: 0.5997 - val_loss: 1.1941 - val_acc: 0.6076\n",
      "Epoch 492/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.2017 - acc: 0.5999 - val_loss: 1.1939 - val_acc: 0.6078\n",
      "Epoch 493/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.2016 - acc: 0.5996 - val_loss: 1.1938 - val_acc: 0.6071\n",
      "Epoch 494/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.2014 - acc: 0.6000 - val_loss: 1.1936 - val_acc: 0.6083\n",
      "Epoch 495/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.2012 - acc: 0.6000 - val_loss: 1.1934 - val_acc: 0.6074\n",
      "Epoch 496/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.2010 - acc: 0.5999 - val_loss: 1.1932 - val_acc: 0.6074\n",
      "Epoch 497/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.2009 - acc: 0.5998 - val_loss: 1.1931 - val_acc: 0.6074\n",
      "Epoch 498/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.2007 - acc: 0.5998 - val_loss: 1.1929 - val_acc: 0.6074\n",
      "Epoch 499/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.2005 - acc: 0.5999 - val_loss: 1.1928 - val_acc: 0.6080\n",
      "Epoch 500/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.2004 - acc: 0.6003 - val_loss: 1.1925 - val_acc: 0.6077\n",
      "Epoch 501/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.2002 - acc: 0.6001 - val_loss: 1.1924 - val_acc: 0.6071\n",
      "Epoch 502/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.2000 - acc: 0.5999 - val_loss: 1.1923 - val_acc: 0.6081\n",
      "Epoch 503/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1998 - acc: 0.6003 - val_loss: 1.1920 - val_acc: 0.6081\n",
      "Epoch 504/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1997 - acc: 0.6002 - val_loss: 1.1919 - val_acc: 0.6078\n",
      "Epoch 505/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1995 - acc: 0.6003 - val_loss: 1.1917 - val_acc: 0.6084\n",
      "Epoch 506/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1994 - acc: 0.6001 - val_loss: 1.1915 - val_acc: 0.6079\n",
      "Epoch 507/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1992 - acc: 0.6000 - val_loss: 1.1914 - val_acc: 0.6084\n",
      "Epoch 508/1000\n",
      "68095/68095 [==============================] - 1s 10us/step - loss: 1.1991 - acc: 0.6004 - val_loss: 1.1912 - val_acc: 0.6081\n",
      "Epoch 509/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1989 - acc: 0.6004 - val_loss: 1.1911 - val_acc: 0.6077\n",
      "Epoch 510/1000\n",
      "68095/68095 [==============================] - 1s 18us/step - loss: 1.1987 - acc: 0.6004 - val_loss: 1.1909 - val_acc: 0.6081\n",
      "Epoch 511/1000\n",
      "68095/68095 [==============================] - 1s 10us/step - loss: 1.1986 - acc: 0.6004 - val_loss: 1.1908 - val_acc: 0.6078\n",
      "Epoch 512/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1984 - acc: 0.6002 - val_loss: 1.1906 - val_acc: 0.6078\n",
      "Epoch 513/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1982 - acc: 0.6005 - val_loss: 1.1904 - val_acc: 0.6083\n",
      "Epoch 514/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1981 - acc: 0.6005 - val_loss: 1.1903 - val_acc: 0.6088\n",
      "Epoch 515/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1979 - acc: 0.6003 - val_loss: 1.1901 - val_acc: 0.6081\n",
      "Epoch 516/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1978 - acc: 0.6006 - val_loss: 1.1900 - val_acc: 0.6085\n",
      "Epoch 517/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1976 - acc: 0.6005 - val_loss: 1.1898 - val_acc: 0.6081\n",
      "Epoch 518/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1974 - acc: 0.6008 - val_loss: 1.1897 - val_acc: 0.6083\n",
      "Epoch 519/1000\n",
      "68095/68095 [==============================] - 1s 10us/step - loss: 1.1973 - acc: 0.6005 - val_loss: 1.1895 - val_acc: 0.6091\n",
      "Epoch 520/1000\n",
      "68095/68095 [==============================] - 1s 10us/step - loss: 1.1971 - acc: 0.6002 - val_loss: 1.1893 - val_acc: 0.6081\n",
      "Epoch 521/1000\n",
      "68095/68095 [==============================] - 1s 10us/step - loss: 1.1970 - acc: 0.6006 - val_loss: 1.1892 - val_acc: 0.6083\n",
      "Epoch 522/1000\n",
      "68095/68095 [==============================] - 1s 10us/step - loss: 1.1968 - acc: 0.6010 - val_loss: 1.1891 - val_acc: 0.6088\n",
      "Epoch 523/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1967 - acc: 0.6008 - val_loss: 1.1888 - val_acc: 0.6084\n",
      "Epoch 524/1000\n",
      "68095/68095 [==============================] - ETA: 0s - loss: 1.1964 - acc: 0.600 - 1s 22us/step - loss: 1.1965 - acc: 0.6003 - val_loss: 1.1887 - val_acc: 0.6090\n",
      "Epoch 525/1000\n",
      "68095/68095 [==============================] - 2s 25us/step - loss: 1.1963 - acc: 0.6009 - val_loss: 1.1886 - val_acc: 0.6080\n",
      "Epoch 526/1000\n",
      "68095/68095 [==============================] - 1s 20us/step - loss: 1.1962 - acc: 0.6008 - val_loss: 1.1884 - val_acc: 0.6091\n",
      "Epoch 527/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.1960 - acc: 0.6004 - val_loss: 1.1883 - val_acc: 0.6087\n",
      "Epoch 528/1000\n",
      "68095/68095 [==============================] - 2s 24us/step - loss: 1.1959 - acc: 0.6009 - val_loss: 1.1881 - val_acc: 0.6083\n",
      "Epoch 529/1000\n",
      "68095/68095 [==============================] - 2s 27us/step - loss: 1.1957 - acc: 0.6008 - val_loss: 1.1880 - val_acc: 0.6093\n",
      "Epoch 530/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.1956 - acc: 0.6005 - val_loss: 1.1878 - val_acc: 0.6086\n",
      "Epoch 531/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1954 - acc: 0.6014 - val_loss: 1.1876 - val_acc: 0.6088\n",
      "Epoch 532/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1953 - acc: 0.6014 - val_loss: 1.1875 - val_acc: 0.6080\n",
      "Epoch 533/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1951 - acc: 0.6008 - val_loss: 1.1873 - val_acc: 0.6090\n",
      "Epoch 534/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1950 - acc: 0.6010 - val_loss: 1.1872 - val_acc: 0.6081\n",
      "Epoch 535/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1948 - acc: 0.6012 - val_loss: 1.1870 - val_acc: 0.6094\n",
      "Epoch 536/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1947 - acc: 0.6013 - val_loss: 1.1869 - val_acc: 0.6090\n",
      "Epoch 537/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1945 - acc: 0.6009 - val_loss: 1.1867 - val_acc: 0.6090\n",
      "Epoch 538/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1944 - acc: 0.6010 - val_loss: 1.1866 - val_acc: 0.6086\n",
      "Epoch 539/1000\n",
      "68095/68095 [==============================] - ETA: 0s - loss: 1.1948 - acc: 0.600 - 1s 12us/step - loss: 1.1942 - acc: 0.6010 - val_loss: 1.1865 - val_acc: 0.6076\n",
      "Epoch 540/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.1941 - acc: 0.6016 - val_loss: 1.1863 - val_acc: 0.6088\n",
      "Epoch 541/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.1939 - acc: 0.6010 - val_loss: 1.1862 - val_acc: 0.6078\n",
      "Epoch 542/1000\n",
      "68095/68095 [==============================] - 1s 18us/step - loss: 1.1938 - acc: 0.6015 - val_loss: 1.1861 - val_acc: 0.6086\n",
      "Epoch 543/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.1937 - acc: 0.6013 - val_loss: 1.1858 - val_acc: 0.6094\n",
      "Epoch 544/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1935 - acc: 0.6015 - val_loss: 1.1857 - val_acc: 0.6091\n",
      "Epoch 545/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1934 - acc: 0.6014 - val_loss: 1.1855 - val_acc: 0.6091\n",
      "Epoch 546/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.1932 - acc: 0.6011 - val_loss: 1.1855 - val_acc: 0.6084\n",
      "Epoch 547/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1931 - acc: 0.6017 - val_loss: 1.1853 - val_acc: 0.6091\n",
      "Epoch 548/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1929 - acc: 0.6012 - val_loss: 1.1851 - val_acc: 0.6091\n",
      "Epoch 549/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1928 - acc: 0.6017 - val_loss: 1.1850 - val_acc: 0.6100\n",
      "Epoch 550/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1926 - acc: 0.6013 - val_loss: 1.1848 - val_acc: 0.6084\n",
      "Epoch 551/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1925 - acc: 0.6014 - val_loss: 1.1847 - val_acc: 0.6094\n",
      "Epoch 552/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1923 - acc: 0.6020 - val_loss: 1.1846 - val_acc: 0.6096\n",
      "Epoch 553/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.1922 - acc: 0.6017 - val_loss: 1.1845 - val_acc: 0.6078\n",
      "Epoch 554/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1921 - acc: 0.6016 - val_loss: 1.1843 - val_acc: 0.6091\n",
      "Epoch 555/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1919 - acc: 0.6020 - val_loss: 1.1842 - val_acc: 0.6089\n",
      "Epoch 556/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.1918 - acc: 0.6020 - val_loss: 1.1840 - val_acc: 0.6083\n",
      "Epoch 557/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.1916 - acc: 0.6019 - val_loss: 1.1839 - val_acc: 0.6100\n",
      "Epoch 558/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.1915 - acc: 0.6019 - val_loss: 1.1837 - val_acc: 0.6091\n",
      "Epoch 559/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.1914 - acc: 0.6017 - val_loss: 1.1836 - val_acc: 0.6103\n",
      "Epoch 560/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.1912 - acc: 0.6017 - val_loss: 1.1834 - val_acc: 0.6097\n",
      "Epoch 561/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1911 - acc: 0.6015 - val_loss: 1.1833 - val_acc: 0.6101\n",
      "Epoch 562/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1909 - acc: 0.6017 - val_loss: 1.1832 - val_acc: 0.6095\n",
      "Epoch 563/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1908 - acc: 0.6017 - val_loss: 1.1830 - val_acc: 0.6098\n",
      "Epoch 564/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1907 - acc: 0.6018 - val_loss: 1.1829 - val_acc: 0.6094\n",
      "Epoch 565/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1905 - acc: 0.6022 - val_loss: 1.1827 - val_acc: 0.6095\n",
      "Epoch 566/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1904 - acc: 0.6024 - val_loss: 1.1826 - val_acc: 0.6098\n",
      "Epoch 567/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1903 - acc: 0.6018 - val_loss: 1.1825 - val_acc: 0.6101\n",
      "Epoch 568/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1901 - acc: 0.6019 - val_loss: 1.1824 - val_acc: 0.6084\n",
      "Epoch 569/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1900 - acc: 0.6024 - val_loss: 1.1822 - val_acc: 0.6096\n",
      "Epoch 570/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1899 - acc: 0.6022 - val_loss: 1.1821 - val_acc: 0.6098\n",
      "Epoch 571/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1897 - acc: 0.6020 - val_loss: 1.1819 - val_acc: 0.6096\n",
      "Epoch 572/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1896 - acc: 0.6022 - val_loss: 1.1818 - val_acc: 0.6100\n",
      "Epoch 573/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1895 - acc: 0.6021 - val_loss: 1.1817 - val_acc: 0.6102\n",
      "Epoch 574/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1893 - acc: 0.6023 - val_loss: 1.1816 - val_acc: 0.6101\n",
      "Epoch 575/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1892 - acc: 0.6021 - val_loss: 1.1814 - val_acc: 0.6097\n",
      "Epoch 576/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1891 - acc: 0.6023 - val_loss: 1.1813 - val_acc: 0.6097\n",
      "Epoch 577/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1889 - acc: 0.6018 - val_loss: 1.1811 - val_acc: 0.6107\n",
      "Epoch 578/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1888 - acc: 0.6021 - val_loss: 1.1810 - val_acc: 0.6100\n",
      "Epoch 579/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1887 - acc: 0.6027 - val_loss: 1.1809 - val_acc: 0.6101\n",
      "Epoch 580/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1885 - acc: 0.6024 - val_loss: 1.1808 - val_acc: 0.6099\n",
      "Epoch 581/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1884 - acc: 0.6027 - val_loss: 1.1806 - val_acc: 0.6104\n",
      "Epoch 582/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1883 - acc: 0.6025 - val_loss: 1.1805 - val_acc: 0.6096\n",
      "Epoch 583/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1881 - acc: 0.6020 - val_loss: 1.1803 - val_acc: 0.6100\n",
      "Epoch 584/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1880 - acc: 0.6025 - val_loss: 1.1803 - val_acc: 0.6100\n",
      "Epoch 585/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1879 - acc: 0.6023 - val_loss: 1.1801 - val_acc: 0.6104\n",
      "Epoch 586/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1877 - acc: 0.6027 - val_loss: 1.1800 - val_acc: 0.6101\n",
      "Epoch 587/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1876 - acc: 0.6024 - val_loss: 1.1799 - val_acc: 0.6105\n",
      "Epoch 588/1000\n",
      "68095/68095 [==============================] - 1s 10us/step - loss: 1.1875 - acc: 0.6022 - val_loss: 1.1797 - val_acc: 0.6097\n",
      "Epoch 589/1000\n",
      "68095/68095 [==============================] - 1s 10us/step - loss: 1.1874 - acc: 0.6027 - val_loss: 1.1796 - val_acc: 0.6107\n",
      "Epoch 590/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1872 - acc: 0.6027 - val_loss: 1.1795 - val_acc: 0.6105\n",
      "Epoch 591/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1871 - acc: 0.6022 - val_loss: 1.1793 - val_acc: 0.6100\n",
      "Epoch 592/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1870 - acc: 0.6027 - val_loss: 1.1792 - val_acc: 0.6101\n",
      "Epoch 593/1000\n",
      "68095/68095 [==============================] - 1s 10us/step - loss: 1.1868 - acc: 0.6025 - val_loss: 1.1791 - val_acc: 0.6103\n",
      "Epoch 594/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1867 - acc: 0.6027 - val_loss: 1.1790 - val_acc: 0.6108\n",
      "Epoch 595/1000\n",
      "68095/68095 [==============================] - 1s 10us/step - loss: 1.1866 - acc: 0.6028 - val_loss: 1.1788 - val_acc: 0.6104\n",
      "Epoch 596/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1865 - acc: 0.6031 - val_loss: 1.1787 - val_acc: 0.6103\n",
      "Epoch 597/1000\n",
      "68095/68095 [==============================] - 1s 10us/step - loss: 1.1863 - acc: 0.6028 - val_loss: 1.1786 - val_acc: 0.6110\n",
      "Epoch 598/1000\n",
      "68095/68095 [==============================] - 1s 10us/step - loss: 1.1862 - acc: 0.6029 - val_loss: 1.1785 - val_acc: 0.6103\n",
      "Epoch 599/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1861 - acc: 0.6027 - val_loss: 1.1783 - val_acc: 0.6103\n",
      "Epoch 600/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.1860 - acc: 0.6032 - val_loss: 1.1782 - val_acc: 0.6105\n",
      "Epoch 601/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1859 - acc: 0.6027 - val_loss: 1.1781 - val_acc: 0.6104\n",
      "Epoch 602/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1857 - acc: 0.6028 - val_loss: 1.1780 - val_acc: 0.6102\n",
      "Epoch 603/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1856 - acc: 0.6027 - val_loss: 1.1779 - val_acc: 0.6105\n",
      "Epoch 604/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1855 - acc: 0.6031 - val_loss: 1.1777 - val_acc: 0.6115\n",
      "Epoch 605/1000\n",
      "68095/68095 [==============================] - 1s 10us/step - loss: 1.1854 - acc: 0.6030 - val_loss: 1.1776 - val_acc: 0.6108\n",
      "Epoch 606/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1852 - acc: 0.6033 - val_loss: 1.1775 - val_acc: 0.6105\n",
      "Epoch 607/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1851 - acc: 0.6032 - val_loss: 1.1774 - val_acc: 0.6109\n",
      "Epoch 608/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1850 - acc: 0.6028 - val_loss: 1.1772 - val_acc: 0.6110\n",
      "Epoch 609/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1848 - acc: 0.6032 - val_loss: 1.1771 - val_acc: 0.6097\n",
      "Epoch 610/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1847 - acc: 0.6033 - val_loss: 1.1770 - val_acc: 0.6108\n",
      "Epoch 611/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1846 - acc: 0.6028 - val_loss: 1.1769 - val_acc: 0.6109\n",
      "Epoch 612/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1845 - acc: 0.6030 - val_loss: 1.1767 - val_acc: 0.6108\n",
      "Epoch 613/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1844 - acc: 0.6035 - val_loss: 1.1766 - val_acc: 0.6107\n",
      "Epoch 614/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1843 - acc: 0.6032 - val_loss: 1.1765 - val_acc: 0.6109\n",
      "Epoch 615/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1842 - acc: 0.6031 - val_loss: 1.1764 - val_acc: 0.6108\n",
      "Epoch 616/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1840 - acc: 0.6031 - val_loss: 1.1763 - val_acc: 0.6102\n",
      "Epoch 617/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1839 - acc: 0.6028 - val_loss: 1.1761 - val_acc: 0.6110\n",
      "Epoch 618/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1838 - acc: 0.6032 - val_loss: 1.1761 - val_acc: 0.6116\n",
      "Epoch 619/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1837 - acc: 0.6032 - val_loss: 1.1759 - val_acc: 0.6108\n",
      "Epoch 620/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1836 - acc: 0.6030 - val_loss: 1.1758 - val_acc: 0.6110\n",
      "Epoch 621/1000\n",
      "68095/68095 [==============================] - 1s 10us/step - loss: 1.1835 - acc: 0.6032 - val_loss: 1.1757 - val_acc: 0.6107\n",
      "Epoch 622/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1833 - acc: 0.6035 - val_loss: 1.1756 - val_acc: 0.6107\n",
      "Epoch 623/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1832 - acc: 0.6033 - val_loss: 1.1754 - val_acc: 0.6107\n",
      "Epoch 624/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1831 - acc: 0.6035 - val_loss: 1.1753 - val_acc: 0.6109\n",
      "Epoch 625/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1830 - acc: 0.6034 - val_loss: 1.1752 - val_acc: 0.6108\n",
      "Epoch 626/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1829 - acc: 0.6033 - val_loss: 1.1751 - val_acc: 0.6108\n",
      "Epoch 627/1000\n",
      "68095/68095 [==============================] - 1s 10us/step - loss: 1.1827 - acc: 0.6037 - val_loss: 1.1751 - val_acc: 0.6104\n",
      "Epoch 628/1000\n",
      "68095/68095 [==============================] - 1s 10us/step - loss: 1.1826 - acc: 0.6031 - val_loss: 1.1749 - val_acc: 0.6109\n",
      "Epoch 629/1000\n",
      "68095/68095 [==============================] - 1s 10us/step - loss: 1.1825 - acc: 0.6029 - val_loss: 1.1748 - val_acc: 0.6106\n",
      "Epoch 630/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1824 - acc: 0.6037 - val_loss: 1.1747 - val_acc: 0.6110\n",
      "Epoch 631/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1823 - acc: 0.6030 - val_loss: 1.1746 - val_acc: 0.6111\n",
      "Epoch 632/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1822 - acc: 0.6035 - val_loss: 1.1744 - val_acc: 0.6105\n",
      "Epoch 633/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1821 - acc: 0.6035 - val_loss: 1.1743 - val_acc: 0.6110\n",
      "Epoch 634/1000\n",
      "68095/68095 [==============================] - 1s 10us/step - loss: 1.1820 - acc: 0.6036 - val_loss: 1.1742 - val_acc: 0.6108\n",
      "Epoch 635/1000\n",
      "68095/68095 [==============================] - 1s 10us/step - loss: 1.1818 - acc: 0.6039 - val_loss: 1.1741 - val_acc: 0.6113\n",
      "Epoch 636/1000\n",
      "68095/68095 [==============================] - 1s 10us/step - loss: 1.1817 - acc: 0.6033 - val_loss: 1.1740 - val_acc: 0.6113\n",
      "Epoch 637/1000\n",
      "68095/68095 [==============================] - 1s 10us/step - loss: 1.1816 - acc: 0.6034 - val_loss: 1.1739 - val_acc: 0.6111\n",
      "Epoch 638/1000\n",
      "68095/68095 [==============================] - 1s 10us/step - loss: 1.1815 - acc: 0.6032 - val_loss: 1.1738 - val_acc: 0.6113\n",
      "Epoch 639/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1814 - acc: 0.6038 - val_loss: 1.1737 - val_acc: 0.6115\n",
      "Epoch 640/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1813 - acc: 0.6037 - val_loss: 1.1735 - val_acc: 0.6112\n",
      "Epoch 641/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1812 - acc: 0.6035 - val_loss: 1.1734 - val_acc: 0.6112\n",
      "Epoch 642/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1811 - acc: 0.6036 - val_loss: 1.1733 - val_acc: 0.6113\n",
      "Epoch 643/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1810 - acc: 0.6039 - val_loss: 1.1732 - val_acc: 0.6114\n",
      "Epoch 644/1000\n",
      "68095/68095 [==============================] - 1s 10us/step - loss: 1.1809 - acc: 0.6035 - val_loss: 1.1731 - val_acc: 0.6114\n",
      "Epoch 645/1000\n",
      "68095/68095 [==============================] - 1s 10us/step - loss: 1.1807 - acc: 0.6037 - val_loss: 1.1730 - val_acc: 0.6119\n",
      "Epoch 646/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1806 - acc: 0.6038 - val_loss: 1.1729 - val_acc: 0.6115\n",
      "Epoch 647/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1805 - acc: 0.6038 - val_loss: 1.1728 - val_acc: 0.6110\n",
      "Epoch 648/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1804 - acc: 0.6038 - val_loss: 1.1727 - val_acc: 0.6113\n",
      "Epoch 649/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1803 - acc: 0.6037 - val_loss: 1.1726 - val_acc: 0.6115\n",
      "Epoch 650/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1802 - acc: 0.6037 - val_loss: 1.1724 - val_acc: 0.6117\n",
      "Epoch 651/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1801 - acc: 0.6038 - val_loss: 1.1724 - val_acc: 0.6111\n",
      "Epoch 652/1000\n",
      "68095/68095 [==============================] - 1s 10us/step - loss: 1.1800 - acc: 0.6036 - val_loss: 1.1723 - val_acc: 0.6111\n",
      "Epoch 653/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1799 - acc: 0.6038 - val_loss: 1.1722 - val_acc: 0.6113\n",
      "Epoch 654/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1798 - acc: 0.6039 - val_loss: 1.1720 - val_acc: 0.6112\n",
      "Epoch 655/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1797 - acc: 0.6037 - val_loss: 1.1719 - val_acc: 0.6113\n",
      "Epoch 656/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1796 - acc: 0.6037 - val_loss: 1.1718 - val_acc: 0.6112\n",
      "Epoch 657/1000\n",
      "68095/68095 [==============================] - 1s 10us/step - loss: 1.1794 - acc: 0.6036 - val_loss: 1.1717 - val_acc: 0.6120\n",
      "Epoch 658/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1793 - acc: 0.6036 - val_loss: 1.1717 - val_acc: 0.6111\n",
      "Epoch 659/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1792 - acc: 0.6041 - val_loss: 1.1715 - val_acc: 0.6115\n",
      "Epoch 660/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1791 - acc: 0.6039 - val_loss: 1.1714 - val_acc: 0.6113\n",
      "Epoch 661/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1790 - acc: 0.6039 - val_loss: 1.1714 - val_acc: 0.6121\n",
      "Epoch 662/1000\n",
      "68095/68095 [==============================] - 1s 10us/step - loss: 1.1789 - acc: 0.6039 - val_loss: 1.1712 - val_acc: 0.6125\n",
      "Epoch 663/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1788 - acc: 0.6038 - val_loss: 1.1711 - val_acc: 0.6114\n",
      "Epoch 664/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1787 - acc: 0.6040 - val_loss: 1.1710 - val_acc: 0.6117\n",
      "Epoch 665/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1786 - acc: 0.6041 - val_loss: 1.1710 - val_acc: 0.6111\n",
      "Epoch 666/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1785 - acc: 0.6042 - val_loss: 1.1708 - val_acc: 0.6112\n",
      "Epoch 667/1000\n",
      "68095/68095 [==============================] - 1s 10us/step - loss: 1.1784 - acc: 0.6038 - val_loss: 1.1707 - val_acc: 0.6116\n",
      "Epoch 668/1000\n",
      "68095/68095 [==============================] - 1s 10us/step - loss: 1.1783 - acc: 0.6037 - val_loss: 1.1706 - val_acc: 0.6120\n",
      "Epoch 669/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1782 - acc: 0.6038 - val_loss: 1.1704 - val_acc: 0.6122\n",
      "Epoch 670/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1781 - acc: 0.6040 - val_loss: 1.1704 - val_acc: 0.6120\n",
      "Epoch 671/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.1780 - acc: 0.6038 - val_loss: 1.1703 - val_acc: 0.6117\n",
      "Epoch 672/1000\n",
      "68095/68095 [==============================] - 1s 10us/step - loss: 1.1779 - acc: 0.6042 - val_loss: 1.1702 - val_acc: 0.6112\n",
      "Epoch 673/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1778 - acc: 0.6039 - val_loss: 1.1701 - val_acc: 0.6114\n",
      "Epoch 674/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1777 - acc: 0.6040 - val_loss: 1.1700 - val_acc: 0.6117\n",
      "Epoch 675/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1776 - acc: 0.6040 - val_loss: 1.1699 - val_acc: 0.6124\n",
      "Epoch 676/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1775 - acc: 0.6042 - val_loss: 1.1698 - val_acc: 0.6111\n",
      "Epoch 677/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1774 - acc: 0.6042 - val_loss: 1.1697 - val_acc: 0.6120\n",
      "Epoch 678/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.1773 - acc: 0.6042 - val_loss: 1.1696 - val_acc: 0.6111\n",
      "Epoch 679/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1772 - acc: 0.6039 - val_loss: 1.1695 - val_acc: 0.6115\n",
      "Epoch 680/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.1771 - acc: 0.6041 - val_loss: 1.1693 - val_acc: 0.6120\n",
      "Epoch 681/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1770 - acc: 0.6044 - val_loss: 1.1692 - val_acc: 0.6121\n",
      "Epoch 682/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1769 - acc: 0.6044 - val_loss: 1.1691 - val_acc: 0.6118\n",
      "Epoch 683/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1768 - acc: 0.6041 - val_loss: 1.1691 - val_acc: 0.6114\n",
      "Epoch 684/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.1767 - acc: 0.6043 - val_loss: 1.1689 - val_acc: 0.6117\n",
      "Epoch 685/1000\n",
      "68095/68095 [==============================] - 2s 22us/step - loss: 1.1766 - acc: 0.6043 - val_loss: 1.1688 - val_acc: 0.6120\n",
      "Epoch 686/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1765 - acc: 0.6043 - val_loss: 1.1688 - val_acc: 0.6118\n",
      "Epoch 687/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1764 - acc: 0.6046 - val_loss: 1.1687 - val_acc: 0.6108\n",
      "Epoch 688/1000\n",
      "68095/68095 [==============================] - 1s 10us/step - loss: 1.1763 - acc: 0.6044 - val_loss: 1.1686 - val_acc: 0.6116\n",
      "Epoch 689/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1762 - acc: 0.6042 - val_loss: 1.1684 - val_acc: 0.6113\n",
      "Epoch 690/1000\n",
      "68095/68095 [==============================] - 2s 23us/step - loss: 1.1761 - acc: 0.6043 - val_loss: 1.1683 - val_acc: 0.6120\n",
      "Epoch 691/1000\n",
      "68095/68095 [==============================] - 2s 27us/step - loss: 1.1760 - acc: 0.6045 - val_loss: 1.1683 - val_acc: 0.6125\n",
      "Epoch 692/1000\n",
      "68095/68095 [==============================] - 2s 23us/step - loss: 1.1759 - acc: 0.6043 - val_loss: 1.1682 - val_acc: 0.6124\n",
      "Epoch 693/1000\n",
      "68095/68095 [==============================] - 1s 19us/step - loss: 1.1758 - acc: 0.6042 - val_loss: 1.1681 - val_acc: 0.6120\n",
      "Epoch 694/1000\n",
      "68095/68095 [==============================] - 2s 28us/step - loss: 1.1757 - acc: 0.6046 - val_loss: 1.1680 - val_acc: 0.6124\n",
      "Epoch 695/1000\n",
      "68095/68095 [==============================] - 2s 27us/step - loss: 1.1756 - acc: 0.6042 - val_loss: 1.1679 - val_acc: 0.6124\n",
      "Epoch 696/1000\n",
      "68095/68095 [==============================] - 1s 19us/step - loss: 1.1755 - acc: 0.6044 - val_loss: 1.1678 - val_acc: 0.6121\n",
      "Epoch 697/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.1754 - acc: 0.6044 - val_loss: 1.1677 - val_acc: 0.6122\n",
      "Epoch 698/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.1753 - acc: 0.6044 - val_loss: 1.1676 - val_acc: 0.6114\n",
      "Epoch 699/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1752 - acc: 0.6043 - val_loss: 1.1675 - val_acc: 0.6121\n",
      "Epoch 700/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1751 - acc: 0.6042 - val_loss: 1.1674 - val_acc: 0.6114\n",
      "Epoch 701/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.1751 - acc: 0.6045 - val_loss: 1.1673 - val_acc: 0.6120\n",
      "Epoch 702/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1749 - acc: 0.6044 - val_loss: 1.1673 - val_acc: 0.6116\n",
      "Epoch 703/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1749 - acc: 0.6044 - val_loss: 1.1672 - val_acc: 0.6114\n",
      "Epoch 704/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1748 - acc: 0.6045 - val_loss: 1.1670 - val_acc: 0.6126\n",
      "Epoch 705/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1746 - acc: 0.6044 - val_loss: 1.1670 - val_acc: 0.6118\n",
      "Epoch 706/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.1746 - acc: 0.6044 - val_loss: 1.1669 - val_acc: 0.6119\n",
      "Epoch 707/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.1745 - acc: 0.6048 - val_loss: 1.1668 - val_acc: 0.6123\n",
      "Epoch 708/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.1744 - acc: 0.6047 - val_loss: 1.1666 - val_acc: 0.6124\n",
      "Epoch 709/1000\n",
      "68095/68095 [==============================] - 1s 19us/step - loss: 1.1743 - acc: 0.6046 - val_loss: 1.1666 - val_acc: 0.6120\n",
      "Epoch 710/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.1742 - acc: 0.6044 - val_loss: 1.1665 - val_acc: 0.6121\n",
      "Epoch 711/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.1741 - acc: 0.6048 - val_loss: 1.1664 - val_acc: 0.6121\n",
      "Epoch 712/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1740 - acc: 0.6047 - val_loss: 1.1664 - val_acc: 0.6119\n",
      "Epoch 713/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.1739 - acc: 0.6047 - val_loss: 1.1662 - val_acc: 0.6119\n",
      "Epoch 714/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.1738 - acc: 0.6050 - val_loss: 1.1661 - val_acc: 0.6124\n",
      "Epoch 715/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1738 - acc: 0.6045 - val_loss: 1.1660 - val_acc: 0.6122\n",
      "Epoch 716/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.1737 - acc: 0.6048 - val_loss: 1.1660 - val_acc: 0.6118\n",
      "Epoch 717/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1736 - acc: 0.6049 - val_loss: 1.1659 - val_acc: 0.6131\n",
      "Epoch 718/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1735 - acc: 0.6049 - val_loss: 1.1658 - val_acc: 0.6120\n",
      "Epoch 719/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.1734 - acc: 0.6050 - val_loss: 1.1657 - val_acc: 0.6120\n",
      "Epoch 720/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.1733 - acc: 0.6051 - val_loss: 1.1656 - val_acc: 0.6121\n",
      "Epoch 721/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.1732 - acc: 0.6048 - val_loss: 1.1655 - val_acc: 0.6125\n",
      "Epoch 722/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.1731 - acc: 0.6047 - val_loss: 1.1654 - val_acc: 0.6117\n",
      "Epoch 723/1000\n",
      "68095/68095 [==============================] - 2s 24us/step - loss: 1.1730 - acc: 0.6048 - val_loss: 1.1653 - val_acc: 0.6123\n",
      "Epoch 724/1000\n",
      "68095/68095 [==============================] - 2s 22us/step - loss: 1.1729 - acc: 0.6044 - val_loss: 1.1652 - val_acc: 0.6126\n",
      "Epoch 725/1000\n",
      "68095/68095 [==============================] - 1s 21us/step - loss: 1.1728 - acc: 0.6050 - val_loss: 1.1652 - val_acc: 0.6120\n",
      "Epoch 726/1000\n",
      "68095/68095 [==============================] - 1s 21us/step - loss: 1.1728 - acc: 0.6047 - val_loss: 1.1650 - val_acc: 0.6123\n",
      "Epoch 727/1000\n",
      "68095/68095 [==============================] - 2s 22us/step - loss: 1.1727 - acc: 0.6049 - val_loss: 1.1650 - val_acc: 0.6121\n",
      "Epoch 728/1000\n",
      "68095/68095 [==============================] - 1s 21us/step - loss: 1.1726 - acc: 0.6050 - val_loss: 1.1649 - val_acc: 0.6121\n",
      "Epoch 729/1000\n",
      "68095/68095 [==============================] - 2s 34us/step - loss: 1.1725 - acc: 0.6049 - val_loss: 1.1647 - val_acc: 0.6124\n",
      "Epoch 730/1000\n",
      "68095/68095 [==============================] - 2s 26us/step - loss: 1.1724 - acc: 0.6049 - val_loss: 1.1647 - val_acc: 0.6125\n",
      "Epoch 731/1000\n",
      "68095/68095 [==============================] - 2s 26us/step - loss: 1.1723 - acc: 0.6050 - val_loss: 1.1646 - val_acc: 0.6120\n",
      "Epoch 732/1000\n",
      "68095/68095 [==============================] - 2s 31us/step - loss: 1.1722 - acc: 0.6053 - val_loss: 1.1645 - val_acc: 0.6123\n",
      "Epoch 733/1000\n",
      "68095/68095 [==============================] - 2s 34us/step - loss: 1.1721 - acc: 0.6049 - val_loss: 1.1645 - val_acc: 0.6130\n",
      "Epoch 734/1000\n",
      "68095/68095 [==============================] - 2s 36us/step - loss: 1.1721 - acc: 0.6051 - val_loss: 1.1643 - val_acc: 0.6120\n",
      "Epoch 735/1000\n",
      "68095/68095 [==============================] - 2s 24us/step - loss: 1.1720 - acc: 0.6052 - val_loss: 1.1643 - val_acc: 0.6125\n",
      "Epoch 736/1000\n",
      "68095/68095 [==============================] - 2s 31us/step - loss: 1.1719 - acc: 0.6049 - val_loss: 1.1642 - val_acc: 0.6122\n",
      "Epoch 737/1000\n",
      "68095/68095 [==============================] - 3s 38us/step - loss: 1.1718 - acc: 0.6049 - val_loss: 1.1641 - val_acc: 0.6126\n",
      "Epoch 738/1000\n",
      "68095/68095 [==============================] - 3s 47us/step - loss: 1.1717 - acc: 0.6049 - val_loss: 1.1640 - val_acc: 0.6124\n",
      "Epoch 739/1000\n",
      "68095/68095 [==============================] - 3s 38us/step - loss: 1.1716 - acc: 0.6050 - val_loss: 1.1640 - val_acc: 0.6125\n",
      "Epoch 740/1000\n",
      "68095/68095 [==============================] - 2s 34us/step - loss: 1.1715 - acc: 0.6050 - val_loss: 1.1639 - val_acc: 0.6127\n",
      "Epoch 741/1000\n",
      "68095/68095 [==============================] - 2s 22us/step - loss: 1.1715 - acc: 0.6051 - val_loss: 1.1637 - val_acc: 0.6121\n",
      "Epoch 742/1000\n",
      "68095/68095 [==============================] - 2s 27us/step - loss: 1.1714 - acc: 0.6049 - val_loss: 1.1636 - val_acc: 0.6123\n",
      "Epoch 743/1000\n",
      "68095/68095 [==============================] - 2s 29us/step - loss: 1.1713 - acc: 0.6050 - val_loss: 1.1636 - val_acc: 0.6133\n",
      "Epoch 744/1000\n",
      "68095/68095 [==============================] - 2s 27us/step - loss: 1.1712 - acc: 0.6059 - val_loss: 1.1635 - val_acc: 0.6127\n",
      "Epoch 745/1000\n",
      "68095/68095 [==============================] - 2s 25us/step - loss: 1.1711 - acc: 0.6053 - val_loss: 1.1634 - val_acc: 0.6128\n",
      "Epoch 746/1000\n",
      "68095/68095 [==============================] - 2s 32us/step - loss: 1.1710 - acc: 0.6053 - val_loss: 1.1634 - val_acc: 0.6134\n",
      "Epoch 747/1000\n",
      "68095/68095 [==============================] - 2s 29us/step - loss: 1.1709 - acc: 0.6056 - val_loss: 1.1633 - val_acc: 0.6120\n",
      "Epoch 748/1000\n",
      "68095/68095 [==============================] - 1s 21us/step - loss: 1.1708 - acc: 0.6054 - val_loss: 1.1632 - val_acc: 0.6118\n",
      "Epoch 749/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.1708 - acc: 0.6050 - val_loss: 1.1631 - val_acc: 0.6118\n",
      "Epoch 750/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1707 - acc: 0.6051 - val_loss: 1.1630 - val_acc: 0.6122\n",
      "Epoch 751/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.1706 - acc: 0.6055 - val_loss: 1.1629 - val_acc: 0.6123\n",
      "Epoch 752/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.1705 - acc: 0.6051 - val_loss: 1.1628 - val_acc: 0.6124\n",
      "Epoch 753/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.1704 - acc: 0.6054 - val_loss: 1.1628 - val_acc: 0.6124\n",
      "Epoch 754/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.1704 - acc: 0.6053 - val_loss: 1.1626 - val_acc: 0.6118\n",
      "Epoch 755/1000\n",
      "68095/68095 [==============================] - 1s 21us/step - loss: 1.1702 - acc: 0.6054 - val_loss: 1.1626 - val_acc: 0.6132\n",
      "Epoch 756/1000\n",
      "68095/68095 [==============================] - 2s 28us/step - loss: 1.1702 - acc: 0.6051 - val_loss: 1.1625 - val_acc: 0.6127\n",
      "Epoch 757/1000\n",
      "68095/68095 [==============================] - 1s 18us/step - loss: 1.1701 - acc: 0.6053 - val_loss: 1.1624 - val_acc: 0.6129\n",
      "Epoch 758/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.1700 - acc: 0.6060 - val_loss: 1.1624 - val_acc: 0.6125\n",
      "Epoch 759/1000\n",
      "68095/68095 [==============================] - 1s 19us/step - loss: 1.1699 - acc: 0.6055 - val_loss: 1.1623 - val_acc: 0.6127\n",
      "Epoch 760/1000\n",
      "68095/68095 [==============================] - 1s 21us/step - loss: 1.1699 - acc: 0.6057 - val_loss: 1.1621 - val_acc: 0.6125\n",
      "Epoch 761/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.1698 - acc: 0.6056 - val_loss: 1.1620 - val_acc: 0.6127\n",
      "Epoch 762/1000\n",
      "68095/68095 [==============================] - 1s 18us/step - loss: 1.1697 - acc: 0.6056 - val_loss: 1.1620 - val_acc: 0.6122\n",
      "Epoch 763/1000\n",
      "68095/68095 [==============================] - 1s 20us/step - loss: 1.1696 - acc: 0.6057 - val_loss: 1.1619 - val_acc: 0.6127\n",
      "Epoch 764/1000\n",
      "68095/68095 [==============================] - 1s 19us/step - loss: 1.1695 - acc: 0.6056 - val_loss: 1.1618 - val_acc: 0.6129\n",
      "Epoch 765/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.1694 - acc: 0.6055 - val_loss: 1.1617 - val_acc: 0.6130\n",
      "Epoch 766/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1694 - acc: 0.6058 - val_loss: 1.1617 - val_acc: 0.6131\n",
      "Epoch 767/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1693 - acc: 0.6053 - val_loss: 1.1615 - val_acc: 0.6132\n",
      "Epoch 768/1000\n",
      "68095/68095 [==============================] - 2s 23us/step - loss: 1.1692 - acc: 0.6055 - val_loss: 1.1615 - val_acc: 0.6120\n",
      "Epoch 769/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.1691 - acc: 0.6057 - val_loss: 1.1614 - val_acc: 0.6133\n",
      "Epoch 770/1000\n",
      "68095/68095 [==============================] - 2s 27us/step - loss: 1.1690 - acc: 0.6057 - val_loss: 1.1613 - val_acc: 0.6133\n",
      "Epoch 771/1000\n",
      "68095/68095 [==============================] - 2s 30us/step - loss: 1.1689 - acc: 0.6059 - val_loss: 1.1613 - val_acc: 0.6131\n",
      "Epoch 772/1000\n",
      "68095/68095 [==============================] - 1s 19us/step - loss: 1.1689 - acc: 0.6059 - val_loss: 1.1612 - val_acc: 0.6124\n",
      "Epoch 773/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1688 - acc: 0.6058 - val_loss: 1.1611 - val_acc: 0.6133\n",
      "Epoch 774/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1687 - acc: 0.6054 - val_loss: 1.1610 - val_acc: 0.6135\n",
      "Epoch 775/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1686 - acc: 0.6060 - val_loss: 1.1611 - val_acc: 0.6126\n",
      "Epoch 776/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1686 - acc: 0.6057 - val_loss: 1.1609 - val_acc: 0.6126\n",
      "Epoch 777/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.1685 - acc: 0.6056 - val_loss: 1.1608 - val_acc: 0.6130\n",
      "Epoch 778/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1684 - acc: 0.6057 - val_loss: 1.1608 - val_acc: 0.6127\n",
      "Epoch 779/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1683 - acc: 0.6055 - val_loss: 1.1606 - val_acc: 0.6133\n",
      "Epoch 780/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1682 - acc: 0.6061 - val_loss: 1.1606 - val_acc: 0.6132\n",
      "Epoch 781/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1682 - acc: 0.6058 - val_loss: 1.1605 - val_acc: 0.6133\n",
      "Epoch 782/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1681 - acc: 0.6059 - val_loss: 1.1604 - val_acc: 0.6130\n",
      "Epoch 783/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1680 - acc: 0.6057 - val_loss: 1.1604 - val_acc: 0.6128\n",
      "Epoch 784/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1679 - acc: 0.6058 - val_loss: 1.1603 - val_acc: 0.6127\n",
      "Epoch 785/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1678 - acc: 0.6059 - val_loss: 1.1602 - val_acc: 0.6120\n",
      "Epoch 786/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1678 - acc: 0.6056 - val_loss: 1.1601 - val_acc: 0.6133\n",
      "Epoch 787/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.1677 - acc: 0.6062 - val_loss: 1.1600 - val_acc: 0.6128\n",
      "Epoch 788/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1677 - acc: 0.6059 - val_loss: 1.1599 - val_acc: 0.6135\n",
      "Epoch 789/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1676 - acc: 0.6060 - val_loss: 1.1599 - val_acc: 0.6133\n",
      "Epoch 790/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1675 - acc: 0.6062 - val_loss: 1.1598 - val_acc: 0.6133\n",
      "Epoch 791/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1674 - acc: 0.6061 - val_loss: 1.1598 - val_acc: 0.6141\n",
      "Epoch 792/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1673 - acc: 0.6057 - val_loss: 1.1596 - val_acc: 0.6141\n",
      "Epoch 793/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1673 - acc: 0.6061 - val_loss: 1.1596 - val_acc: 0.6127\n",
      "Epoch 794/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1671 - acc: 0.6060 - val_loss: 1.1595 - val_acc: 0.6120\n",
      "Epoch 795/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1671 - acc: 0.6064 - val_loss: 1.1594 - val_acc: 0.6128\n",
      "Epoch 796/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1670 - acc: 0.6061 - val_loss: 1.1593 - val_acc: 0.6132\n",
      "Epoch 797/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1669 - acc: 0.6059 - val_loss: 1.1593 - val_acc: 0.6124\n",
      "Epoch 798/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1669 - acc: 0.6061 - val_loss: 1.1592 - val_acc: 0.6127\n",
      "Epoch 799/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1668 - acc: 0.6059 - val_loss: 1.1591 - val_acc: 0.6134\n",
      "Epoch 800/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1667 - acc: 0.6056 - val_loss: 1.1591 - val_acc: 0.6128\n",
      "Epoch 801/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1666 - acc: 0.6067 - val_loss: 1.1589 - val_acc: 0.6137\n",
      "Epoch 802/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.1666 - acc: 0.6061 - val_loss: 1.1589 - val_acc: 0.6133\n",
      "Epoch 803/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1665 - acc: 0.6064 - val_loss: 1.1588 - val_acc: 0.6134\n",
      "Epoch 804/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1664 - acc: 0.6062 - val_loss: 1.1587 - val_acc: 0.6135\n",
      "Epoch 805/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1664 - acc: 0.6062 - val_loss: 1.1586 - val_acc: 0.6138\n",
      "Epoch 806/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.1663 - acc: 0.6060 - val_loss: 1.1586 - val_acc: 0.6139\n",
      "Epoch 807/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.1662 - acc: 0.6059 - val_loss: 1.1586 - val_acc: 0.6138\n",
      "Epoch 808/1000\n",
      "68095/68095 [==============================] - 1s 19us/step - loss: 1.1661 - acc: 0.6062 - val_loss: 1.1585 - val_acc: 0.6128\n",
      "Epoch 809/1000\n",
      "68095/68095 [==============================] - 1s 19us/step - loss: 1.1661 - acc: 0.6062 - val_loss: 1.1583 - val_acc: 0.6134\n",
      "Epoch 810/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1660 - acc: 0.6060 - val_loss: 1.1583 - val_acc: 0.6124\n",
      "Epoch 811/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1659 - acc: 0.6060 - val_loss: 1.1582 - val_acc: 0.6134\n",
      "Epoch 812/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1658 - acc: 0.6064 - val_loss: 1.1581 - val_acc: 0.6134\n",
      "Epoch 813/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1658 - acc: 0.6061 - val_loss: 1.1581 - val_acc: 0.6134\n",
      "Epoch 814/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.1657 - acc: 0.6067 - val_loss: 1.1581 - val_acc: 0.6141\n",
      "Epoch 815/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.1656 - acc: 0.6064 - val_loss: 1.1579 - val_acc: 0.6135\n",
      "Epoch 816/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1656 - acc: 0.6064 - val_loss: 1.1579 - val_acc: 0.6133\n",
      "Epoch 817/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1655 - acc: 0.6060 - val_loss: 1.1578 - val_acc: 0.6140\n",
      "Epoch 818/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1654 - acc: 0.6065 - val_loss: 1.1578 - val_acc: 0.6127\n",
      "Epoch 819/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1653 - acc: 0.6061 - val_loss: 1.1577 - val_acc: 0.6119\n",
      "Epoch 820/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1653 - acc: 0.6064 - val_loss: 1.1576 - val_acc: 0.6135\n",
      "Epoch 821/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1652 - acc: 0.6067 - val_loss: 1.1575 - val_acc: 0.6132\n",
      "Epoch 822/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1651 - acc: 0.6064 - val_loss: 1.1574 - val_acc: 0.6138\n",
      "Epoch 823/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1650 - acc: 0.6061 - val_loss: 1.1573 - val_acc: 0.6130\n",
      "Epoch 824/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1650 - acc: 0.6064 - val_loss: 1.1573 - val_acc: 0.6138\n",
      "Epoch 825/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1649 - acc: 0.6065 - val_loss: 1.1573 - val_acc: 0.6134\n",
      "Epoch 826/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1649 - acc: 0.6065 - val_loss: 1.1572 - val_acc: 0.6135\n",
      "Epoch 827/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1648 - acc: 0.6067 - val_loss: 1.1571 - val_acc: 0.6141\n",
      "Epoch 828/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1647 - acc: 0.6062 - val_loss: 1.1570 - val_acc: 0.6134\n",
      "Epoch 829/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1646 - acc: 0.6067 - val_loss: 1.1569 - val_acc: 0.6137\n",
      "Epoch 830/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1646 - acc: 0.6062 - val_loss: 1.1569 - val_acc: 0.6133\n",
      "Epoch 831/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1645 - acc: 0.6063 - val_loss: 1.1569 - val_acc: 0.6129\n",
      "Epoch 832/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1644 - acc: 0.6066 - val_loss: 1.1568 - val_acc: 0.6130\n",
      "Epoch 833/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1644 - acc: 0.6061 - val_loss: 1.1567 - val_acc: 0.6128\n",
      "Epoch 834/1000\n",
      "68095/68095 [==============================] - 1s 20us/step - loss: 1.1643 - acc: 0.6064 - val_loss: 1.1566 - val_acc: 0.6130\n",
      "Epoch 835/1000\n",
      "68095/68095 [==============================] - 1s 21us/step - loss: 1.1642 - acc: 0.6067 - val_loss: 1.1565 - val_acc: 0.6132\n",
      "Epoch 836/1000\n",
      "68095/68095 [==============================] - 2s 27us/step - loss: 1.1641 - acc: 0.6067 - val_loss: 1.1565 - val_acc: 0.6132\n",
      "Epoch 837/1000\n",
      "68095/68095 [==============================] - 2s 26us/step - loss: 1.1641 - acc: 0.6061 - val_loss: 1.1564 - val_acc: 0.6133\n",
      "Epoch 838/1000\n",
      "68095/68095 [==============================] - 2s 23us/step - loss: 1.1640 - acc: 0.6067 - val_loss: 1.1563 - val_acc: 0.6140\n",
      "Epoch 839/1000\n",
      "68095/68095 [==============================] - 2s 24us/step - loss: 1.1639 - acc: 0.6065 - val_loss: 1.1563 - val_acc: 0.6135\n",
      "Epoch 840/1000\n",
      "68095/68095 [==============================] - 1s 22us/step - loss: 1.1639 - acc: 0.6065 - val_loss: 1.1562 - val_acc: 0.6133\n",
      "Epoch 841/1000\n",
      "68095/68095 [==============================] - 1s 19us/step - loss: 1.1638 - acc: 0.6067 - val_loss: 1.1562 - val_acc: 0.6134\n",
      "Epoch 842/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.1637 - acc: 0.6068 - val_loss: 1.1561 - val_acc: 0.6128\n",
      "Epoch 843/1000\n",
      "68095/68095 [==============================] - 2s 29us/step - loss: 1.1637 - acc: 0.6067 - val_loss: 1.1559 - val_acc: 0.6137\n",
      "Epoch 844/1000\n",
      "68095/68095 [==============================] - 2s 24us/step - loss: 1.1636 - acc: 0.6071 - val_loss: 1.1559 - val_acc: 0.6135\n",
      "Epoch 845/1000\n",
      "68095/68095 [==============================] - 2s 23us/step - loss: 1.1635 - acc: 0.6067 - val_loss: 1.1559 - val_acc: 0.6142\n",
      "Epoch 846/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1635 - acc: 0.6065 - val_loss: 1.1558 - val_acc: 0.6135\n",
      "Epoch 847/1000\n",
      "68095/68095 [==============================] - 1s 19us/step - loss: 1.1634 - acc: 0.6067 - val_loss: 1.1557 - val_acc: 0.6137\n",
      "Epoch 848/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1633 - acc: 0.6068 - val_loss: 1.1556 - val_acc: 0.6127\n",
      "Epoch 849/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1633 - acc: 0.6068 - val_loss: 1.1556 - val_acc: 0.6125\n",
      "Epoch 850/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1632 - acc: 0.6069 - val_loss: 1.1555 - val_acc: 0.6142\n",
      "Epoch 851/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1631 - acc: 0.6067 - val_loss: 1.1555 - val_acc: 0.6137\n",
      "Epoch 852/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1631 - acc: 0.6070 - val_loss: 1.1554 - val_acc: 0.6138\n",
      "Epoch 853/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1630 - acc: 0.6066 - val_loss: 1.1553 - val_acc: 0.6135\n",
      "Epoch 854/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1629 - acc: 0.6066 - val_loss: 1.1552 - val_acc: 0.6140\n",
      "Epoch 855/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1629 - acc: 0.6067 - val_loss: 1.1552 - val_acc: 0.6133\n",
      "Epoch 856/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.1628 - acc: 0.6070 - val_loss: 1.1551 - val_acc: 0.6136\n",
      "Epoch 857/1000\n",
      "68095/68095 [==============================] - 2s 28us/step - loss: 1.1627 - acc: 0.6066 - val_loss: 1.1551 - val_acc: 0.6138\n",
      "Epoch 858/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.1627 - acc: 0.6070 - val_loss: 1.1550 - val_acc: 0.6144\n",
      "Epoch 859/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.1626 - acc: 0.6069 - val_loss: 1.1549 - val_acc: 0.6145\n",
      "Epoch 860/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.1625 - acc: 0.6069 - val_loss: 1.1548 - val_acc: 0.6137\n",
      "Epoch 861/1000\n",
      "68095/68095 [==============================] - 1s 18us/step - loss: 1.1624 - acc: 0.6066 - val_loss: 1.1548 - val_acc: 0.6133\n",
      "Epoch 862/1000\n",
      "68095/68095 [==============================] - 1s 20us/step - loss: 1.1624 - acc: 0.6065 - val_loss: 1.1547 - val_acc: 0.6134\n",
      "Epoch 863/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1623 - acc: 0.6067 - val_loss: 1.1547 - val_acc: 0.6133\n",
      "Epoch 864/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.1623 - acc: 0.6072 - val_loss: 1.1546 - val_acc: 0.6136\n",
      "Epoch 865/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.1622 - acc: 0.6072 - val_loss: 1.1545 - val_acc: 0.6135\n",
      "Epoch 866/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.1621 - acc: 0.6069 - val_loss: 1.1545 - val_acc: 0.6138\n",
      "Epoch 867/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1621 - acc: 0.6069 - val_loss: 1.1544 - val_acc: 0.6143\n",
      "Epoch 868/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1620 - acc: 0.6070 - val_loss: 1.1543 - val_acc: 0.6137\n",
      "Epoch 869/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.1620 - acc: 0.6068 - val_loss: 1.1543 - val_acc: 0.6138\n",
      "Epoch 870/1000\n",
      "68095/68095 [==============================] - 2s 22us/step - loss: 1.1619 - acc: 0.6070 - val_loss: 1.1543 - val_acc: 0.6135\n",
      "Epoch 871/1000\n",
      "68095/68095 [==============================] - 2s 23us/step - loss: 1.1618 - acc: 0.6070 - val_loss: 1.1541 - val_acc: 0.6137\n",
      "Epoch 872/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1618 - acc: 0.6070 - val_loss: 1.1541 - val_acc: 0.6137\n",
      "Epoch 873/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.1617 - acc: 0.6072 - val_loss: 1.1540 - val_acc: 0.6135\n",
      "Epoch 874/1000\n",
      "68095/68095 [==============================] - 1s 18us/step - loss: 1.1616 - acc: 0.6071 - val_loss: 1.1539 - val_acc: 0.6144\n",
      "Epoch 875/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.1616 - acc: 0.6072 - val_loss: 1.1539 - val_acc: 0.6144\n",
      "Epoch 876/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.1615 - acc: 0.6069 - val_loss: 1.1539 - val_acc: 0.6136\n",
      "Epoch 877/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.1614 - acc: 0.6069 - val_loss: 1.1538 - val_acc: 0.6137\n",
      "Epoch 878/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.1614 - acc: 0.6064 - val_loss: 1.1537 - val_acc: 0.6140\n",
      "Epoch 879/1000\n",
      "68095/68095 [==============================] - 1s 22us/step - loss: 1.1613 - acc: 0.6070 - val_loss: 1.1537 - val_acc: 0.6133\n",
      "Epoch 880/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1613 - acc: 0.6071 - val_loss: 1.1536 - val_acc: 0.6131\n",
      "Epoch 881/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.1612 - acc: 0.6071 - val_loss: 1.1536 - val_acc: 0.6150\n",
      "Epoch 882/1000\n",
      "68095/68095 [==============================] - 2s 23us/step - loss: 1.1611 - acc: 0.6069 - val_loss: 1.1536 - val_acc: 0.6140\n",
      "Epoch 883/1000\n",
      "68095/68095 [==============================] - 1s 18us/step - loss: 1.1610 - acc: 0.6069 - val_loss: 1.1534 - val_acc: 0.6135\n",
      "Epoch 884/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.1610 - acc: 0.6066 - val_loss: 1.1534 - val_acc: 0.6138\n",
      "Epoch 885/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1609 - acc: 0.6073 - val_loss: 1.1533 - val_acc: 0.6122\n",
      "Epoch 886/1000\n",
      "68095/68095 [==============================] - ETA: 0s - loss: 1.1594 - acc: 0.607 - 1s 13us/step - loss: 1.1609 - acc: 0.6071 - val_loss: 1.1532 - val_acc: 0.6137\n",
      "Epoch 887/1000\n",
      "68095/68095 [==============================] - 1s 20us/step - loss: 1.1608 - acc: 0.6071 - val_loss: 1.1532 - val_acc: 0.6135\n",
      "Epoch 888/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.1608 - acc: 0.6071 - val_loss: 1.1531 - val_acc: 0.6136\n",
      "Epoch 889/1000\n",
      "68095/68095 [==============================] - 1s 18us/step - loss: 1.1607 - acc: 0.6074 - val_loss: 1.1530 - val_acc: 0.6139\n",
      "Epoch 890/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.1606 - acc: 0.6074 - val_loss: 1.1529 - val_acc: 0.6134\n",
      "Epoch 891/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.1606 - acc: 0.6070 - val_loss: 1.1530 - val_acc: 0.6139\n",
      "Epoch 892/1000\n",
      "68095/68095 [==============================] - 1s 18us/step - loss: 1.1605 - acc: 0.6073 - val_loss: 1.1529 - val_acc: 0.6124\n",
      "Epoch 893/1000\n",
      "68095/68095 [==============================] - 1s 22us/step - loss: 1.1605 - acc: 0.6072 - val_loss: 1.1527 - val_acc: 0.6140\n",
      "Epoch 894/1000\n",
      "68095/68095 [==============================] - 1s 20us/step - loss: 1.1604 - acc: 0.6071 - val_loss: 1.1527 - val_acc: 0.6135\n",
      "Epoch 895/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.1603 - acc: 0.6073 - val_loss: 1.1527 - val_acc: 0.6146\n",
      "Epoch 896/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1603 - acc: 0.6071 - val_loss: 1.1526 - val_acc: 0.6139\n",
      "Epoch 897/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1602 - acc: 0.6072 - val_loss: 1.1526 - val_acc: 0.6136\n",
      "Epoch 898/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1601 - acc: 0.6073 - val_loss: 1.1524 - val_acc: 0.6138\n",
      "Epoch 899/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1601 - acc: 0.6072 - val_loss: 1.1524 - val_acc: 0.6144\n",
      "Epoch 900/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.1600 - acc: 0.6073 - val_loss: 1.1524 - val_acc: 0.6131\n",
      "Epoch 901/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.1600 - acc: 0.6073 - val_loss: 1.1523 - val_acc: 0.6143\n",
      "Epoch 902/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1599 - acc: 0.6074 - val_loss: 1.1522 - val_acc: 0.6140\n",
      "Epoch 903/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1599 - acc: 0.6070 - val_loss: 1.1522 - val_acc: 0.6138\n",
      "Epoch 904/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1598 - acc: 0.6073 - val_loss: 1.1522 - val_acc: 0.6150\n",
      "Epoch 905/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.1597 - acc: 0.6073 - val_loss: 1.1521 - val_acc: 0.6137\n",
      "Epoch 906/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.1597 - acc: 0.6072 - val_loss: 1.1520 - val_acc: 0.6137\n",
      "Epoch 907/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.1596 - acc: 0.6074 - val_loss: 1.1519 - val_acc: 0.6143\n",
      "Epoch 908/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.1595 - acc: 0.6076 - val_loss: 1.1520 - val_acc: 0.6146\n",
      "Epoch 909/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1595 - acc: 0.6078 - val_loss: 1.1518 - val_acc: 0.6134\n",
      "Epoch 910/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.1594 - acc: 0.6076 - val_loss: 1.1518 - val_acc: 0.6136\n",
      "Epoch 911/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.1594 - acc: 0.6073 - val_loss: 1.1517 - val_acc: 0.6134\n",
      "Epoch 912/1000\n",
      "68095/68095 [==============================] - 1s 18us/step - loss: 1.1593 - acc: 0.6073 - val_loss: 1.1517 - val_acc: 0.6143\n",
      "Epoch 913/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.1592 - acc: 0.6076 - val_loss: 1.1516 - val_acc: 0.6136\n",
      "Epoch 914/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1592 - acc: 0.6076 - val_loss: 1.1515 - val_acc: 0.6138\n",
      "Epoch 915/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1591 - acc: 0.6072 - val_loss: 1.1515 - val_acc: 0.6140\n",
      "Epoch 916/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1591 - acc: 0.6072 - val_loss: 1.1514 - val_acc: 0.6137\n",
      "Epoch 917/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1590 - acc: 0.6079 - val_loss: 1.1513 - val_acc: 0.6135\n",
      "Epoch 918/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.1590 - acc: 0.6073 - val_loss: 1.1513 - val_acc: 0.6136\n",
      "Epoch 919/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.1589 - acc: 0.6072 - val_loss: 1.1513 - val_acc: 0.6140\n",
      "Epoch 920/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.1589 - acc: 0.6076 - val_loss: 1.1512 - val_acc: 0.6134\n",
      "Epoch 921/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.1588 - acc: 0.6077 - val_loss: 1.1511 - val_acc: 0.6141\n",
      "Epoch 922/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.1587 - acc: 0.6073 - val_loss: 1.1511 - val_acc: 0.6141\n",
      "Epoch 923/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.1587 - acc: 0.6077 - val_loss: 1.1510 - val_acc: 0.6141\n",
      "Epoch 924/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.1586 - acc: 0.6079 - val_loss: 1.1510 - val_acc: 0.6141\n",
      "Epoch 925/1000\n",
      "68095/68095 [==============================] - 1s 20us/step - loss: 1.1586 - acc: 0.6078 - val_loss: 1.1509 - val_acc: 0.6134\n",
      "Epoch 926/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.1585 - acc: 0.6081 - val_loss: 1.1508 - val_acc: 0.6141\n",
      "Epoch 927/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.1585 - acc: 0.6075 - val_loss: 1.1508 - val_acc: 0.6141\n",
      "Epoch 928/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.1584 - acc: 0.6075 - val_loss: 1.1507 - val_acc: 0.6137\n",
      "Epoch 929/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.1583 - acc: 0.6075 - val_loss: 1.1506 - val_acc: 0.6142\n",
      "Epoch 930/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1583 - acc: 0.6078 - val_loss: 1.1506 - val_acc: 0.6141\n",
      "Epoch 931/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1582 - acc: 0.6079 - val_loss: 1.1506 - val_acc: 0.6145\n",
      "Epoch 932/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1582 - acc: 0.6077 - val_loss: 1.1505 - val_acc: 0.6145\n",
      "Epoch 933/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1581 - acc: 0.6079 - val_loss: 1.1504 - val_acc: 0.6135\n",
      "Epoch 934/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1580 - acc: 0.6079 - val_loss: 1.1504 - val_acc: 0.6139\n",
      "Epoch 935/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1580 - acc: 0.6080 - val_loss: 1.1504 - val_acc: 0.6129\n",
      "Epoch 936/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1579 - acc: 0.6075 - val_loss: 1.1503 - val_acc: 0.6131\n",
      "Epoch 937/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1579 - acc: 0.6082 - val_loss: 1.1502 - val_acc: 0.6137\n",
      "Epoch 938/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1578 - acc: 0.6078 - val_loss: 1.1502 - val_acc: 0.6144\n",
      "Epoch 939/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1578 - acc: 0.6079 - val_loss: 1.1501 - val_acc: 0.6140\n",
      "Epoch 940/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1577 - acc: 0.6076 - val_loss: 1.1500 - val_acc: 0.6145\n",
      "Epoch 941/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1576 - acc: 0.6079 - val_loss: 1.1500 - val_acc: 0.6143\n",
      "Epoch 942/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1576 - acc: 0.6078 - val_loss: 1.1500 - val_acc: 0.6134\n",
      "Epoch 943/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1575 - acc: 0.6082 - val_loss: 1.1499 - val_acc: 0.6136\n",
      "Epoch 944/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.1575 - acc: 0.6079 - val_loss: 1.1498 - val_acc: 0.6140\n",
      "Epoch 945/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1574 - acc: 0.6080 - val_loss: 1.1498 - val_acc: 0.6141\n",
      "Epoch 946/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1574 - acc: 0.6078 - val_loss: 1.1497 - val_acc: 0.6146\n",
      "Epoch 947/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1573 - acc: 0.6079 - val_loss: 1.1497 - val_acc: 0.6143\n",
      "Epoch 948/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1573 - acc: 0.6082 - val_loss: 1.1496 - val_acc: 0.6137\n",
      "Epoch 949/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1572 - acc: 0.6077 - val_loss: 1.1495 - val_acc: 0.6142\n",
      "Epoch 950/1000\n",
      "68095/68095 [==============================] - 1s 10us/step - loss: 1.1572 - acc: 0.6076 - val_loss: 1.1495 - val_acc: 0.6147\n",
      "Epoch 951/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1571 - acc: 0.6080 - val_loss: 1.1494 - val_acc: 0.6136\n",
      "Epoch 952/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1570 - acc: 0.6081 - val_loss: 1.1495 - val_acc: 0.6137\n",
      "Epoch 953/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1570 - acc: 0.6080 - val_loss: 1.1494 - val_acc: 0.6140\n",
      "Epoch 954/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1569 - acc: 0.6077 - val_loss: 1.1493 - val_acc: 0.6135\n",
      "Epoch 955/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1569 - acc: 0.6083 - val_loss: 1.1493 - val_acc: 0.6137\n",
      "Epoch 956/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1568 - acc: 0.6079 - val_loss: 1.1492 - val_acc: 0.6150\n",
      "Epoch 957/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1568 - acc: 0.6085 - val_loss: 1.1492 - val_acc: 0.6130\n",
      "Epoch 958/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1567 - acc: 0.6082 - val_loss: 1.1491 - val_acc: 0.6135\n",
      "Epoch 959/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1566 - acc: 0.6082 - val_loss: 1.1492 - val_acc: 0.6142\n",
      "Epoch 960/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1566 - acc: 0.6078 - val_loss: 1.1490 - val_acc: 0.6140\n",
      "Epoch 961/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1566 - acc: 0.6079 - val_loss: 1.1489 - val_acc: 0.6138\n",
      "Epoch 962/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1565 - acc: 0.6084 - val_loss: 1.1489 - val_acc: 0.6136\n",
      "Epoch 963/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1565 - acc: 0.6080 - val_loss: 1.1488 - val_acc: 0.6139\n",
      "Epoch 964/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1564 - acc: 0.6082 - val_loss: 1.1487 - val_acc: 0.6147\n",
      "Epoch 965/1000\n",
      "68095/68095 [==============================] - 1s 11us/step - loss: 1.1564 - acc: 0.6085 - val_loss: 1.1487 - val_acc: 0.6135\n",
      "Epoch 966/1000\n",
      "68095/68095 [==============================] - 1s 12us/step - loss: 1.1563 - acc: 0.6083 - val_loss: 1.1487 - val_acc: 0.6143\n",
      "Epoch 967/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.1563 - acc: 0.6086 - val_loss: 1.1487 - val_acc: 0.6145\n",
      "Epoch 968/1000\n",
      "68095/68095 [==============================] - 2s 25us/step - loss: 1.1562 - acc: 0.6082 - val_loss: 1.1485 - val_acc: 0.6148\n",
      "Epoch 969/1000\n",
      "68095/68095 [==============================] - 2s 23us/step - loss: 1.1561 - acc: 0.6081 - val_loss: 1.1485 - val_acc: 0.6139\n",
      "Epoch 970/1000\n",
      "68095/68095 [==============================] - 2s 25us/step - loss: 1.1561 - acc: 0.6083 - val_loss: 1.1484 - val_acc: 0.6142\n",
      "Epoch 971/1000\n",
      "68095/68095 [==============================] - 2s 25us/step - loss: 1.1561 - acc: 0.6084 - val_loss: 1.1484 - val_acc: 0.6143\n",
      "Epoch 972/1000\n",
      "68095/68095 [==============================] - 1s 19us/step - loss: 1.1560 - acc: 0.6082 - val_loss: 1.1484 - val_acc: 0.6140\n",
      "Epoch 973/1000\n",
      "68095/68095 [==============================] - 1s 21us/step - loss: 1.1559 - acc: 0.6083 - val_loss: 1.1483 - val_acc: 0.6140\n",
      "Epoch 974/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.1559 - acc: 0.6083 - val_loss: 1.1483 - val_acc: 0.6140\n",
      "Epoch 975/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.1558 - acc: 0.6084 - val_loss: 1.1482 - val_acc: 0.6141\n",
      "Epoch 976/1000\n",
      "68095/68095 [==============================] - 1s 19us/step - loss: 1.1558 - acc: 0.6084 - val_loss: 1.1481 - val_acc: 0.6139\n",
      "Epoch 977/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.1557 - acc: 0.6082 - val_loss: 1.1481 - val_acc: 0.6145\n",
      "Epoch 978/1000\n",
      "68095/68095 [==============================] - 1s 20us/step - loss: 1.1557 - acc: 0.6084 - val_loss: 1.1481 - val_acc: 0.6140\n",
      "Epoch 979/1000\n",
      "68095/68095 [==============================] - 1s 15us/step - loss: 1.1556 - acc: 0.6084 - val_loss: 1.1481 - val_acc: 0.6144\n",
      "Epoch 980/1000\n",
      "68095/68095 [==============================] - 2s 24us/step - loss: 1.1556 - acc: 0.6083 - val_loss: 1.1479 - val_acc: 0.6143\n",
      "Epoch 981/1000\n",
      "68095/68095 [==============================] - 2s 24us/step - loss: 1.1555 - acc: 0.6086 - val_loss: 1.1479 - val_acc: 0.6144\n",
      "Epoch 982/1000\n",
      "68095/68095 [==============================] - 1s 19us/step - loss: 1.1554 - acc: 0.6089 - val_loss: 1.1478 - val_acc: 0.6150\n",
      "Epoch 983/1000\n",
      "68095/68095 [==============================] - 2s 23us/step - loss: 1.1554 - acc: 0.6085 - val_loss: 1.1478 - val_acc: 0.6135\n",
      "Epoch 984/1000\n",
      "68095/68095 [==============================] - 2s 22us/step - loss: 1.1554 - acc: 0.6088 - val_loss: 1.1477 - val_acc: 0.6147\n",
      "Epoch 985/1000\n",
      "68095/68095 [==============================] - 1s 21us/step - loss: 1.1553 - acc: 0.6084 - val_loss: 1.1477 - val_acc: 0.6144\n",
      "Epoch 986/1000\n",
      "68095/68095 [==============================] - 1s 21us/step - loss: 1.1553 - acc: 0.6086 - val_loss: 1.1476 - val_acc: 0.6143\n",
      "Epoch 987/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.1552 - acc: 0.6089 - val_loss: 1.1476 - val_acc: 0.6147\n",
      "Epoch 988/1000\n",
      "68095/68095 [==============================] - 1s 18us/step - loss: 1.1552 - acc: 0.6086 - val_loss: 1.1475 - val_acc: 0.6141\n",
      "Epoch 989/1000\n",
      "68095/68095 [==============================] - 1s 20us/step - loss: 1.1551 - acc: 0.6086 - val_loss: 1.1475 - val_acc: 0.6144\n",
      "Epoch 990/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.1551 - acc: 0.6088 - val_loss: 1.1474 - val_acc: 0.6142\n",
      "Epoch 991/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.1550 - acc: 0.6086 - val_loss: 1.1474 - val_acc: 0.6146\n",
      "Epoch 992/1000\n",
      "68095/68095 [==============================] - 1s 16us/step - loss: 1.1550 - acc: 0.6083 - val_loss: 1.1473 - val_acc: 0.6146\n",
      "Epoch 993/1000\n",
      "68095/68095 [==============================] - 1s 13us/step - loss: 1.1549 - acc: 0.6086 - val_loss: 1.1473 - val_acc: 0.6140\n",
      "Epoch 994/1000\n",
      "68095/68095 [==============================] - 1s 14us/step - loss: 1.1549 - acc: 0.6084 - val_loss: 1.1472 - val_acc: 0.6148\n",
      "Epoch 995/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.1548 - acc: 0.6086 - val_loss: 1.1472 - val_acc: 0.6139\n",
      "Epoch 996/1000\n",
      "68095/68095 [==============================] - 2s 24us/step - loss: 1.1547 - acc: 0.6086 - val_loss: 1.1472 - val_acc: 0.6140\n",
      "Epoch 997/1000\n",
      "68095/68095 [==============================] - 2s 25us/step - loss: 1.1547 - acc: 0.6086 - val_loss: 1.1470 - val_acc: 0.6143\n",
      "Epoch 998/1000\n",
      "68095/68095 [==============================] - 1s 19us/step - loss: 1.1547 - acc: 0.6087 - val_loss: 1.1471 - val_acc: 0.6144\n",
      "Epoch 999/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.1546 - acc: 0.6088 - val_loss: 1.1470 - val_acc: 0.6147\n",
      "Epoch 1000/1000\n",
      "68095/68095 [==============================] - 1s 17us/step - loss: 1.1546 - acc: 0.6089 - val_loss: 1.1469 - val_acc: 0.6145\n"
     ]
    }
   ],
   "source": [
    "# Compile and fit the model using Gradient Descent Optimizer\n",
    "model.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics=['accuracy']) \n",
    "history = model.fit(train_x, train_y+1, batch_size=batch_size, nb_epoch=nb_epoch,verbose=1, validation_data=(test_x, test_y+1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score: 1.1469246541198932\n",
      "Accuracy: 0.6144854323308271\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_x,test_y+1,verbose=0)\n",
    "print('Test Score:', score[0])\n",
    "print('Accuracy:',score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Accuracy for Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3deZwcdb3v/9dnenr2fcm+TEggISEQwhwkgGwiInJUFMVcAQU0Rz0P8ejVK97r7+DB40/wuCGogBoQPLIooByvCLIctgAhhBACSSAh2ySZzJbZ9+nP/aM6ySSZSWYy6amZ6ffz8ahHd1dVd30qBfPu7/dbXWXujoiIJK+UsAsQEZFwKQhERJKcgkBEJMkpCEREkpyCQEQkySkIRESSnIJAZADMrMzM3MxSB7DuZ83s+aF+jshwURDImGNmm82s08xKDpj/WvyPcFk4lYmMTAoCGas2AYv3vDCz+UBWeOWIjFwKAhmr7gGu7PX6M8DdvVcws3wzu9vMqs1si5l928xS4ssiZvZDM6sxs3eBD/Xx3t+Y2U4z225m/25mkcEWaWaTzOwRM6szsw1m9vley041sxVm1mhmu8zsx/H5GWb2OzOrNbN6M3vFzMYPdtsieygIZKx6Ccgzs+Pjf6A/BfzugHVuAfKBY4CzCYLjqviyzwMXAycD5cClB7z3LqAbmBVf5wLgc0dQ531ABTApvo3/38zOiy+7GbjZ3fOAmcAD8fmfidc9FSgGvgC0HcG2RQAFgYxte1oF7wfWAtv3LOgVDt9y9yZ33wz8CLgivsongZ+6+zZ3rwO+3+u944GLgH9x9xZ3rwJ+Ev+8ATOzqcAZwDfdvd3dVwG/Zl9LpguYZWYl7t7s7i/1ml8MzHL3Hnd/1d0bB7Ntkd4UBDKW3QP8D+CzHNAtBJQAUWBLr3lbgMnx55OAbQcs22N6/L07410z9cDtwLhB1jcJqHP3pn5quAY4DlgX7/65uNd+PQbcZ2Y7zOwHZhYd5LZF9lIQyJjl7lsIBo0vAh46YHENwTfr6b3mTWNfq2EnQddL72V7bAM6gBJ3L4hPee4+b5Al7gCKzCy3rxrc/R13X0wQMDcBfzSzbHfvcvd/c/e5wOkEXVhXInKEFAQy1l0DnOfuLb1nunsPQZ/798ws18ymA19j3zjCA8C1ZjbFzAqB63q9dyfwOPAjM8szsxQzm2lmZw+mMHffBiwDvh8fAD4xXu/vAMzscjMrdfcYUB9/W8zMzjWz+fHurUaCQIsNZtsivSkIZExz943uvqKfxV8GWoB3geeB3wNL48t+RdD98jqwkoNbFFcCacBbwG7gj8DEIyhxMVBG0Dp4GLje3Z+IL7sQeNPMmgkGjj/l7m3AhPj2GgnGPp4h6C4SOSKmG9OIiCQ3tQhERJKcgkBEJMkpCEREkpyCQEQkyY26S+GWlJR4WVlZ2GWIiIwqr776ao27l/a1bNQFQVlZGStW9Hc2oIiI9MXMtvS3TF1DIiJJTkEgIpLkFAQiIklu1I0R9KWrq4uKigra29vDLmXYZGRkMGXKFKJRXXRSRIZmTARBRUUFubm5lJWVYWZhl5Nw7k5tbS0VFRXMmDEj7HJEZJQbE11D7e3tFBcXJ0UIAJgZxcXFSdUCEpHEGRNBACRNCOyRbPsrIokzZoLgcNq7eqhsaKerR5dtFxHpLWFBYGZLzazKzNb0s7zQzB42s9VmttzMTkhULRAEQVVTOz2xo3/Z7draWhYsWMCCBQuYMGECkydP3vu6s7NzwJ+zdOlSKisrj3p9IiKHksjB4ruAWzn4XrF7/G9glbtfYmZzgJ8D70tUMXs6UhJx94Xi4mJWrVoFwHe+8x1ycnL4+te/PujPWbp0KQsXLmTChAlHu0QRkX4lrEXg7s8CdYdYZS7wVHzddUCZmY1PVD3s6VMf5vvw/Pa3v+XUU09lwYIFfOlLXyIWi9Hd3c0VV1zB/PnzOeGEE/jZz37G/fffz6pVq7jssssG3ZIQERmKME8ffR34GPCcmZ1KcBPxKcCuA1c0syXAEoBp06YduHg///Zfb/LWjsaD5vfEnPauHjLTIqQMcqB17qQ8rv/Hwd6XHNasWcPDDz/MsmXLSE1NZcmSJdx3333MnDmTmpoa3njjDQDq6+spKCjglltu4dZbb2XBggWD3paIyJEKc7D4RqDAzFYR3Dv2NaCnrxXd/Q53L3f38tLSPi+eNyI98cQTvPLKK5SXl7NgwQKeeeYZNm7cyKxZs1i/fj3XXnstjz32GPn5+WGXKiJJLLQWgbs3AlcBWHAu5CaCm4gPSX/f3Bvbuthc28KscTlkpQ3Pbrs7V199Nd/97ncPWrZ69WoeffRRfv7zn/Pggw9yxx13DEtNIiIHCq1FYGYFZpYWf/k54Nl4OCRog/HHYRwjOP/883nggQeoqakBgrOLtm7dSnV1Ne7OJz7xCW644QZWrlwJQG5uLk1NTcNXoIgICWwRmNm9wDlAiZlVANcDUQB3vw04HvitmTnwJnBNomqBUHKA+fPnc/3113P++ecTi8WIRqPcdtttRCIRrrnmGtwdM+Omm24C4KqrruJzn/scmZmZLF++nLS0tMNsQURk6Mx9mE+jGaLy8nI/8MY0a9eu5fjjjz/k+5rau9hU08LM0hyy08fEJZYGtN8iIgBm9qq7l/e1LGl+WRxGi0BEZDRImiDYGwVKAhGR/SRPEOz96YCSQESkt6QJAnUNiYj0LWmCQERE+qYgEBFJckkTBHuvOZeAvqGjcRnqq666ivXr1x/94kREDmNsnFAfsoFchtrdcXdSUvrO3jvvvDPhdYqI9CV5WgTxx+EcLN6wYQNz587l05/+NPPmzWPnzp0sWbKE8vJy5s2bxw033LB33TPPPJNVq1bR3d1NQUEB1113HSeddBKLFi2iqqpqGKsWkWQz9loEj14HlW8cNDvNnWM6e8iIpkA/38r7NWE+fPDGIypn3bp13H333ZSXBz/ou/HGGykqKqK7u5tzzz2XSy+9lLlz5+73noaGBs4++2xuvPFGvva1r7F06VKuu+66I9q+iMjhJE2LICwzZ87cGwIA9957LwsXLmThwoWsXbuWt95666D3ZGZm8sEPfhCAU045hc2bNw9XuSKShMZei6Cfb+5dXT28u6uJaUVZFGQN38XcsrOz9z5/5513uPnmm1m+fDkFBQVcfvnltLe3H/Se3hebi0QidHd3D0utIpKc1CIYRo2NjeTm5pKXl8fOnTt57LHHwi5JRGQMtghGsIULFzJ37lzmzJnD9OnTOeOMM8IuSUQkeS5D3dHVw/pdTUwtyqJwGLuGEkmXoRaRgdJlqEEXGxIR6UfSBIFyQESkbwkLAjNbamZVZramn+X5ZvZfZva6mb1pZlcNZXujrYtrqJJtf0UkcRLZIrgLuPAQy/8ZeMvdTyK4t/GPet3MflAyMjKora09zB/HsdMmcHdqa2vJyMgIuxQRGQMSdtaQuz9rZmWHWgXINTMDcoA64IhOmJ8yZQoVFRVUV1f3u05PzNnV0E5nTZRdY+CexRkZGUyZMiXsMkRkDAjzL+KtwCPADiAXuMzdY32taGZLgCUA06ZNO2h5NBplxowZh9xYVVM7F3/vSf79oydw+YLpQyxdRGTsCHOw+APAKmASsAC41czy+lrR3e9w93J3Ly8tLT2ijaXEr0OtvnURkf2FGQRXAQ95YAOwCZiTqI0FQeDEYgoCEZHewgyCrcD7AMxsPDAbeDdRG0tf/2feTb+c3OaEbUJEZFRK2BiBmd1LcDZQiZlVANcDUQB3vw34LnCXmb1BcErPN929JlH1EImSYo7FBnbHMBGRZJHIs4YWH2b5DuCCRG3/QBaJBo89XcO1SRGRUSF5flmcmh489nSEXImIyMiSNEGQEo0HQUwtAhGR3pInCNQiEBHpU9IEQWo0uHpFd5cGi0VEekuaILDU4Lo8PZ1qEYiI9JY0QUD8rKFYt4JARKS35AmC+BhBrOvgm8WLiCSz5AmCSDBGEOvWGIGISG9JFwSuriERkf0kXRCgFoGIyH6SJwhSM4hhpPS0hV2JiMiIkjxBkJJCp2WQ2t0adiUiIiNK8gQB0BnJJL1HQSAi0luSBUE2aTF1DYmI9JZUQdAdyVQQiIgcILmCIDWbLNp0u0oRkV6SKgh6UrPJop22rp6wSxERGTESFgRmttTMqsxsTT/Lv2Fmq+LTGjPrMbOiRNUD4GlZZNNOS2d3IjcjIjKqJLJFcBdwYX8L3f0/3H2Buy8AvgU84+51CawHS88l29ppbFMQiIjskbAgcPdngYH+YV8M3JuoWvZISc8hi3Ya2nSXMhGRPUIfIzCzLIKWw4OHWGeJma0wsxXV1dVHvK3UzFyyaaexVZeZEBHZI/QgAP4ReOFQ3ULufoe7l7t7eWlp6RFvKC0rj4g5zc1NR/wZIiJjzUgIgk8xDN1CAOlZeQC0NtcPx+ZEREaFUIPAzPKBs4E/D8f2MvJKAOhqqh2OzYmIjAqpifpgM7sXOAcoMbMK4HogCuDut8VXuwR43N1bElVHb6nZwdmp3S0JPTlJRGRUSVgQuPviAaxzF8FppsMjKwgCb1WLQERkj5EwRjB8MuO/V2vTGIGIyB5JFgSFwWP77nDrEBEZQZIrCNJz6SFCREEgIrJXcgWBGe2peaR1NuCuK5CKiECyBQHQmVZArjfS1KHrDYmIQBIGQU9mCSXWSE1TR9iliIiMCEkXBJ4znnHsplpBICICJGEQRPInMs7qqW5qD7sUEZERIemCIKNwEtnWQX29fl0sIgJJGgQA7XU7Qq5ERGRkSLogSMmbAEBn/c6QKxERGRmSLgjICYKgp0FBICICyRgEueMBsJbKkAsRERkZki8IMgrotjTS22voienXxSIiyRcEZrRljmcCNexq1CmkIiLJFwRAd940plo1Fbvbwi5FRCR0CQsCM1tqZlVmtuYQ65xjZqvM7E0zeyZRtRwoUlTGFKtme33rcG1SRGTESmSL4C7gwv4WmlkB8Avgw+4+D/hEAmvZT+a4YyixRiqrdacyEZGEBYG7Pwsc6ue7/wN4yN23xtevSlQtB4qWzACgaeeG4dqkiMiIFeYYwXFAoZn9t5m9amZXDtuWC8oA6KjZNGybFBEZqRJ28/oBbvsU4H1AJvCimb3k7m8fuKKZLQGWAEybNm3oWy6cDkCkYSuxmJOSYkP/TBGRUSrMFkEF8Ji7t7h7DfAscFJfK7r7He5e7u7lpaWlQ99yVjGdqTlMju1gR4POHBKR5BZmEPwZONPMUs0sC3gPsHZYtmxGZ8FMZtkONlQ1D8smRURGqkSePnov8CIw28wqzOwaM/uCmX0BwN3XAn8DVgPLgV+7e7+nmh5tqePnMCtlu4JARJJewsYI3H3xANb5D+A/ElXDoWRMPJ6MN++norISOCaMEkRERoSk/GUxACWzAWjfMTy9USIiI1XyBkFpEAQptW/T3RMLuRgRkfAkbxAUltEdyeTY2CY2VGucQESSV/IGQUqErtJ5nJCyidUVDWFXIyISmuQNAiBj2kLm2RbWbNON7EUkeQ0oCMxsppmlx5+fY2bXxi8aN6rZpJPJsg5qt74VdikiIqEZaIvgQaDHzGYBdwBTgd8nrKrhMjH4IXNm9Ru0d/WEXIyISDgGGgQxd+8GLgFucfdvABMTV9YwKZlNTySd43mX17fVh12NiEgoBhoEXWa2GPgM8Jf4vGhiShpGkVR8wgIWprzDy5s0TiAiyWmgQXAVsAj4nrtvMrMZwD2JK2v4pM44nfkpm1i1cXvYpYiIhGJAQeDub7n7te5+r5kVArnuflOCaxse088glR5i25bT2a0flolI8hnoWUP/bWZ5ZlYErAR+ZWY/Tmxpw2TqqTgpnOxreW3r7rCrEREZdgPtGsp390bgY8Dd7v4e4PzElTWMMvKJjT+B01LW8tT6YbtbpojIiDHQIEg1s4nAJ9k3WDxmRI45i4UpG3hx7ZawSxERGXYDDYIbgMeAje7+ipkdA7yTuLKG2bHvJ0oXpTXLqdjdGnY1IiLDaqCDxX9w9xPd/Yvx1++6+8cTW9owmraIWDSbc1NW8fQ6dQ+JSHIZ6GDxFDN72Myq4tODZjYl0cUNm9R07JizOT+6msffrAy7GhGRYTXQrqE7gUeASfHpv+Lz+mVmS+Oh0eftJ+PXLGows1Xx6V8HU/jRZsdewASvpurd1dQ2d4RZiojIsBpoEJS6+53u3h2f7gJKD/Oeu4ALD7POc+6+ID7dMMBaEuPYCwB4n73CX9/YGWopIiLDaaBBUGtml5tZJD5dDtQe6g3u/iwweq7bkD8Zpp7GJ9Jf5s+rdoRdjYjIsBloEFxNcOpoJbATuBT47FHY/iIze93MHjWzeUfh84Zm/qXMiG2haevrOntIRJLGQM8a2uLuH3b3Uncf5+4fBYZ61tBKYLq7nwTcAvypvxXNbImZrTCzFdXV1UPc7CHMuwS3CB+JLFOrQESSxlDuUPa1oWzY3RvdvTn+/K9A1MxK+ln3Dncvd/fy0tLDDU0MQXYJNvM8PpH+Eve9vJmemCduWyIiI8RQgsCGsmEzm2BmFn9+aryWQ447DIv5n6C0p4pxDat59u0Etj5EREaI1CG895Bfl83sXuAcoMTMKoDrid/DwN1vIxhn+KKZdQNtwKfcPfyv4HMuwqNZXGEvcM9L7+XcOePCrkhEJKEOGQRm1kTff/ANyDzUe9198WGW3wrcergCh116Ljb/Uj606n6uX/9JttXNY2pRVthViYgkzCG7htw9193z+phy3X0orYmR7R8+TzTWwScjz7H0hU1hVyMiklBDGSMYuyaeCFNPY0nWU9y/fAt1LZ1hVyQikjAKgv6c+nlKO7dzas9K7lKrQETGMAVBf47/MORN4du5j3LXsk00d3SHXZGISEIoCPqTmgZnfIVZHWs4vmMNv37u3bArEhFJCAXBoSy8ArLH8Z2Cv/KrZ9+lRlclFZExSEFwKNFMOP3LHN/2KrO713HrUxvCrkhE5KhTEBxO+dWQVcKPCh7iP1/ezNZaXYxORMYWBcHhpOfAef+HGa2v84+R5dzwlzfDrkhE5KhSEAzEws/A+BP4t8z7eG5tBX9/a1fYFYmIHDUKgoFIicCFN5LbUcl1+X/nO4+8SVtnT9hViYgcFQqCgZrxXjj+w1zZ8xDd9du55al3wq5IROSoUBAMxgXfJeIxfjH+EW5/9l1WbasPuyIRkSFTEAxGYRks+mdOaXici7Lf5msPrFIXkYiMegqCwTrrG1A8ix9Gb6Omuoqb/rYu7IpERIZEQTBYaVlwyR2kt1Vxz6QHuWvZZp57R3cyE5HRS0FwJKacAmd9nZPq/sZVhav5l/tWUdnQHnZVIiJHJGFBYGZLzazKzNYcZr1/MLNuM7s0UbUkxFnfgEkn822/neyuWv759yvp6omFXZWIyKAlskVwF3DhoVYwswhwE/B4AutIjEgULrmDSHcbf5z4O1ZuqeX7f9V4gYiMPgkLAnd/Fqg7zGpfBh4EqhJVR0KVHgcf+B7jdj3H0hlPs/SFTTz4akXYVYmIDEpoYwRmNhm4BPjlANZdYmYrzGxFdfUIG5gtvwZOWsw5O5fyxUkbue6h1bz0bm3YVYmIDFiYg8U/Bb7p7oftWHf3O9y93N3LS0tLh6G0QTCDi3+CTTiB/9XyQxYV1PNP97zKxurmsCsTERmQMIOgHLjPzDYDlwK/MLOPhljPkYtmwmW/wyKp/Cb6HxRZM1ff9Qq1upGNiIwCoQWBu89w9zJ3LwP+CHzJ3f8UVj1DVlgGn/o90aYKHhl3G3WNzVy5dDmN7V1hVyYickiJPH30XuBFYLaZVZjZNWb2BTP7QqK2Gbppp8FHfkFu5cs8OeNeNuxq4Jq7XtFlKERkREtN1Ae7++JBrPvZRNUx7E78BDTtYNzf/5W/H5fDOes+zOfvXsGvriwnMy0SdnUiIgfRL4sT4YyvwJlfZdqm+3l03lMs21jNZ+9cTktHd9iViYgcREGQKO+7HsqvZvaG3/C3+c+yYkudxgxEZERSECSKGVz0I1j4GY57+3Yem/8Mr2/bzeW/fllnE4nIiKIgSKSUFLj4p3DKZ5m1/naeOPFp1lc28rFfLmNTTUvY1YmIAAqCxEtJgQ/9BMqvpmzdr1g2+wFaW1v5+C+XsXLr7rCrExFREAyLlBT40I/h3G9TvPFhnp10KxPT21l8x0v8bc3OsKsTkSSnIBguZnD2N+CSO8jc+Qp/zriBs8e38YXfreTHj68nFvOwKxSRJKUgGG4nXQZXPExq6y5ub/8mX53bzM+e2sDn716hM4pEJBQKgjDMeC9c83csmsG12/6FOxdV88zb1Xz01hdYX9kUdnUikmQUBGEpnQ3XPIGVzubcVV/l6fesoKmtkw/f+jy/e2kL7uoqEpHhoSAIU+54+Oz/hXkfY+prP+KFKT/n/OkRvv2nNXzxdyupb+0Mu0IRSQIKgrClZcPHfw0X/5S0ihe5tfFabjm9jSfX7eKim5/j+Xdqwq5QRMY4BcFIYAblV8HnnsCimfzja0t4dtFrZKXC5b95mf/5wOvsblHrQEQSQ0Ewkkw8EZY8A3M/wsQVP+Dxwpv49mlR/rxqO+//yTM88voOjR2IyFGnIBhpMvLg0qVwye2k1Kzjc2uu4IWz1zI1P41r732Na367gu31bWFXKSJjiIJgJDKDkz4FX3oZjjmH8S/ewENZ3+cH5+Xw4sZaLvjxM/x22WZ69CM0ETkKFAQjWd5EWHwffPQ2rPotPrn8Ml5872pOm57D9Y+8yUd//gIvv1sbdpUiMsol8laVS82syszW9LP8I2a22sxWmdkKMzszUbWMamawYDF86SWYeS4Fy77Hr5u+xB/eW0lNUzuX3fES/3TPCl3NVESOmCVq8NHMzgKagbvd/YQ+lucALe7uZnYi8IC7zznc55aXl/uKFSuOfsGjxcan4fFvw641xKacykOlX+L6VzPp6I5xxaLpfOV9x1KQlRZ2lSIywpjZq+5e3teyhLUI3P1ZoO4Qy5t9XwplA+rwHoiZ58I/PQsfvoWU3Zu59LXP8urc+/n8iVF+u2wzZ/3gaX7x3xt0W0wRGbCEtQgAzKwM+EtfLYL48kuA7wPjgA+5+4v9rLcEWAIwbdq0U7Zs2ZKQekedjiZ44WZYdgu4U3vi5/n/at/PX99uoTArypKzZnLloulkp6eGXamIhOxQLYJQg6DXemcB/+ru5x/uM5O+a6gvDRXw5A2w+n7ILmXrSV/l+m0LefqdOoqy0/ins47h8tMUCCLJLJSuocGIdyMdY2YlYdcyKuVPgY/dAZ9/CopnMW3Z/+bOjq/xxMVtzJuYy/cfXcfpNz7FDx9bT43ulywiBwgtCMxslplZ/PlCIB3QuZBDMfkUuOpR+OTd0NXGrCeu4Z7ur/PURQ0smpHPz/97A2fc+BTf/tMbbNZZRiISl8izhu4FzgFKgF3A9UAUwN1vM7NvAlcCXUAb8A13f/5wn6uuoQHq6YI3/gDP/hDqNkLRTKrnf46ba/6BB1bV0BWLcfZxpVy5aDpnHzeOSIqFXbGIJFBoYwSJoCAYpFgPrH0EXvgZ7FgJWcU0n3Q19/RcwJ2vNVLV1MHUokw+/Z7pfLJ8KkXZOvVUZCxSEAi4w5ZlsOxn8PbfIDWTnpMW83zhJfzizSgvb6ojLTWFi+dP5IpF01kwtYB4z52IjAEKAtlf1Tp48dbgLKOeTpjyD1TO+iS/ql3Afa/X0dLZw5wJuXxs4WQ+smAy4/Mywq5YRIZIQSB9a6mB1++DlXdDzXpIy6Hr+I/yZOYHuH1jEa9tayDF4MxjS/n4wslcMHcCmWmRsKsWkSOgIJBDc4dty+G1u2HNQ9DVCqXHU3vcJ7mv4wx+v6aF7fVt5KSnctH8CVxy8hROnVGkAWaRUURBIAPX3ghvPgQr74HtKyAlis/5EGsnXsJdO6fzf9fsoqWzh3G56Vw0fyIXzZ/IKdMLFQoiI5yCQI7MrjeDQFh9H7TthvxpdM37BM9nnsX9m3N5en0VHd0xSnLSeP/c8Xxg3gROn1lCWuqI+J2iiPSiIJCh6e6AdX8JQmHTM+AxKJ1Dx+yP8FL66fxhay5Pr6+mpbOH3IxUzp09jnNml/LeY0spzU0Pu3oRQUEgR1NzFbz1Z3jz4eB0VByKZ9F97AdZlXU6f9g1gSfW1VLb0gnAvEl5nHVcKWcdW8op0wvVWhAJiYJAEqNpV9BSWPtfsPk5iHVDdil+3IVsLT2HR1vn8NTGJlZu2U13zMlOi7BoZglnH1fC2ceNY1pxVth7IJI0FASSeO0N8M7fYf1fg8eORohmwczzaC87j5cjJ/N4RZRn3q6mYncbAGXFWZx1XClnH1fKaccU6+qoIgmkIJDh1d0JW56HdX+F9Y9CY0Uwv+Q4fOZ5VJaewROts3hqYzMvvVtHW1cP0YhxwuR8Fk4rDKbpBUzMzwx3P0TGEAWBhMcdqtfDxidhw5Ow5QXobodIOkw9le5pZ7A2/SQerZ/M8q0trN7eQGd3DICJ+RksnFbIydMKWDi9kHmT8khP1Q/aRI6EgkBGjq62IAw2Pg2bnoXKNwCH1Ix4MLyXTTkn80LbdFZUNPPa1nq21wddSWmRFE6YnMfJajWIDJqCQEautt3B2Uebnw8GnCvXEARDJkx7D0w7nd1FJ7Kyazov7zJWbtmtVoPIEVAQyOjRWtcrGJ6HXfFgACicAZMX0j3hZDalz+HF1sks397eZ6shCIdC5kzMZXpRFqkRnbYqyU1BIKNXeyPsXAXbX4XtK4Npz+CzpcC4uTDpZBqLT+QNn8lzDeNYsa1pv1ZDWmoKx47LYfb4XI6bkMvsCbnMHp/LxPwMXWpbkoaCQMaWpl3BTXa2rwwCYsfKoIsJgrGGCSfSM2khO7KO502bxcrmQtbtauHtyiYqG9v3fkxuRuq+cBi/LyAKdXMeGYNCCQIzWwpcDFS5+wl9LP808E3AgCbgi+7++uE+V0EgB3GH3Zv2tRh2rISdrwdXUQXIyIdJJ8OEE2ktmsvGlBm83l7C+qp21lc2sa6ykcb27r0fV5qbzpwJuRzXKxyOHZ9DVpp+5yCjV1hBcBbQDNzdTxCcDl5Z8vQAAA3lSURBVKx1991m9kHgO+7+nsN9roJABqSnG6rXxVsOr8KO16BqbXAjHoBIGpTMhvHz8HHHU59zDG/HJrO6KZ/1VS2sr2zinaom2ruC7iUzmFaUxTEl2ZSVZDOjJJvpxdnMKM5mUkGGxiBkxAuta8jMyoC/9BUEB6xXCKxx98mH+0wFgRyxni6oeRt2vQW73giurrrrTWjauW+d1EwoPQ5K5xArmU11RhnreybxeksB66raeLe6hS21LbR29ux9SzRiTC3Moqwkm+nFWQoJGZFGQxB8HZjj7p/rZ/kSYAnAtGnTTtmyZctRrlSSWttuqH47aEFUr9/3uGdQGoIWRNFMKJmFF82iKXs6FZHJvNM1nnVNaWypa2VTTWu/ITG1KIupRZlMKcxiSmHwOLUwk6LsNA1Yy7AY0UFgZucCvwDOdPfaw32mWgQybNobgxZE9frgVp7Vb0PthmA8IrZvTIH0fCg+Bgpn4IUzaMqaSoVNYEN3CWubs9lc28a23a1U7G6jvrVrv01kRiNMKcxkatGegMhkYn4mE/MzmJCfwfi8DKJqUchRcKggCHX0y8xOBH4NfHAgISAyrDLyYEp5MPXW0w31W6Du3SAYajcEz3e8hr31Z/K8h7nAXODDqRlQWAZF02HmdNpzplCdOpEKxrGxq5hNTRG21QUhsWJz3X6D1hCMTZTkpDMpHgwT8zPjjxlMyAtej89P14/oZEhCCwIzmwY8BFzh7m+HVYfIoEVSoXhmMB37/v2X9XRBwzao2xSEw+7N8WkLbH2RjI5GpgJTgUUAmYVQMA3GTYFjJ9OePYm66AQqKWFbTzGb2rPZ2dDBzsZ2NtW0sGxjLU0HhAVASU4aE/IzmJC3f2uiNDedcbnplOamU5SVRopuKSp9SORZQ/cC5wAlwC7geiAK4O63mdmvgY8Dezr8u/trtvSmriEZtdyD8Yjdm4MWxe4twWP9VmjYDg0V0Nm0/3si6ZA/BfImBVPuRDqyJ1IXKaXSC9nWnc+W9ix2NHZT2dDGzoZ2dja009DWddDmIylGSU4a43L3BURxThrF2cFjSU46RdlpFOekUZSVpkHuMUY/KBMZLdrqg0Bo2Ab124LHhm3QuBOadgSPsQP/yBtkl0LueMidCDnj6coaR2O0hDoroooCdnTlsbUrl8rmGNXNHVQ1dlDV1MHu1k56Yn3/DSjMilKck05x9v4hUZyTTkl28BgESRr5mVENeo9wI3aMQEQOkFkQTBP6Ob8iFoOW6uCMpqbKfVNzZfCL66adsPN1oi3VFHuMYuDY3u/PKoacCVA0AaZPwLPH05ZRQmOkkDoKqPY8Krtz2NGRQW1rF3UtndQ0d7KuspHals6DBrv3SE2xeFCkU5gVpTArjYKsKEXZaRRkpe03b89jbkaUiLqqRgQFgchokpIS/+Y//tDr9XQHgdE7IJp37R8cVWux5l1keQ9ZwIT9tpMKWSVBSyOnFEpLIbuUnqwSWlILqU8poC6WS3Usm51dOexqj1Db0kVNcyf1rZ2srWykvrWL+tZO+mlwYAZ5GVEKs6LkZ6VRkBmlICtKQWaU/MwoeQc+ZkTJz4qSl5FKTnqqWiBHkYJAZCyKpELexGA6lFgPtNYGodFSDS018QCp2v917QZoribS3UYekAdM22976ZBdEgx+ZxRAUSFkFuIZRbRH82mJ5NFgudSTQ10sh+rubKq6s9jdHmN3axf1bUFobK5tYXdLJ00d3Ryq1zrF2C8g8jJTycuIkpuRSu4Bj3l9zMvNSNWZVr0oCESSWUoEcsYF00B0tsSDohra6oKgaK2JP9YGYxxtu6FmA7TVYa11ZMa6yCQ4a+Qg6XlBeGQVQX4RTCyC9Fw8mk1Hai6taSU0peTS5Jk0eBa7YxnUdWdR3ZVOQ0eMhrYuGtu6aGzvZldjM03tXTS1d+/3o77+pEVS9oZCTryVkZMeJSc9QlZ6KtlpEbLTU8lOSyUrPRI8pkXISU/du7z3eqP59x4KAhEZuLTsYCosG9j67tDZHNxnoq0uCInW3o91+y+rexc6m7HOFjK6WskAivqtJTe4oGBGfvCbj/z8va9jabl0pObSlpJDS0oWzWTT6Fk0xDKo78lgd086dZ1RGjt6aGrvoqWjm5aOHrbXt9Hc0UVbZw8tHT20dR0+UPaWE0nZLzCy01PJTo+QlXZwaIy0cFEQiEjimEF6bjAVTh/ce7vag7GM9obgV97tDcHU0et5W/2+eY3bgwsLtjeQ0tFIpsfI5BBBYinx2vL21ZiTu9+8WFounanZdESyaSeDNsukhQxayKAplkGTp9PQk0ljV4SWrhitnUGgtHZ209wRtExqm1tp7dw3b8+FDAfiwHBZfOo0rj5zxuD+HQdAQSAiI1M0Y+AtjwPtaYn0DpGOpiAwOhrjz+NTe695rXXB7zvir1PirZIMIP9Q27MIpOdANHtfqyktG/KyIZoFaTmQlgVp2cRSs+iMZNJpQbC0kUEr6bSQTnMsmBp70mjoSaOxK0JrV8/ecCnIih7Zv8dhKAhEZOzp3RI55F/ww+jpDn7k194Y3N+isyUIjM5m6GiOPzb1emyFrpZgvc7WYNB9z/s6m6GzhZRY995wyTvsfqQEIRINQoT2q4AvD2GH+qYgEBHpTyQ1GMzOLDx6n9nduX9YdDb3CoteU1/r5BzmtOEjpCAQERlOqWnBdDTDZYhG7/lOIiJyVCgIRESSnIJARCTJKQhERJKcgkBEJMkpCEREkpyCQEQkySkIRESS3Ki7VaWZVbPvPseDVQLUHMVyRgPtc3LQPieHoezzdHcv7WvBqAuCoTCzFf3ds3Os0j4nB+1zckjUPqtrSEQkySkIRESSXLIFwR1hFxAC7XNy0D4nh4Tsc1KNEYiIyMGSrUUgIiIHUBCIiCS5pAkCM7vQzNab2QYzuy7seo4WM5tqZk+b2Vtm9qaZfSU+v8jM/m5m78QfC+Pzzcx+Fv93WG1mC8PdgyNjZhEze83M/hJ/PcPMXo7v1/1mlhafnx5/vSG+vCzMuofCzArM7I9mts7M1prZorF8nM3sq/H/pteY2b1mljEWj7OZLTWzKjNb02veoI+rmX0mvv47ZvaZwdSQFEFgZhHg58AHgbnAYjObG25VR0038D/dfS5wGvDP8X27DnjS3Y8Fnoy/huDf4Nj4tAT45fCXfFR8BVjb6/VNwE/cfRawG7gmPv8aYHd8/k/i641WNwN/c/c5wEkE+z8mj7OZTQauBcrd/QQgAnyKsXmc7wIuPGDeoI6rmRUB1wPvAU4Frt8THgPi7mN+AhYBj/V6/S3gW2HXlaB9/TPwfmA9MDE+byKwPv78dmBxr/X3rjdaJmBK/H+O84C/AEbwa8vUA4838BiwKP48Nb6ehb0PR7DP+cCmA2sfq8cZmAxsA4rix+0vwAfG6nEGyoA1R3pcgcXA7b3m77fe4aakaBGw7z+qPSri88aUeHP4ZOBlYLy774wvqgT23PV6LPxb/BT4X0As/roYqHf37vjr3vu0d3/jyxvi6482M4Bq4M54l9ivzSybMXqc3X078ENgK7CT4Li9ytg/znsM9rgO6XgnSxCMeWaWAzwI/Iu7N/Ze5sFXhDFxnrCZXQxUufurYdcyzFKBhcAv3f1koIV93QXAmDvOhcBHCAJwEpDNwd0nSWE4jmuyBMF2YGqv11Pi88YEM4sShMB/uvtD8dm7zGxifPlEoCo+f7T/W5wBfNjMNgP3EXQP3QwUmFlqfJ3e+7R3f+PL84Ha4Sz4KKkAKtz95fjrPxIEw1g9zucDm9y92t27gIcIjv1YP857DPa4Dul4J0sQvAIcGz/jII1g0OmRkGs6KszMgN8Aa939x70WPQLsOXPgMwRjB3vmXxk/++A0oKFXE3TEc/dvufsUdy8jOI5PufungaeBS+OrHbi/e/4dLo2vP+q+Nbt7JbDNzGbHZ70PeIsxepwJuoROM7Os+H/je/Z3TB/nXgZ7XB8DLjCzwnhr6oL4vIEJe5BkGAdjLgLeBjYC/yfseo7ifp1J0GxcDayKTxcR9I8+CbwDPAEUxdc3gjOoNgJvEJyVEfp+HOG+nwP8Jf78GGA5sAH4A5Aen58Rf70hvvyYsOsewv4uAFbEj/WfgMKxfJyBfwPWAWuAe4D0sXicgXsJxkG6CFp+1xzJcQWuju//BuCqwdSgS0yIiCS5ZOkaEhGRfigIRESSnIJARCTJKQhERJKcgkBEJMkpCEQOYGY9Zraq13TUrlZrZmW9rzIpMhKkHn4VkaTT5u4Lwi5CZLioRSAyQGa22cx+YGZvmNlyM5sVn19mZk/Frw//pJlNi88fb2YPm9nr8en0+EdFzOxX8WvtP25mmaHtlAgKApG+ZB7QNXRZr2UN7j4fuJXgKqgAtwC/dfcTgf8Efhaf/zPgGXc/ieC6QG/G5x8L/Nzd5wH1wMcTvD8ih6RfFoscwMya3T2nj/mbgfPc/d34hf4q3b3YzGoIrh3fFZ+/091LzKwamOLuHb0+owz4uwc3HMHMvglE3f3fE79nIn1Ti0BkcLyf54PR0et5Dxqrk5ApCEQG57Jejy/Gny8juBIqwKeB5+LPnwS+CHvvsZw/XEWKDIa+iYgcLNPMVvV6/Td333MKaaGZrSb4Vr84Pu/LBHcO+wbBXcSuis//CnCHmV1D8M3/iwRXmRQZUTRGIDJA8TGCcnevCbsWkaNJXUMiIklOLQIRkSSnFoGISJJTEIiIJDkFgYhIklMQiIgkOQWBiEiS+394DHzcSSmDvgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot two graphs 'Accuracy v/s Epoch' and 'Loss v/s Epoch'\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Test', 'Train'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Test', 'Train'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
